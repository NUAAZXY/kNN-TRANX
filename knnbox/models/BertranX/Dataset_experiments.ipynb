{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import sys\n",
    "from collections import Counter\n",
    "from collections import deque\n",
    "from nltk import ngrams\n",
    "\n",
    "import astor\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from asdl.ast_operation import make_iterlists, seq2ast, Grammar, GrammarRule\n",
    "from asdl.grammar import ReduceAction\n",
    "from config.config import init_arg_parser\n",
    "from dataset.utils import tokenize_for_bleu_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoNaLa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conala_path_train = 'dataset/data_conala/conala-corpus/conala-train.json'\n",
    "conala_path_test = 'dataset/data_conala/conala-corpus/conala-test.json'\n",
    "\n",
    "conala_preprocessing = json.load(open(conala_path_train))\n",
    "conala_test_preprocessing = json.load(open(conala_path_test))\n",
    "\n",
    "train_set_conala = pd.read_csv('dataset/data_conala/train/conala-train.csv')\n",
    "dev_set_conala = pd.read_csv('dataset/data_conala/train/conala-val.csv')\n",
    "test_set_conala = pd.read_csv('dataset/data_conala/test/conala-test.csv')\n",
    "\n",
    "# pydf_conala_preprocess = pd.concat([train_set_conala, dev_set_conala, test_set_conala])\n",
    "pydf_conala_preprocess = pd.concat([train_set_conala, dev_set_conala])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of replacement CoNaLa: 55812\n"
     ]
    }
   ],
   "source": [
    "replace_number = 0\n",
    "\n",
    "for x in pydf_conala_preprocess.values:\n",
    "    replace_number += len(eval(x[4]))\n",
    "\n",
    "print('number of replacement CoNaLa:', replace_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre tokens nl total 33327\n",
      "nombre de tokens dans input et actions (sans les var0, str0 + sans duplicates...) 877\n",
      "nombre de tokens dans input et actions (sans les var0, str0...) 3726\n"
     ]
    }
   ],
   "source": [
    "number_token_input_actions = 0\n",
    "total_token_input_actions = []\n",
    "nl_tokens = 0\n",
    "\n",
    "variables = ['var_0', 'var_1', 'var_2', 'var_3', 'str_0', 'str_1', 'str_2', 'str_3', 'str_4']\n",
    "\n",
    "for x in pydf_conala_preprocess.values:\n",
    "    nl = eval(x[0])\n",
    "    nl_tokens += len(eval(x[0]))\n",
    "    nl = [b for b in nl if b not in variables]\n",
    "    actions = eval(x[1])\n",
    "    actions = [b for b in actions if b not in variables]\n",
    "    token_input_actions = list(set(nl) & set(actions))\n",
    "    total_token_input_actions += token_input_actions\n",
    "    number_token_input_actions += len(token_input_actions)\n",
    "\n",
    "print('nombre tokens nl total', nl_tokens)\n",
    "print('nombre de tokens dans input et actions (sans les var0, str0 + sans duplicates...)', len(list(dict.fromkeys(total_token_input_actions))))\n",
    "print('nombre de tokens dans input et actions (sans les var0, str0...)', number_token_input_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur CoNaLa, tokenizer BERT fait perdre 200 mots pr√©sents dans input et output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre tokens nl total 24593\n",
      "nombre de tokens dans input et code (sans les var0, str0 + sans duplicates...) 441\n",
      "nombre de tokens dans input et code 2024\n"
     ]
    }
   ],
   "source": [
    "number_token_input_output = 0\n",
    "total_token_input_code = []\n",
    "nl_tokens = 0\n",
    "\n",
    "for i, example in enumerate(conala_preprocessing):\n",
    "    try:\n",
    "        intent = example['rewritten_intent'].split()\n",
    "    except:\n",
    "        intent = example['intent'].split()\n",
    "    nl_tokens += len(intent)\n",
    "    snippet = tokenize_for_bleu_eval(example['snippet'])\n",
    "    \n",
    "    number_token_input_output += len(set(intent) & set(snippet))\n",
    "    total_token_input_code += list(set(intent) & set(snippet))\n",
    "\n",
    "#for i, example in enumerate(conala_test_preprocessing):\n",
    "#    try:\n",
    "#        intent = example['rewritten_intent'].split()\n",
    "#    except:\n",
    "#        intent = example['intent'].split()\n",
    "#    nl_tokens += len(intent)\n",
    "#    snippet = tokenize_for_bleu_eval(example['snippet'])\n",
    "#    number_token_input_output += len(set(intent) & set(snippet))\n",
    "    \n",
    "print('nombre tokens nl total', nl_tokens)\n",
    "print('nombre de tokens dans input et code (sans les var0, str0 + sans duplicates...)', len(list(dict.fromkeys(total_token_input_code))))\n",
    "print('nombre de tokens dans input et code', number_token_input_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2882628605719756\n"
     ]
    }
   ],
   "source": [
    "intent = []\n",
    "code = []\n",
    "\n",
    "for value in conala_preprocessing:\n",
    "    try:\n",
    "        intent.append(value['rewritten_intent'].split())\n",
    "    except:\n",
    "        intent.append(value['intent'].split())\n",
    "    code.append([tokenize_for_bleu_eval(value['snippet'])])\n",
    "    \n",
    "for value in conala_test_preprocessing:\n",
    "    try:\n",
    "        intent.append(value['rewritten_intent'].split())\n",
    "    except:\n",
    "        intent.append(value['intent'].split())\n",
    "    code.append([tokenize_for_bleu_eval(value['snippet'])])\n",
    "\n",
    "BLEU = bleu_score(intent, code)\n",
    "print(BLEU * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32475339248776436\n"
     ]
    }
   ],
   "source": [
    "intent = []\n",
    "code = []\n",
    "\n",
    "for value in conala_preprocessing:\n",
    "    try:\n",
    "        intent.append([value['rewritten_intent'].split()])\n",
    "    except:\n",
    "        intent.append([value['intent'].split()])\n",
    "    code.append(tokenize_for_bleu_eval(value['snippet']))\n",
    "    \n",
    "for value in conala_test_preprocessing:\n",
    "    try:\n",
    "        intent.append([value['rewritten_intent'].split()])\n",
    "    except:\n",
    "        intent.append([value['intent'].split()])\n",
    "    code.append(tokenize_for_bleu_eval(value['snippet']))\n",
    "\n",
    "BLEU = bleu_score(code, intent)\n",
    "print(BLEU * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Django"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_file = './dataset/data_django/all.anno'\n",
    "code_file = './dataset/data_django/all.code'\n",
    "\n",
    "train_set_django = pd.read_csv('./dataset/data_django/train.csv')\n",
    "dev_set_django = pd.read_csv('./dataset/data_django/dev.csv')\n",
    "#test_set_django = pd.read_csv('./dataset/data_django/test.csv')\n",
    "\n",
    "#pydf_django_preprocess = pd.concat([train_set_django, dev_set_django, test_set_django])\n",
    "pydf_django_preprocess = pd.concat([train_set_django, dev_set_django])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272851\n",
      "72674\n",
      "nombre de tokens dans input et code 72674\n"
     ]
    }
   ],
   "source": [
    "number_token_input_output = 0\n",
    "nl_tokens = 0\n",
    "total_token_input_code = []\n",
    "\n",
    "for idx, (src_query, tgt_code) in enumerate(zip(open(annot_file), open(code_file))):\n",
    "    try:\n",
    "        src_query = src_query.strip()\n",
    "        src_query = tokenize_for_bleu_eval(src_query)\n",
    "        # src_query = src_query.split()\n",
    "        nl_tokens += len(src_query)\n",
    "        tgt_code = tgt_code.strip()\n",
    "        tgt_code = tgt_code.split()\n",
    "        number_token_input_output += len(set(src_query) & set(tgt_code))\n",
    "        total_token_input_code += list(set(src_query) & set(tgt_code))\n",
    "    except: \n",
    "        pass\n",
    "    \n",
    "print(nl_tokens)\n",
    "print(len(total_token_input_code))\n",
    "print('nombre de tokens dans input et code', number_token_input_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256304\n",
      "5382\n",
      "nombre de tokens dans input et actions (sans les var0, str0...) 42043\n"
     ]
    }
   ],
   "source": [
    "number_token_input_actions = 0\n",
    "nl_tokens = 0\n",
    "variables = ['str_0', 'str_1', 'str_2', 'str_3', 'str_4', 'str_5', 'str_6', 'str_7', 'var_0','var_1','var_2','var_3','var_4','var_5', 'var_6', 'var_7']\n",
    "\n",
    "for x in pydf_django_preprocess.values:\n",
    "    nl = eval(x[0])\n",
    "    nl_tokens += len(nl)\n",
    "    nl = [b for b in nl if b not in variables]\n",
    "    actions = eval(x[2])\n",
    "    actions = [b for b in actions if b not in variables]\n",
    "    number_token_input_actions += len(set(nl) & set(actions))\n",
    "    total_token_input_code += list(set(nl) & set(actions))\n",
    "\n",
    "print(nl_tokens)\n",
    "print(len(list(dict.fromkeys(total_token_input_code))))\n",
    "print('nombre de tokens dans input et actions (sans les var0, str0...)', number_token_input_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.419631361961365\n"
     ]
    }
   ],
   "source": [
    "src_query = []\n",
    "tgt_code = []\n",
    "\n",
    "for idx, (query, code) in enumerate(zip(open(annot_file), open(code_file))):\n",
    "    try:\n",
    "        src_query.append(tokenize_for_bleu_eval(query.strip()))\n",
    "        tgt_code.append([tokenize_for_bleu_eval(code.strip())])\n",
    "        # tgt_code.append([code.strip().split()])\n",
    "        # print('query', tokenize_for_bleu_eval(query.strip()))\n",
    "        # print('code', tokenize_for_bleu_eval(code.strip()))\n",
    "    except: \n",
    "        pass\n",
    "    \n",
    "BLEU = bleu_score(src_query, tgt_code)\n",
    "print(BLEU * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.86678050452676\n"
     ]
    }
   ],
   "source": [
    "src_query = []\n",
    "tgt_code = []\n",
    "\n",
    "for idx, (query, code) in enumerate(zip(open(annot_file), open(code_file))):\n",
    "    try:\n",
    "        src_query.append([tokenize_for_bleu_eval(query.strip())])\n",
    "        tgt_code.append(tokenize_for_bleu_eval(code.strip()))\n",
    "    except: \n",
    "        pass\n",
    "    \n",
    "BLEU = bleu_score(tgt_code, src_query)\n",
    "print(BLEU * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CodeSearchNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = './dataset/data_github/python/final/jsonl/train/'\n",
    "dev_path = './dataset/data_github/python/final/jsonl/valid/'\n",
    "test_path = './dataset/data_github/python/final/jsonl/test/'\n",
    "\n",
    "train_set_csn = pd.read_csv(train_path + 'train.csv')\n",
    "dev_set_csn = pd.read_csv(dev_path + 'valid.csv')\n",
    "test_set_csn = pd.read_csv(test_path + 'test.csv')\n",
    "\n",
    "pydf_csn_preprocess = pd.concat([train_set_csn, dev_set_csn, test_set_csn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nombre de tokens dans input et code 22450\n",
      "nombre de tokens dans input et actions (sans les var0, str0...) 7622\n"
     ]
    }
   ],
   "source": [
    "number_token_input_actions = 0\n",
    "number_token_input_output = 0\n",
    "\n",
    "variables = ['var_0', 'var_1', 'var_2', 'var_3', 'str_0', 'str_1', 'str_2', 'str_3', 'str_4']\n",
    "\n",
    "for x in pydf_csn_preprocess.values:\n",
    "    nl = eval(x[0])\n",
    "    nl = [b for b in nl if b not in variables]\n",
    "    actions = eval(x[6])\n",
    "    actions = [b for b in actions if b not in variables]\n",
    "    code = eval(x[1])\n",
    "    number_token_input_output += len(set(nl) & set(code))\n",
    "    number_token_input_actions += len(set(nl) & set(actions))\n",
    "    \n",
    "\n",
    "print('nombre de tokens dans input et code', number_token_input_output)\n",
    "print('nombre de tokens dans input et actions (sans les var0, str0...)', number_token_input_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
