{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caractérisation et préprocessing des exemples NL -> Python \n",
    "### Overleaf disponible ici : https://www.overleaf.com/7663237845srdhzcgvqdwg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "from dataset.data_conala import *\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "import ast\n",
    "import astor\n",
    "from tokenize import tokenize, untokenize, COMMENT, TokenError\n",
    "from io import BytesIO\n",
    "from asdl.ast_operation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identifier, int, string, bytes, object, singleton, constant\n",
      "\n",
      "    mod = Module(stmt* body)\n",
      "        | Interactive(stmt* body)\n",
      "        | Expression(expr body)\n",
      "        | Suite(stmt* body)\n",
      "\n",
      "    stmt = FunctionDef(identifier name, arguments args,\n",
      "                       stmt* body, expr* decorator_list, expr? returns)\n",
      "          | AsyncFunctionDef(identifier name, arguments args,\n",
      "                             stmt* body, expr* decorator_list, expr? returns)\n",
      "\n",
      "          | ClassDef(identifier name,\n",
      "             expr* bases,\n",
      "             keyword* keywords,\n",
      "             stmt* body,\n",
      "             expr* decorator_list)\n",
      "          | Return(expr? value)\n",
      "\n",
      "          | Delete(expr* targets)\n",
      "          | Assign(expr* targets, expr value)\n",
      "          | AugAssign(expr target, operator op, expr value)\n",
      "\n",
      "          | AnnAssign(expr target, expr annotation, expr? value, int simple)\n",
      "\n",
      "          | For(expr target, expr iter, stmt* body, stmt* orelse)\n",
      "          | AsyncFor(expr target, expr iter, stmt* body, stmt* orelse)\n",
      "          | While(expr test, stmt* body, stmt* orelse)\n",
      "          | If(expr test, stmt* body, stmt* orelse)\n",
      "          | With(withitem* items, stmt* body)\n",
      "          | AsyncWith(withitem* items, stmt* body)\n",
      "\n",
      "          | Raise(expr? exc, expr? cause)\n",
      "          | Try(stmt* body, excepthandler* handlers, stmt* orelse, stmt* finalbody)\n",
      "          | Assert(expr test, expr? msg)\n",
      "\n",
      "          | Import(alias* names)\n",
      "          | ImportFrom(identifier? module, alias* names, int? level)\n",
      "\n",
      "          | Global(identifier* names)\n",
      "          | Nonlocal(identifier* names)\n",
      "          | Expr(expr value)\n",
      "          | Pass\n",
      "          | Break\n",
      "          | Continue\n",
      "\n",
      "    expr = BoolOp(boolop op, expr* values)\n",
      "         | BinOp(expr left, operator op, expr right)\n",
      "         | UnaryOp(unaryop op, expr operand)\n",
      "         | Lambda(arguments args, expr body)\n",
      "         | IfExp(expr test, expr body, expr orelse)\n",
      "         | Dict(expr* keys, expr* values)\n",
      "         | Set(expr* elts)\n",
      "         | ListComp(expr elt, comprehension* generators)\n",
      "         | SetComp(expr elt, comprehension* generators)\n",
      "         | DictComp(expr key, expr value, comprehension* generators)\n",
      "         | GeneratorExp(expr elt, comprehension* generators)\n",
      "         | Await(expr value)\n",
      "         | Yield(expr? value)\n",
      "         | YieldFrom(expr value)\n",
      "         | Compare(expr left, cmpop* ops, expr* comparators)\n",
      "         | Call(expr func, expr* args, keyword* keywords)\n",
      "         | Num(object n) -- a number as a PyObject.\n",
      "         | Str(string s) -- need to specify raw, unicode, etc?\n",
      "         | FormattedValue(expr value, int? conversion, expr? format_spec)\n",
      "         | JoinedStr(expr* values)\n",
      "         | Bytes(bytes s)\n",
      "         | NameConstant(singleton value)\n",
      "         | Ellipsis\n",
      "         | Constant(constant value)\n",
      "\n",
      "         | Attribute(expr value, identifier attr, expr_context ctx)\n",
      "         | Subscript(expr value, slice slice, expr_context ctx)\n",
      "         | Starred(expr value, expr_context ctx)\n",
      "         | Name(identifier id, expr_context ctx)\n",
      "         | List(expr* elts, expr_context ctx)\n",
      "         | Tuple(expr* elts, expr_context ctx)\n",
      "\n",
      "    expr_context = Load | Store | Del | AugLoad | AugStore | Param\n",
      "\n",
      "    slice = Slice(expr? lower, expr? upper, expr? step)\n",
      "          | ExtSlice(slice* dims)\n",
      "          | Index(expr value)\n",
      "\n",
      "    boolop = And | Or\n",
      "\n",
      "    operator = Add | Sub | Mult | MatMult | Div | Mod | Pow | LShift\n",
      "                 | RShift | BitOr | BitXor | BitAnd | FloorDiv\n",
      "\n",
      "    unaryop = Invert | Not | UAdd | USub\n",
      "\n",
      "    cmpop = Eq | NotEq | Lt | LtE | Gt | GtE | Is | IsNot | In | NotIn\n",
      "\n",
      "    comprehension = comprehension(expr target, expr iter, expr* ifs, int is_async)\n",
      "\n",
      "    excepthandler = ExceptHandler(expr? type, identifier? name, stmt* body)\n",
      "\n",
      "    arguments = arguments(arg* args, arg? vararg, arg* kwonlyargs, expr* kw_defaults,\n",
      "                 arg? kwarg, expr* defaults)\n",
      "\n",
      "    arg = arg(identifier arg, expr? annotation)\n",
      "\n",
      "    keyword = keyword(identifier? arg, expr value)\n",
      "\n",
      "    alias = alias(identifier name, identifier? asname)\n",
      "\n",
      "    withitem = withitem(expr context_expr, expr? optional_vars)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "asdl_text = open('./asdl/grammar.txt').read()\n",
    "print(asdl_text)\n",
    "\n",
    "grammar, _, _ = Grammar.from_text(asdl_text)\n",
    "act_list = [GrammarRule(rule.constructor.name, rule.type.name, rule.fields) for rule in grammar]\n",
    "Reduce = ReduceAction('Reduce')\n",
    "act_dict = dict([(act.label, act) for act in act_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CodeSearchNet (Github)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "python_files = sorted(Path('./dataset/data_github/python/').glob('**/*.gz'))\n",
    "\n",
    "print(len(python_files))\n",
    "\n",
    "codesearchnet_columns=['docstring', 'docstring_tokens', 'code', 'code_tokens', 'partition']\n",
    "\n",
    "def jsonl_list_to_dataframe(file_list, columns=codesearchnet_columns):\n",
    "    \"\"\"Load a list of jsonl.gz files into a pandas DataFrame.\"\"\"\n",
    "    return pd.concat([pd.read_json(f, \n",
    "                                   orient='records', \n",
    "                                   compression='gzip',\n",
    "                                   lines=True)[columns] \n",
    "                      for f in file_list], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pydf = jsonl_list_to_dataframe(python_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracts video ID from URL.\n",
      "Extracts video ID from URL.\n",
      "Extracts video ID from URL.\n",
      "str->list\n",
      "    Convert XML to URL List.\n",
      "    From Biligrab.\n",
      "str->list\n",
      "    Convert XML to URL List.\n",
      "    From Biligrab.\n",
      "str->list\n",
      "    Convert XML to URL List.\n",
      "    From Biligrab.\n",
      "From http://cdn37.atwikiimg.com/sitescript/pub/dksitescript/FC2.site.js\n",
      "    Also com.hps.util.fc2.FC2EncrptUtil.makeMimiLocal\n",
      "    L110\n",
      "From http://cdn37.atwikiimg.com/sitescript/pub/dksitescript/FC2.site.js\n",
      "    Also com.hps.util.fc2.FC2EncrptUtil.makeMimiLocal\n",
      "    L110\n",
      "From http://cdn37.atwikiimg.com/sitescript/pub/dksitescript/FC2.site.js\n",
      "    Also com.hps.util.fc2.FC2EncrptUtil.makeMimiLocal\n",
      "    L110\n",
      "wrapper\n",
      "wrapper\n",
      "wrapper\n",
      "Downloads Dailymotion videos by URL.\n",
      "Downloads Dailymotion videos by URL.\n",
      "Downloads Dailymotion videos by URL.\n",
      "http://stackoverflow.com/a/30923963/2946714\n",
      "http://stackoverflow.com/a/30923963/2946714\n",
      "http://stackoverflow.com/a/30923963/2946714\n",
      "video page\n",
      "video page\n",
      "video page\n",
      "course page\n",
      "course page\n",
      "course page\n",
      "Downloads a Sina video by its unique vid.\n",
      "    http://video.sina.com.cn/\n",
      "Downloads a Sina video by its unique vid.\n",
      "    http://video.sina.com.cn/\n",
      "Downloads a Sina video by its unique vid.\n",
      "    http://video.sina.com.cn/\n",
      "Downloads a Sina video by its unique vkey.\n",
      "    http://video.sina.com/\n",
      "Downloads a Sina video by its unique vkey.\n",
      "    http://video.sina.com/\n",
      "Downloads a Sina video by its unique vkey.\n",
      "    http://video.sina.com/\n",
      "Downloads Sina videos by URL.\n",
      "Downloads Sina videos by URL.\n",
      "Downloads Sina videos by URL.\n",
      "wrapper\n",
      "wrapper\n",
      "wrapper\n",
      "Get item_id\n",
      "Get item_id\n",
      "Get item_id\n",
      "Source: Android mobile\n",
      "Source: Android mobile\n",
      "Source: Android mobile\n",
      "self, str->None\n",
      "        \n",
      "        Keyword arguments:\n",
      "        self: self\n",
      "        vid: The video ID for BokeCC cloud, something like\n",
      "        FE3BB999594978049C33DC5901307461\n",
      "        \n",
      "        Calls the prepare() to download the video.\n",
      "        \n",
      "        If no title is provided, this method shall try to find a proper title\n",
      "        with the information providin within the\n",
      "        returned content of the API.\n",
      "self, str->None\n",
      "        \n",
      "        Keyword arguments:\n",
      "        self: self\n",
      "        vid: The video ID for BokeCC cloud, something like\n",
      "        FE3BB999594978049C33DC5901307461\n",
      "        \n",
      "        Calls the prepare() to download the video.\n",
      "        \n",
      "        If no title is provided, this method shall try to find a proper title\n",
      "        with the information providin within the\n",
      "        returned content of the API.\n",
      "self, str->None\n",
      "        \n",
      "        Keyword arguments:\n",
      "        self: self\n",
      "        vid: The video ID for BokeCC cloud, something like\n",
      "        FE3BB999594978049C33DC5901307461\n",
      "        \n",
      "        Calls the prepare() to download the video.\n",
      "        \n",
      "        If no title is provided, this method shall try to find a proper title\n",
      "        with the information providin within the\n",
      "        returned content of the API.\n",
      "Extracts video ID from live.qq.com.\n",
      "Extracts video ID from live.qq.com.\n",
      "Extracts video ID from live.qq.com.\n",
      "Format text with color or other effects into ANSI escaped string.\n",
      "Format text with color or other effects into ANSI escaped string.\n",
      "Format text with color or other effects into ANSI escaped string.\n",
      "Print a log message to standard error.\n",
      "Print a log message to standard error.\n",
      "Print a log message to standard error.\n",
      "Print an error log message.\n",
      "Print an error log message.\n",
      "Print an error log message.\n",
      "What a Terrible Failure!\n",
      "What a Terrible Failure!\n",
      "What a Terrible Failure!\n",
      "Detect operating system.\n",
      "Detect operating system.\n",
      "Detect operating system.\n",
      "Source: Android mobile\n",
      "Source: Android mobile\n",
      "Source: Android mobile\n",
      "str->None\n",
      "str->None\n",
      "str->None\n",
      "str/int->None\n",
      "str/int->None\n",
      "str/int->None\n",
      "try:\n",
      "        # normal Vimeo video\n",
      "        html = get_content('https://vimeo.com/' + id)\n",
      "        cfg_patt = r'clip_page_config\\s*=\\s*(\\{.+?\\});'\n",
      "        cfg = json.loads(match1(html, cfg_patt))\n",
      "        video_page = get_content(cfg['player']['config_url'], headers=fake_headers)\n",
      "        title = cfg['clip']['title']\n",
      "        info = loads(video_page)\n",
      "    except:\n",
      "        # embedded player - referer may be required\n",
      "        if 'referer' in kwargs:\n",
      "            fake_headers['Referer'] = kwargs['referer']\n",
      "\n",
      "        video_page = get_content('http://player.vimeo.com/video/%s' % id, headers=fake_headers)\n",
      "        title = r1(r'<title>([^<]+)</title>', video_page)\n",
      "        info = loads(match1(video_page, r'var t=(\\{.+?\\});'))\n",
      "\n",
      "    streams = info['request']['files']['progressive']\n",
      "    streams = sorted(streams, key=lambda i: i['height'])\n",
      "    url = streams[-1]['url']\n",
      "\n",
      "    type, ext, size = url_info(url, faker=True)\n",
      "\n",
      "    print_info(site_info, title, type, size)\n",
      "    if not info_only:\n",
      "        download_urls([url], title, ext, size, output_dir, merge=merge, faker=True)\n",
      "try:\n",
      "        # normal Vimeo video\n",
      "        html = get_content('https://vimeo.com/' + id)\n",
      "        cfg_patt = r'clip_page_config\\s*=\\s*(\\{.+?\\});'\n",
      "        cfg = json.loads(match1(html, cfg_patt))\n",
      "        video_page = get_content(cfg['player']['config_url'], headers=fake_headers)\n",
      "        title = cfg['clip']['title']\n",
      "        info = loads(video_page)\n",
      "    except:\n",
      "        # embedded player - referer may be required\n",
      "        if 'referer' in kwargs:\n",
      "            fake_headers['Referer'] = kwargs['referer']\n",
      "\n",
      "        video_page = get_content('http://player.vimeo.com/video/%s' % id, headers=fake_headers)\n",
      "        title = r1(r'<title>([^<]+)</title>', video_page)\n",
      "        info = loads(match1(video_page, r'var t=(\\{.+?\\});'))\n",
      "\n",
      "    streams = info['request']['files']['progressive']\n",
      "    streams = sorted(streams, key=lambda i: i['height'])\n",
      "    url = streams[-1]['url']\n",
      "\n",
      "    type, ext, size = url_info(url, faker=True)\n",
      "\n",
      "    print_info(site_info, title, type, size)\n",
      "    if not info_only:\n",
      "        download_urls([url], title, ext, size, output_dir, merge=merge, faker=True)\n",
      "try:\n",
      "        # normal Vimeo video\n",
      "        html = get_content('https://vimeo.com/' + id)\n",
      "        cfg_patt = r'clip_page_config\\s*=\\s*(\\{.+?\\});'\n",
      "        cfg = json.loads(match1(html, cfg_patt))\n",
      "        video_page = get_content(cfg['player']['config_url'], headers=fake_headers)\n",
      "        title = cfg['clip']['title']\n",
      "        info = loads(video_page)\n",
      "    except:\n",
      "        # embedded player - referer may be required\n",
      "        if 'referer' in kwargs:\n",
      "            fake_headers['Referer'] = kwargs['referer']\n",
      "\n",
      "        video_page = get_content('http://player.vimeo.com/video/%s' % id, headers=fake_headers)\n",
      "        title = r1(r'<title>([^<]+)</title>', video_page)\n",
      "        info = loads(match1(video_page, r'var t=(\\{.+?\\});'))\n",
      "\n",
      "    streams = info['request']['files']['progressive']\n",
      "    streams = sorted(streams, key=lambda i: i['height'])\n",
      "    url = streams[-1]['url']\n",
      "\n",
      "    type, ext, size = url_info(url, faker=True)\n",
      "\n",
      "    print_info(site_info, title, type, size)\n",
      "    if not info_only:\n",
      "        download_urls([url], title, ext, size, output_dir, merge=merge, faker=True)\n",
      "str->dict\n",
      "    Information for CKPlayer API content.\n",
      "str->dict\n",
      "    Information for CKPlayer API content.\n",
      "str->dict\n",
      "    Information for CKPlayer API content.\n",
      "Splicing URLs according to video ID to get video details\n",
      "Splicing URLs according to video ID to get video details\n",
      "Splicing URLs according to video ID to get video details\n",
      "Extracts video ID from URL.\n",
      "Extracts video ID from URL.\n",
      "Extracts video ID from URL.\n",
      "str->list of str\n",
      "        Give you the real URLs.\n",
      "str->list of str\n",
      "        Give you the real URLs.\n",
      "str->list of str\n",
      "        Give you the real URLs.\n",
      "Get (branch, commit) from HEAD of a git repo.\n",
      "Get (branch, commit) from HEAD of a git repo.\n",
      "Get (branch, commit) from HEAD of a git repo.\n",
      "Converts a string to a valid filename.\n",
      "Converts a string to a valid filename.\n",
      "Converts a string to a valid filename.\n",
      "Get (width, height) of the current terminal.\n",
      "Get (width, height) of the current terminal.\n",
      "Get (width, height) of the current terminal.\n",
      "Downloads CBS videos by URL.\n",
      "Downloads CBS videos by URL.\n",
      "Downloads CBS videos by URL.\n",
      "Override the original one\n",
      "        Ugly ugly dirty hack\n",
      "Override the original one\n",
      "        Ugly ugly dirty hack\n",
      "Override the original one\n",
      "        Ugly ugly dirty hack\n",
      "str, str, str, bool, bool ->None\n",
      "\n",
      "    Download Acfun video by vid.\n",
      "\n",
      "    Call Acfun API, decide which site to use, and pass the job to its\n",
      "    extractor.\n",
      "str, str, str, bool, bool ->None\n",
      "\n",
      "    Download Acfun video by vid.\n",
      "\n",
      "    Call Acfun API, decide which site to use, and pass the job to its\n",
      "    extractor.\n",
      "str, str, str, bool, bool ->None\n",
      "\n",
      "    Download Acfun video by vid.\n",
      "\n",
      "    Call Acfun API, decide which site to use, and pass the job to its\n",
      "    extractor.\n",
      "Main entry point.\n",
      "    you-get-dev\n",
      "Main entry point.\n",
      "    you-get-dev\n",
      "Main entry point.\n",
      "    you-get-dev\n",
      "str, str->True\n",
      "    WARNING: NOT THE SAME PARMS AS OTHER FUNCTIONS!!!!!!\n",
      "    You can basicly download anything with this function\n",
      "    but better leave it alone with\n",
      "str, str->True\n",
      "    WARNING: NOT THE SAME PARMS AS OTHER FUNCTIONS!!!!!!\n",
      "    You can basicly download anything with this function\n",
      "    but better leave it alone with\n",
      "str, str->True\n",
      "    WARNING: NOT THE SAME PARMS AS OTHER FUNCTIONS!!!!!!\n",
      "    You can basicly download anything with this function\n",
      "    but better leave it alone with\n",
      "Scans through a string for substrings matched some patterns (first-subgroups only).\n",
      "\n",
      "    Args:\n",
      "        text: A string to be scanned.\n",
      "        patterns: Arbitrary number of regex patterns.\n",
      "\n",
      "    Returns:\n",
      "        When only one pattern is given, returns a string (None if no match found).\n",
      "        When more than one pattern are given, returns a list of strings ([] if no match found).\n",
      "Scans through a string for substrings matched some patterns (first-subgroups only).\n",
      "\n",
      "    Args:\n",
      "        text: A string to be scanned.\n",
      "        patterns: Arbitrary number of regex patterns.\n",
      "\n",
      "    Returns:\n",
      "        When only one pattern is given, returns a string (None if no match found).\n",
      "        When more than one pattern are given, returns a list of strings ([] if no match found).\n",
      "{'text': '`text`', 'patterns': '`patterns`'}\n",
      "Scans through a string for substrings matched some `patterns` (first-subgroups only).\n",
      "\n",
      "    Args:\n",
      "        `text`: A string to be scanned.\n",
      "        `patterns`: Arbitrary number of regex `patterns`.\n",
      "\n",
      "    Returns:\n",
      "        When only one pattern is given, returns a string (None if no match found).\n",
      "        When more than one pattern are given, returns a list of strings ([] if no match found).\n",
      "Scans through a string for substrings matched some patterns.\n",
      "\n",
      "    Args:\n",
      "        text: A string to be scanned.\n",
      "        patterns: a list of regex pattern.\n",
      "\n",
      "    Returns:\n",
      "        a list if matched. empty if not.\n",
      "Scans through a string for substrings matched some patterns.\n",
      "\n",
      "    Args:\n",
      "        text: A string to be scanned.\n",
      "        patterns: a list of regex pattern.\n",
      "\n",
      "    Returns:\n",
      "        a list if matched. empty if not.\n",
      "{'text': '`text`', 'patterns': '`patterns`'}\n",
      "Scans through a string for substrings matched some `patterns`.\n",
      "\n",
      "    Args:\n",
      "        `text`: A string to be scanned.\n",
      "        `patterns`: a list of regex pattern.\n",
      "\n",
      "    Returns:\n",
      "        a list if matched. empty if not.\n",
      "Parses the query string of a URL and returns the value of a parameter.\n",
      "\n",
      "    Args:\n",
      "        url: A URL.\n",
      "        param: A string representing the name of the parameter.\n",
      "\n",
      "    Returns:\n",
      "        The value of the parameter.\n",
      "Parses the query string of a URL and returns the value of a parameter.\n",
      "\n",
      "    Args:\n",
      "        url: A URL.\n",
      "        param: A string representing the name of the parameter.\n",
      "\n",
      "    Returns:\n",
      "        The value of the parameter.\n",
      "{'url': '`url`', 'param': '`param`'}\n",
      "Parses the query string of a URL and returns the value of a `param`eter.\n",
      "\n",
      "    Args:\n",
      "        `url`: A URL.\n",
      "        `param`: A string representing the name of the `param`eter.\n",
      "\n",
      "    Returns:\n",
      "        The value of the `param`eter.\n",
      "Decompresses data for Content-Encoding: gzip.\n",
      "Decompresses data for Content-Encoding: gzip.\n",
      "Decompresses data for Content-Encoding: gzip.\n",
      "Decompresses data for Content-Encoding: deflate.\n",
      "    (the zlib compression is used.)\n",
      "Decompresses data for Content-Encoding: deflate.\n",
      "    (the zlib compression is used.)\n",
      "Decompresses data for Content-Encoding: deflate.\n",
      "    (the zlib compression is used.)\n",
      "Gets the content of a URL via sending a HTTP GET request.\n",
      "\n",
      "    Args:\n",
      "        url: A URL.\n",
      "        headers: Request headers used by the client.\n",
      "        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n",
      "\n",
      "    Returns:\n",
      "        The content as a string.\n",
      "Gets the content of a URL via sending a HTTP GET request.\n",
      "\n",
      "    Args:\n",
      "        url: A URL.\n",
      "        headers: Request headers used by the client.\n",
      "        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n",
      "\n",
      "    Returns:\n",
      "        The content as a string.\n",
      "{'url': '`url`', 'headers': '`headers`', 'decoded': '`decoded`'}\n",
      "Gets the content of a URL via sending a HTTP GET request.\n",
      "\n",
      "    Args:\n",
      "        `url`: A URL.\n",
      "        `headers`: Request `headers` used by the client.\n",
      "        `decoded`: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n",
      "\n",
      "    Returns:\n",
      "        The content as a string.\n",
      "Post the content of a URL via sending a HTTP POST request.\n",
      "\n",
      "    Args:\n",
      "        url: A URL.\n",
      "        headers: Request headers used by the client.\n",
      "        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n",
      "\n",
      "    Returns:\n",
      "        The content as a string.\n",
      "Post the content of a URL via sending a HTTP POST request.\n",
      "\n",
      "    Args:\n",
      "        url: A URL.\n",
      "        headers: Request headers used by the client.\n",
      "        decoded: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n",
      "\n",
      "    Returns:\n",
      "        The content as a string.\n",
      "{'url': '`url`', 'headers': '`headers`', 'decoded': '`decoded`'}\n",
      "Post the content of a URL via sending a HTTP POST request.\n",
      "\n",
      "    Args:\n",
      "        `url`: A URL.\n",
      "        `headers`: Request `headers` used by the client.\n",
      "        `decoded`: Whether decode the response body using UTF-8 or the charset specified in Content-Type.\n",
      "\n",
      "    Returns:\n",
      "        The content as a string.\n",
      "Parses host name and port number from a string.\n",
      "Parses host name and port number from a string.\n",
      "Parses host name and port number from a string.\n",
      "Overload default print function as py (<3.3) does not support 'flush' keyword.\n",
      "    Although the function name can be same as print to get itself overloaded automatically,\n",
      "    I'd rather leave it with a different name and only overload it when importing to make less confusion.\n",
      "Overload default print function as py (<3.3) does not support 'flush' keyword.\n",
      "    Although the function name can be same as print to get itself overloaded automatically,\n",
      "    I'd rather leave it with a different name and only overload it when importing to make less confusion.\n",
      "Overload default print function as py (<3.3) does not support 'flush' keyword.\n",
      "    Although the function name can be same as print to get itself overloaded automatically,\n",
      "    I'd rather leave it with a different name and only overload it when importing to make less confusion.\n",
      "str->str\n",
      "str->str\n",
      "str->str\n",
      "Source: Android mobile\n",
      "Source: Android mobile\n",
      "Source: Android mobile\n",
      "JSON, int, int, int->str\n",
      "    \n",
      "    Get a proper title with courseid+topicID+partID.\n",
      "JSON, int, int, int->str\n",
      "    \n",
      "    Get a proper title with courseid+topicID+partID.\n",
      "JSON, int, int, int->str\n",
      "    \n",
      "    Get a proper title with courseid+topicID+partID.\n",
      "int->None\n",
      "    \n",
      "    Download a WHOLE course.\n",
      "    Reuse the API call to save time.\n",
      "int->None\n",
      "    \n",
      "    Download a WHOLE course.\n",
      "    Reuse the API call to save time.\n",
      "int->None\n",
      "    \n",
      "    Download a WHOLE course.\n",
      "    Reuse the API call to save time.\n",
      "int, int, int->None\n",
      "    \n",
      "    Download ONE PART of the course.\n",
      "int, int, int->None\n",
      "    \n",
      "    Download ONE PART of the course.\n",
      "int, int, int->None\n",
      "    \n",
      "    Download ONE PART of the course.\n",
      "int, int->list\n",
      "        \n",
      "        Get the height of the videos.\n",
      "        \n",
      "        Since brightcove is using 3 kinds of links: rtmp, http and https,\n",
      "        we will be using the HTTPS one to make it secure.\n",
      "        \n",
      "        If somehow akamaihd.net is blocked by the Great Fucking Wall,\n",
      "        change the \"startswith https\" to http.\n",
      "int, int->list\n",
      "        \n",
      "        Get the height of the videos.\n",
      "        \n",
      "        Since brightcove is using 3 kinds of links: rtmp, http and https,\n",
      "        we will be using the HTTPS one to make it secure.\n",
      "        \n",
      "        If somehow akamaihd.net is blocked by the Great Fucking Wall,\n",
      "        change the \"startswith https\" to http.\n",
      "int, int->list\n",
      "        \n",
      "        Get the height of the videos.\n",
      "        \n",
      "        Since brightcove is using 3 kinds of links: rtmp, http and https,\n",
      "        we will be using the HTTPS one to make it secure.\n",
      "        \n",
      "        If somehow akamaihd.net is blocked by the Great Fucking Wall,\n",
      "        change the \"startswith https\" to http.\n",
      "Checks if a task is either queued or running in this executor\n",
      "\n",
      "        :param task_instance: TaskInstance\n",
      "        :return: True if the task is known to this executor\n",
      "Checks if a task is either queued or running in this executor\n",
      "\n",
      "        :param task_instance: TaskInstance\n",
      "        :return: True if the task is known to this executor\n",
      "{'task_instance': '`task_instance`'}\n",
      "Checks if a task is either queued or running in this executor\n",
      "\n",
      "        :param `task_instance`: TaskInstance\n",
      "        :return: True if the task is known to this executor\n",
      "Returns and flush the event buffer. In case dag_ids is specified\n",
      "        it will only return and flush events for the given dag_ids. Otherwise\n",
      "        it returns and flushes all\n",
      "\n",
      "        :param dag_ids: to dag_ids to return events for, if None returns all\n",
      "        :return: a dict of events\n",
      "Returns and flush the event buffer. In case dag_ids is specified\n",
      "        it will only return and flush events for the given dag_ids. Otherwise\n",
      "        it returns and flushes all\n",
      "\n",
      "        :param dag_ids: to dag_ids to return events for, if None returns all\n",
      "        :return: a dict of events\n",
      "{'dag_ids': '`dag_ids`'}\n",
      "Returns and flush the event buffer. In case `dag_ids` is specified\n",
      "        it will only return and flush events for the given `dag_ids`. Otherwise\n",
      "        it returns and flushes all\n",
      "\n",
      "        :param `dag_ids`: to `dag_ids` to return events for, if None returns all\n",
      "        :return: a dict of events\n",
      "one method to fetch connection params as a dict\n",
      "        used in get_uri() and get_connection()\n",
      "one method to fetch connection params as a dict\n",
      "        used in get_uri() and get_connection()\n",
      "one method to fetch connection params as a dict\n",
      "        used in get_uri() and get_connection()\n",
      "override DbApiHook get_uri method for get_sqlalchemy_engine()\n",
      "override DbApiHook get_uri method for get_sqlalchemy_engine()\n",
      "override DbApiHook get_uri method for get_sqlalchemy_engine()\n",
      "Returns a snowflake.connection object\n",
      "Returns a snowflake.connection object\n",
      "Returns a snowflake.connection object\n",
      "returns aws_access_key_id, aws_secret_access_key\n",
      "        from extra\n",
      "\n",
      "        intended to be used by external import and export statements\n",
      "returns aws_access_key_id, aws_secret_access_key\n",
      "        from extra\n",
      "\n",
      "        intended to be used by external import and export statements\n",
      "returns aws_access_key_id, aws_secret_access_key\n",
      "        from extra\n",
      "\n",
      "        intended to be used by external import and export statements\n",
      "Fetches a field from extras, and returns it. This is some Airflow\n",
      "        magic. The grpc hook type adds custom UI elements\n",
      "        to the hook page, which allow admins to specify scopes, credential pem files, etc.\n",
      "        They get formatted as shown below.\n",
      "Fetches a field from extras, and returns it. This is some Airflow\n",
      "        magic. The grpc hook type adds custom UI elements\n",
      "        to the hook page, which allow admins to specify scopes, credential pem files, etc.\n",
      "        They get formatted as shown below.\n",
      "Fetches a field from extras, and returns it. This is some Airflow\n",
      "        magic. The grpc hook type adds custom UI elements\n",
      "        to the hook page, which allow admins to specify scopes, credential pem files, etc.\n",
      "        They get formatted as shown below.\n",
      "Executes SQL using psycopg2 copy_expert method.\n",
      "        Necessary to execute COPY command without access to a superuser.\n",
      "\n",
      "        Note: if this method is called with a \"COPY FROM\" statement and\n",
      "        the specified input file does not exist, it creates an empty\n",
      "        file and no data is loaded, but the operation succeeds.\n",
      "        So if users want to be aware when the input file does not exist,\n",
      "        they have to check its existence by themselves.\n",
      "Executes SQL using psycopg2 copy_expert method.\n",
      "        Necessary to execute COPY command without access to a superuser.\n",
      "\n",
      "        Note: if this method is called with a \"COPY FROM\" statement and\n",
      "        the specified input file does not exist, it creates an empty\n",
      "        file and no data is loaded, but the operation succeeds.\n",
      "        So if users want to be aware when the input file does not exist,\n",
      "        they have to check its existence by themselves.\n",
      "Executes SQL using psycopg2 copy_expert method.\n",
      "        Necessary to execute COPY command without access to a superuser.\n",
      "\n",
      "        Note: if this method is called with a \"COPY FROM\" statement and\n",
      "        the specified input file does not exist, it creates an empty\n",
      "        file and no data is loaded, but the operation succeeds.\n",
      "        So if users want to be aware when the input file does not exist,\n",
      "        they have to check its existence by themselves.\n",
      "Loads a tab-delimited file into a database table\n",
      "Loads a tab-delimited file into a database table\n",
      "Loads a tab-delimited file into a database table\n",
      "Dumps a database table into a tab-delimited file\n",
      "Dumps a database table into a tab-delimited file\n",
      "Dumps a database table into a tab-delimited file\n",
      "Uploads the file to Google cloud storage\n",
      "Uploads the file to Google cloud storage\n",
      "Uploads the file to Google cloud storage\n",
      "Gets the max partition for a table.\n",
      "\n",
      "    :param schema: The hive schema the table lives in\n",
      "    :type schema: str\n",
      "    :param table: The hive table you are interested in, supports the dot\n",
      "        notation as in \"my_database.my_table\", if a dot is found,\n",
      "        the schema param is disregarded\n",
      "    :type table: str\n",
      "    :param metastore_conn_id: The hive connection you are interested in.\n",
      "        If your default is set you don't need to use this parameter.\n",
      "    :type metastore_conn_id: str\n",
      "    :param filter_map: partition_key:partition_value map used for partition filtering,\n",
      "                       e.g. {'key1': 'value1', 'key2': 'value2'}.\n",
      "                       Only partitions matching all partition_key:partition_value\n",
      "                       pairs will be considered as candidates of max partition.\n",
      "    :type filter_map: map\n",
      "    :param field: the field to get the max value from. If there's only\n",
      "        one partition field, this will be inferred\n",
      "    :type field: str\n",
      "\n",
      "    >>> max_partition('airflow.static_babynames_partitioned')\n",
      "    '2015-01-01'\n",
      "Gets the max partition for a table.\n",
      "\n",
      "    :param schema: The hive schema the table lives in\n",
      "    :type schema: str\n",
      "    :param table: The hive table you are interested in, supports the dot\n",
      "        notation as in \"my_database.my_table\", if a dot is found,\n",
      "        the schema param is disregarded\n",
      "    :type table: str\n",
      "    :param metastore_conn_id: The hive connection you are interested in.\n",
      "        If your default is set you don't need to use this parameter.\n",
      "    :type metastore_conn_id: str\n",
      "    :param filter_map: partition_key:partition_value map used for partition filtering,\n",
      "                       e.g. {'key1': 'value1', 'key2': 'value2'}.\n",
      "                       Only partitions matching all partition_key:partition_value\n",
      "                       pairs will be considered as candidates of max partition.\n",
      "    :type filter_map: map\n",
      "    :param field: the field to get the max value from. If there's only\n",
      "        one partition field, this will be inferred\n",
      "    :type field: str\n",
      "\n",
      "    >>> max_partition('airflow.static_babynames_partitioned')\n",
      "    '2015-01-01'\n",
      "{'schema': '`schema`', 'table': '`table`', 'metastore_conn_id': '`metastore_conn_id`', 'filter_map': '`filter_map`', 'field': '`field`'}\n",
      "Gets the max partition for a `table`.\n",
      "\n",
      "    :param `schema`: The hive `schema` the `table` lives in\n",
      "    :type `schema`: str\n",
      "    :param `table`: The hive `table` you are interested in, supports the dot\n",
      "        notation as in \"my_database.my_`table`\", if a dot is found,\n",
      "        the `schema` param is disregarded\n",
      "    :type `table`: str\n",
      "    :param `metastore_conn_id`: The hive connection you are interested in.\n",
      "        If your default is set you don't need to use this parameter.\n",
      "    :type `metastore_conn_id`: str\n",
      "    :param `filter_map`: partition_key:partition_value map used for partition filtering,\n",
      "                       e.g. {'key1': 'value1', 'key2': 'value2'}.\n",
      "                       Only partitions matching all partition_key:partition_value\n",
      "                       pairs will be considered as candidates of max partition.\n",
      "    :type `filter_map`: map\n",
      "    :param `field`: the `field` to get the max value from. If there's only\n",
      "        one partition `field`, this will be inferred\n",
      "    :type `field`: str\n",
      "\n",
      "    >>> max_partition('airflow.static_babynames_partitioned')\n",
      "    '2015-01-01'\n",
      "This function finds the date in a list closest to the target date.\n",
      "    An optional parameter can be given to get the closest before or after.\n",
      "\n",
      "    :param target_dt: The target date\n",
      "    :type target_dt: datetime.date\n",
      "    :param date_list: The list of dates to search\n",
      "    :type date_list: list[datetime.date]\n",
      "    :param before_target: closest before or after the target\n",
      "    :type before_target: bool or None\n",
      "    :returns: The closest date\n",
      "    :rtype: datetime.date or None\n",
      "This function finds the date in a list closest to the target date.\n",
      "    An optional parameter can be given to get the closest before or after.\n",
      "\n",
      "    :param target_dt: The target date\n",
      "    :type target_dt: datetime.date\n",
      "    :param date_list: The list of dates to search\n",
      "    :type date_list: list[datetime.date]\n",
      "    :param before_target: closest before or after the target\n",
      "    :type before_target: bool or None\n",
      "    :returns: The closest date\n",
      "    :rtype: datetime.date or None\n",
      "{'target_dt': '`target_dt`', 'date_list': '`date_list`', 'before_target': '`before_target`'}\n",
      "This function finds the date in a list closest to the target date.\n",
      "    An optional parameter can be given to get the closest before or after.\n",
      "\n",
      "    :param `target_dt`: The target date\n",
      "    :type `target_dt`: datetime.date\n",
      "    :param `date_list`: The list of dates to search\n",
      "    :type `date_list`: list[datetime.date]\n",
      "    :param `before_target`: closest before or after the target\n",
      "    :type `before_target`: bool or None\n",
      "    :returns: The closest date\n",
      "    :rtype: datetime.date or None\n",
      "This function finds the date in a list closest to the target date.\n",
      "    An optional parameter can be given to get the closest before or after.\n",
      "\n",
      "    :param table: A hive table name\n",
      "    :type table: str\n",
      "    :param ds: A datestamp ``%Y-%m-%d`` e.g. ``yyyy-mm-dd``\n",
      "    :type ds: list[datetime.date]\n",
      "    :param before: closest before (True), after (False) or either side of ds\n",
      "    :type before: bool or None\n",
      "    :returns: The closest date\n",
      "    :rtype: str or None\n",
      "\n",
      "    >>> tbl = 'airflow.static_babynames_partitioned'\n",
      "    >>> closest_ds_partition(tbl, '2015-01-02')\n",
      "    '2015-01-01'\n",
      "This function finds the date in a list closest to the target date.\n",
      "    An optional parameter can be given to get the closest before or after.\n",
      "\n",
      "    :param table: A hive table name\n",
      "    :type table: str\n",
      "    :param ds: A datestamp ``%Y-%m-%d`` e.g. ``yyyy-mm-dd``\n",
      "    :type ds: list[datetime.date]\n",
      "    :param before: closest before (True), after (False) or either side of ds\n",
      "    :type before: bool or None\n",
      "    :returns: The closest date\n",
      "    :rtype: str or None\n",
      "\n",
      "    >>> tbl = 'airflow.static_babynames_partitioned'\n",
      "    >>> closest_ds_partition(tbl, '2015-01-02')\n",
      "    '2015-01-01'\n",
      "{'table': '`table`', 'ds': '`ds`', 'before': '`before`'}\n",
      "This function fin`ds` the date in a list closest to the target date.\n",
      "    An optional parameter can be given to get the closest `before` or after.\n",
      "\n",
      "    :param `table`: A hive `table` name\n",
      "    :type `table`: str\n",
      "    :param `ds`: A datestamp ``%Y-%m-%d`` e.g. ``yyyy-mm-dd``\n",
      "    :type `ds`: list[datetime.date]\n",
      "    :param `before`: closest `before` (True), after (False) or either side of `ds`\n",
      "    :type `before`: bool or None\n",
      "    :returns: The closest date\n",
      "    :rtype: str or None\n",
      "\n",
      "    >>> tbl = 'airflow.static_babynames_partitioned'\n",
      "    >>> closest_`ds`_partition(tbl, '2015-01-02')\n",
      "    '2015-01-01'\n",
      "Returns a mysql connection object\n",
      "Returns a mysql connection object\n",
      "Returns a mysql connection object\n",
      "Loads a tab-delimited file into a database table\n",
      "Loads a tab-delimited file into a database table\n",
      "Loads a tab-delimited file into a database table\n",
      "Checks whether new objects have been uploaded and the inactivity_period\n",
      "        has passed and updates the state of the sensor accordingly.\n",
      "\n",
      "        :param current_num_objects: number of objects in bucket during last poke.\n",
      "        :type current_num_objects: int\n",
      "Checks whether new objects have been uploaded and the inactivity_period\n",
      "        has passed and updates the state of the sensor accordingly.\n",
      "\n",
      "        :param current_num_objects: number of objects in bucket during last poke.\n",
      "        :type current_num_objects: int\n",
      "{'current_num_objects': '`current_num_objects`'}\n",
      "Checks whether new objects have been uploaded and the inactivity_period\n",
      "        has passed and updates the state of the sensor accordingly.\n",
      "\n",
      "        :param `current_num_objects`: number of objects in bucket during last poke.\n",
      "        :type `current_num_objects`: int\n",
      "Helps debug deadlocks by printing stacktraces when this gets a SIGQUIT\n",
      "    e.g. kill -s QUIT <PID> or CTRL+\\\n",
      "Helps debug deadlocks by printing stacktraces when this gets a SIGQUIT\n",
      "    e.g. kill -s QUIT <PID> or CTRL+\\\n",
      "Helps debug deadlocks by printing stacktraces when this gets a SIGQUIT\n",
      "    e.g. kill -s QUIT <PID> or CTRL+\\\n",
      "Creates a dag run for the specified dag\n",
      "    :param args:\n",
      "    :return:\n",
      "Creates a dag run for the specified dag\n",
      "    :param args:\n",
      "    :return:\n",
      "{'args': '`args`'}\n",
      "Creates a dag run for the specified dag\n",
      "    :param `args`:\n",
      "    :return:\n",
      "Deletes all DB records related to the specified dag\n",
      "    :param args:\n",
      "    :return:\n",
      "Deletes all DB records related to the specified dag\n",
      "    :param args:\n",
      "    :return:\n",
      "{'args': '`args`'}\n",
      "Deletes all DB records related to the specified dag\n",
      "    :param `args`:\n",
      "    :return:\n",
      "Returns the unmet dependencies for a task instance from the perspective of the\n",
      "    scheduler (i.e. why a task instance doesn't get scheduled and then queued by the\n",
      "    scheduler, and then run by an executor).\n",
      "    >>> airflow task_failed_deps tutorial sleep 2015-01-01\n",
      "    Task instance dependencies not met:\n",
      "    Dagrun Running: Task instance's dagrun did not exist: Unknown reason\n",
      "    Trigger Rule: Task's trigger rule 'all_success' requires all upstream tasks\n",
      "    to have succeeded, but found 1 non-success(es).\n",
      "Returns the unmet dependencies for a task instance from the perspective of the\n",
      "    scheduler (i.e. why a task instance doesn't get scheduled and then queued by the\n",
      "    scheduler, and then run by an executor).\n",
      "    >>> airflow task_failed_deps tutorial sleep 2015-01-01\n",
      "    Task instance dependencies not met:\n",
      "    Dagrun Running: Task instance's dagrun did not exist: Unknown reason\n",
      "    Trigger Rule: Task's trigger rule 'all_success' requires all upstream tasks\n",
      "    to have succeeded, but found 1 non-success(es).\n",
      "Returns the unmet dependencies for a task instance from the perspective of the\n",
      "    scheduler (i.e. why a task instance doesn't get scheduled and then queued by the\n",
      "    scheduler, and then run by an executor).\n",
      "    >>> airflow task_failed_deps tutorial sleep 2015-01-01\n",
      "    Task instance dependencies not met:\n",
      "    Dagrun Running: Task instance's dagrun did not exist: Unknown reason\n",
      "    Trigger Rule: Task's trigger rule 'all_success' requires all upstream tasks\n",
      "    to have succeeded, but found 1 non-success(es).\n",
      "Returns the state of a TaskInstance at the command line.\n",
      "    >>> airflow task_state tutorial sleep 2015-01-01\n",
      "    success\n",
      "Returns the state of a TaskInstance at the command line.\n",
      "    >>> airflow task_state tutorial sleep 2015-01-01\n",
      "    success\n",
      "Returns the state of a TaskInstance at the command line.\n",
      "    >>> airflow task_state tutorial sleep 2015-01-01\n",
      "    success\n",
      "Returns the state of a DagRun at the command line.\n",
      "    >>> airflow dag_state tutorial 2015-01-01T00:00:00.000000\n",
      "    running\n",
      "Returns the state of a DagRun at the command line.\n",
      "    >>> airflow dag_state tutorial 2015-01-01T00:00:00.000000\n",
      "    running\n",
      "Returns the state of a DagRun at the command line.\n",
      "    >>> airflow dag_state tutorial 2015-01-01T00:00:00.000000\n",
      "    running\n",
      "Returns the next execution datetime of a DAG at the command line.\n",
      "    >>> airflow next_execution tutorial\n",
      "    2018-08-31 10:38:00\n",
      "Returns the next execution datetime of a DAG at the command line.\n",
      "    >>> airflow next_execution tutorial\n",
      "    2018-08-31 10:38:00\n",
      "Returns the next execution datetime of a DAG at the command line.\n",
      "    >>> airflow next_execution tutorial\n",
      "    2018-08-31 10:38:00\n",
      "Runs forever, monitoring the child processes of @gunicorn_master_proc and\n",
      "    restarting workers occasionally.\n",
      "    Each iteration of the loop traverses one edge of this state transition\n",
      "    diagram, where each state (node) represents\n",
      "    [ num_ready_workers_running / num_workers_running ]. We expect most time to\n",
      "    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.\n",
      "    The horizontal transition at ? happens after the new worker parses all the\n",
      "    dags (so it could take a while!)\n",
      "       V ────────────────────────────────────────────────────────────────────────┐\n",
      "    [n / n] ──TTIN──> [ [n, n+bs) / n + bs ]  ────?───> [n + bs / n + bs] ──TTOU─┘\n",
      "       ^                          ^───────────────┘\n",
      "       │\n",
      "       │      ┌────────────────v\n",
      "       └──────┴────── [ [0, n) / n ] <─── start\n",
      "    We change the number of workers by sending TTIN and TTOU to the gunicorn\n",
      "    master process, which increases and decreases the number of child workers\n",
      "    respectively. Gunicorn guarantees that on TTOU workers are terminated\n",
      "    gracefully and that the oldest worker is terminated.\n",
      "Runs forever, monitoring the child processes of @gunicorn_master_proc and\n",
      "    restarting workers occasionally.\n",
      "    Each iteration of the loop traverses one edge of this state transition\n",
      "    diagram, where each state (node) represents\n",
      "    [ num_ready_workers_running / num_workers_running ]. We expect most time to\n",
      "    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.\n",
      "    The horizontal transition at ? happens after the new worker parses all the\n",
      "    dags (so it could take a while!)\n",
      "       V ────────────────────────────────────────────────────────────────────────┐\n",
      "    [n / n] ──TTIN──> [ [n, n+bs) / n + bs ]  ────?───> [n + bs / n + bs] ──TTOU─┘\n",
      "       ^                          ^───────────────┘\n",
      "       │\n",
      "       │      ┌────────────────v\n",
      "       └──────┴────── [ [0, n) / n ] <─── start\n",
      "    We change the number of workers by sending TTIN and TTOU to the gunicorn\n",
      "    master process, which increases and decreases the number of child workers\n",
      "    respectively. Gunicorn guarantees that on TTOU workers are terminated\n",
      "    gracefully and that the oldest worker is terminated.\n",
      "Runs forever, monitoring the child processes of @gunicorn_master_proc and\n",
      "    restarting workers occasionally.\n",
      "    Each iteration of the loop traverses one edge of this state transition\n",
      "    diagram, where each state (node) represents\n",
      "    [ num_ready_workers_running / num_workers_running ]. We expect most time to\n",
      "    be spent in [n / n]. `bs` is the setting webserver.worker_refresh_batch_size.\n",
      "    The horizontal transition at ? happens after the new worker parses all the\n",
      "    dags (so it could take a while!)\n",
      "       V ────────────────────────────────────────────────────────────────────────┐\n",
      "    [n / n] ──TTIN──> [ [n, n+bs) / n + bs ]  ────?───> [n + bs / n + bs] ──TTOU─┘\n",
      "       ^                          ^───────────────┘\n",
      "       │\n",
      "       │      ┌────────────────v\n",
      "       └──────┴────── [ [0, n) / n ] <─── start\n",
      "    We change the number of workers by sending TTIN and TTOU to the gunicorn\n",
      "    master process, which increases and decreases the number of child workers\n",
      "    respectively. Gunicorn guarantees that on TTOU workers are terminated\n",
      "    gracefully and that the oldest worker is terminated.\n",
      "Retrieves connection to Cloud Translate\n",
      "\n",
      "        :return: Google Cloud Translate client object.\n",
      "        :rtype: Client\n",
      "Retrieves connection to Cloud Translate\n",
      "\n",
      "        :return: Google Cloud Translate client object.\n",
      "        :rtype: Client\n",
      "Retrieves connection to Cloud Translate\n",
      "\n",
      "        :return: Google Cloud Translate client object.\n",
      "        :rtype: Client\n",
      "Translate a string or list of strings.\n",
      "\n",
      "        See https://cloud.google.com/translate/docs/translating-text\n",
      "\n",
      "        :type values: str or list\n",
      "        :param values: String or list of strings to translate.\n",
      "\n",
      "        :type target_language: str\n",
      "        :param target_language: The language to translate results into. This\n",
      "                                is required by the API and defaults to\n",
      "                                the target language of the current instance.\n",
      "\n",
      "        :type format_: str\n",
      "        :param format_: (Optional) One of ``text`` or ``html``, to specify\n",
      "                        if the input text is plain text or HTML.\n",
      "\n",
      "        :type source_language: str or None\n",
      "        :param source_language: (Optional) The language of the text to\n",
      "                                be translated.\n",
      "\n",
      "        :type model: str or None\n",
      "        :param model: (Optional) The model used to translate the text, such\n",
      "                      as ``'base'`` or ``'nmt'``.\n",
      "\n",
      "        :rtype: str or list\n",
      "        :returns: A list of dictionaries for each queried value. Each\n",
      "                  dictionary typically contains three keys (though not\n",
      "                  all will be present in all cases)\n",
      "\n",
      "                  * ``detectedSourceLanguage``: The detected language (as an\n",
      "                    ISO 639-1 language code) of the text.\n",
      "                  * ``translatedText``: The translation of the text into the\n",
      "                    target language.\n",
      "                  * ``input``: The corresponding input value.\n",
      "                  * ``model``: The model used to translate the text.\n",
      "\n",
      "                  If only a single value is passed, then only a single\n",
      "                  dictionary will be returned.\n",
      "        :raises: :class:`~exceptions.ValueError` if the number of\n",
      "                 values and translations differ.\n",
      "Translate a string or list of strings.\n",
      "\n",
      "        See https://cloud.google.com/translate/docs/translating-text\n",
      "\n",
      "        :type values: str or list\n",
      "        :param values: String or list of strings to translate.\n",
      "\n",
      "        :type target_language: str\n",
      "        :param target_language: The language to translate results into. This\n",
      "                                is required by the API and defaults to\n",
      "                                the target language of the current instance.\n",
      "\n",
      "        :type format_: str\n",
      "        :param format_: (Optional) One of ``text`` or ``html``, to specify\n",
      "                        if the input text is plain text or HTML.\n",
      "\n",
      "        :type source_language: str or None\n",
      "        :param source_language: (Optional) The language of the text to\n",
      "                                be translated.\n",
      "\n",
      "        :type model: str or None\n",
      "        :param model: (Optional) The model used to translate the text, such\n",
      "                      as ``'base'`` or ``'nmt'``.\n",
      "\n",
      "        :rtype: str or list\n",
      "        :returns: A list of dictionaries for each queried value. Each\n",
      "                  dictionary typically contains three keys (though not\n",
      "                  all will be present in all cases)\n",
      "\n",
      "                  * ``detectedSourceLanguage``: The detected language (as an\n",
      "                    ISO 639-1 language code) of the text.\n",
      "                  * ``translatedText``: The translation of the text into the\n",
      "                    target language.\n",
      "                  * ``input``: The corresponding input value.\n",
      "                  * ``model``: The model used to translate the text.\n",
      "\n",
      "                  If only a single value is passed, then only a single\n",
      "                  dictionary will be returned.\n",
      "        :raises: :class:`~exceptions.ValueError` if the number of\n",
      "                 values and translations differ.\n",
      "{'values': '`values`', 'target_language': '`target_language`', 'format_': '`format_`', 'source_language': '`source_language`', 'model': '`model`'}\n",
      "Translate a string or list of strings.\n",
      "\n",
      "        See https://cloud.google.com/translate/docs/translating-text\n",
      "\n",
      "        :type `values`: str or list\n",
      "        :param `values`: String or list of strings to translate.\n",
      "\n",
      "        :type `target_language`: str\n",
      "        :param `target_language`: The language to translate results into. This\n",
      "                                is required by the API and defaults to\n",
      "                                the target language of the current instance.\n",
      "\n",
      "        :type `format_`: str\n",
      "        :param `format_`: (Optional) One of ``text`` or ``html``, to specify\n",
      "                        if the input text is plain text or HTML.\n",
      "\n",
      "        :type `source_language`: str or None\n",
      "        :param `source_language`: (Optional) The language of the text to\n",
      "                                be translated.\n",
      "\n",
      "        :type `model`: str or None\n",
      "        :param `model`: (Optional) The `model` used to translate the text, such\n",
      "                      as ``'base'`` or ``'nmt'``.\n",
      "\n",
      "        :rtype: str or list\n",
      "        :returns: A list of dictionaries for each queried value. Each\n",
      "                  dictionary typically contains three keys (though not\n",
      "                  all will be present in all cases)\n",
      "\n",
      "                  * ``detectedSourceLanguage``: The detected language (as an\n",
      "                    ISO 639-1 language code) of the text.\n",
      "                  * ``translatedText``: The translation of the text into the\n",
      "                    target language.\n",
      "                  * ``input``: The corresponding input value.\n",
      "                  * ```model```: The `model` used to translate the text.\n",
      "\n",
      "                  If only a single value is passed, then only a single\n",
      "                  dictionary will be returned.\n",
      "        :raises: :class:`~exceptions.ValueError` if the number of\n",
      "                 `values` and translations differ.\n",
      "Execute the bash command in a temporary directory\n",
      "        which will be cleaned afterwards\n",
      "Execute the bash command in a temporary directory\n",
      "        which will be cleaned afterwards\n",
      "Execute the bash command in a temporary directory\n",
      "        which will be cleaned afterwards\n",
      "Retrieves a resource containing information about a Cloud SQL instance.\n",
      "\n",
      "        :param instance: Database instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: A Cloud SQL instance resource.\n",
      "        :rtype: dict\n",
      "Retrieves a resource containing information about a Cloud SQL instance.\n",
      "\n",
      "        :param instance: Database instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: A Cloud SQL instance resource.\n",
      "        :rtype: dict\n",
      "{'instance': '`instance`', 'project_id': '`project_id`'}\n",
      "Retrieves a resource containing information about a Cloud SQL `instance`.\n",
      "\n",
      "        :param `instance`: Database `instance` ID. This does not include the project ID.\n",
      "        :type `instance`: str\n",
      "        :param `project_id`: Project ID of the project that contains the `instance`. If set\n",
      "            to None or missing, the default `project_id` from the GCP connection is used.\n",
      "        :type `project_id`: str\n",
      "        :return: A Cloud SQL `instance` resource.\n",
      "        :rtype: dict\n",
      "Creates a new Cloud SQL instance.\n",
      "\n",
      "        :param body: Body required by the Cloud SQL insert API, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n",
      "        :type body: dict\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "Creates a new Cloud SQL instance.\n",
      "\n",
      "        :param body: Body required by the Cloud SQL insert API, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-body.\n",
      "        :type body: dict\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "{'body': '`body`', 'project_id': '`project_id`'}\n",
      "Creates a new Cloud SQL instance.\n",
      "\n",
      "        :param `body`: Body required by the Cloud SQL insert API, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/insert#request-`body`.\n",
      "        :type `body`: dict\n",
      "        :param `project_id`: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default `project_id` from the GCP connection is used.\n",
      "        :type `project_id`: str\n",
      "        :return: None\n",
      "Updates settings of a Cloud SQL instance.\n",
      "\n",
      "        Caution: This is not a partial update, so you must include values for\n",
      "        all the settings that you want to retain.\n",
      "\n",
      "        :param body: Body required by the Cloud SQL patch API, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n",
      "        :type body: dict\n",
      "        :param instance: Cloud SQL instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "Updates settings of a Cloud SQL instance.\n",
      "\n",
      "        Caution: This is not a partial update, so you must include values for\n",
      "        all the settings that you want to retain.\n",
      "\n",
      "        :param body: Body required by the Cloud SQL patch API, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/patch#request-body.\n",
      "        :type body: dict\n",
      "        :param instance: Cloud SQL instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "{'body': '`body`', 'instance': '`instance`', 'project_id': '`project_id`'}\n",
      "Updates settings of a Cloud SQL `instance`.\n",
      "\n",
      "        Caution: This is not a partial update, so you must include values for\n",
      "        all the settings that you want to retain.\n",
      "\n",
      "        :param `body`: Body required by the Cloud SQL patch API, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/`instance`s/patch#request-`body`.\n",
      "        :type `body`: dict\n",
      "        :param `instance`: Cloud SQL `instance` ID. This does not include the project ID.\n",
      "        :type `instance`: str\n",
      "        :param `project_id`: Project ID of the project that contains the `instance`. If set\n",
      "            to None or missing, the default `project_id` from the GCP connection is used.\n",
      "        :type `project_id`: str\n",
      "        :return: None\n",
      "Deletes a Cloud SQL instance.\n",
      "\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :param instance: Cloud SQL instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :return: None\n",
      "Deletes a Cloud SQL instance.\n",
      "\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :param instance: Cloud SQL instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :return: None\n",
      "{'project_id': '`project_id`', 'instance': '`instance`'}\n",
      "Deletes a Cloud SQL `instance`.\n",
      "\n",
      "        :param `project_id`: Project ID of the project that contains the `instance`. If set\n",
      "            to None or missing, the default `project_id` from the GCP connection is used.\n",
      "        :type `project_id`: str\n",
      "        :param `instance`: Cloud SQL `instance` ID. This does not include the project ID.\n",
      "        :type `instance`: str\n",
      "        :return: None\n",
      "Retrieves a database resource from a Cloud SQL instance.\n",
      "\n",
      "        :param instance: Database instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param database: Name of the database in the instance.\n",
      "        :type database: str\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: A Cloud SQL database resource, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n",
      "        :rtype: dict\n",
      "Retrieves a database resource from a Cloud SQL instance.\n",
      "\n",
      "        :param instance: Database instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param database: Name of the database in the instance.\n",
      "        :type database: str\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: A Cloud SQL database resource, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases#resource.\n",
      "        :rtype: dict\n",
      "{'instance': '`instance`', 'database': '`database`', 'project_id': '`project_id`'}\n",
      "Retrieves a `database` resource from a Cloud SQL `instance`.\n",
      "\n",
      "        :param `instance`: Database `instance` ID. This does not include the project ID.\n",
      "        :type `instance`: str\n",
      "        :param `database`: Name of the `database` in the `instance`.\n",
      "        :type `database`: str\n",
      "        :param `project_id`: Project ID of the project that contains the `instance`. If set\n",
      "            to None or missing, the default `project_id` from the GCP connection is used.\n",
      "        :type `project_id`: str\n",
      "        :return: A Cloud SQL `database` resource, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/`database`s#resource.\n",
      "        :rtype: dict\n",
      "Creates a new database inside a Cloud SQL instance.\n",
      "\n",
      "        :param instance: Database instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param body: The request body, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n",
      "        :type body: dict\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "Creates a new database inside a Cloud SQL instance.\n",
      "\n",
      "        :param instance: Database instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param body: The request body, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n",
      "        :type body: dict\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "{'instance': '`instance`', 'body': '`body`', 'project_id': '`project_id`'}\n",
      "Creates a new database inside a Cloud SQL `instance`.\n",
      "\n",
      "        :param `instance`: Database `instance` ID. This does not include the project ID.\n",
      "        :type `instance`: str\n",
      "        :param `body`: The request `body`, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-`body`.\n",
      "        :type `body`: dict\n",
      "        :param `project_id`: Project ID of the project that contains the `instance`. If set\n",
      "            to None or missing, the default `project_id` from the GCP connection is used.\n",
      "        :type `project_id`: str\n",
      "        :return: None\n",
      "Updates a database resource inside a Cloud SQL instance.\n",
      "\n",
      "        This method supports patch semantics.\n",
      "        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n",
      "\n",
      "        :param instance: Database instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param database: Name of the database to be updated in the instance.\n",
      "        :type database: str\n",
      "        :param body: The request body, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n",
      "        :type body: dict\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "Updates a database resource inside a Cloud SQL instance.\n",
      "\n",
      "        This method supports patch semantics.\n",
      "        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n",
      "\n",
      "        :param instance: Database instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param database: Name of the database to be updated in the instance.\n",
      "        :type database: str\n",
      "        :param body: The request body, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/databases/insert#request-body.\n",
      "        :type body: dict\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "{'instance': '`instance`', 'database': '`database`', 'body': '`body`', 'project_id': '`project_id`'}\n",
      "Updates a `database` resource inside a Cloud SQL `instance`.\n",
      "\n",
      "        This method supports patch semantics.\n",
      "        See https://cloud.google.com/sql/docs/mysql/admin-api/how-tos/performance#patch.\n",
      "\n",
      "        :param `instance`: Database `instance` ID. This does not include the project ID.\n",
      "        :type `instance`: str\n",
      "        :param `database`: Name of the `database` to be updated in the `instance`.\n",
      "        :type `database`: str\n",
      "        :param `body`: The request `body`, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/`database`s/insert#request-`body`.\n",
      "        :type `body`: dict\n",
      "        :param `project_id`: Project ID of the project that contains the `instance`. If set\n",
      "            to None or missing, the default `project_id` from the GCP connection is used.\n",
      "        :type `project_id`: str\n",
      "        :return: None\n",
      "Deletes a database from a Cloud SQL instance.\n",
      "\n",
      "        :param instance: Database instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param database: Name of the database to be deleted in the instance.\n",
      "        :type database: str\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "Deletes a database from a Cloud SQL instance.\n",
      "\n",
      "        :param instance: Database instance ID. This does not include the project ID.\n",
      "        :type instance: str\n",
      "        :param database: Name of the database to be deleted in the instance.\n",
      "        :type database: str\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "{'instance': '`instance`', 'database': '`database`', 'project_id': '`project_id`'}\n",
      "Deletes a `database` from a Cloud SQL `instance`.\n",
      "\n",
      "        :param `instance`: Database `instance` ID. This does not include the project ID.\n",
      "        :type `instance`: str\n",
      "        :param `database`: Name of the `database` to be deleted in the `instance`.\n",
      "        :type `database`: str\n",
      "        :param `project_id`: Project ID of the project that contains the `instance`. If set\n",
      "            to None or missing, the default `project_id` from the GCP connection is used.\n",
      "        :type `project_id`: str\n",
      "        :return: None\n",
      "Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n",
      "        or CSV file.\n",
      "\n",
      "        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n",
      "            project ID.\n",
      "        :type instance: str\n",
      "        :param body: The request body, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n",
      "        :type body: dict\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "Exports data from a Cloud SQL instance to a Cloud Storage bucket as a SQL dump\n",
      "        or CSV file.\n",
      "\n",
      "        :param instance: Database instance ID of the Cloud SQL instance. This does not include the\n",
      "            project ID.\n",
      "        :type instance: str\n",
      "        :param body: The request body, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/instances/export#request-body\n",
      "        :type body: dict\n",
      "        :param project_id: Project ID of the project that contains the instance. If set\n",
      "            to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "{'instance': '`instance`', 'body': '`body`', 'project_id': '`project_id`'}\n",
      "Exports data from a Cloud SQL `instance` to a Cloud Storage bucket as a SQL dump\n",
      "        or CSV file.\n",
      "\n",
      "        :param `instance`: Database `instance` ID of the Cloud SQL `instance`. This does not include the\n",
      "            project ID.\n",
      "        :type `instance`: str\n",
      "        :param `body`: The request `body`, as described in\n",
      "            https://cloud.google.com/sql/docs/mysql/admin-api/v1beta4/`instance`s/export#request-`body`\n",
      "        :type `body`: dict\n",
      "        :param `project_id`: Project ID of the project that contains the `instance`. If set\n",
      "            to None or missing, the default `project_id` from the GCP connection is used.\n",
      "        :type `project_id`: str\n",
      "        :return: None\n",
      "Waits for the named operation to complete - checks status of the\n",
      "        asynchronous call.\n",
      "\n",
      "        :param project_id: Project ID of the project that contains the instance.\n",
      "        :type project_id: str\n",
      "        :param operation_name: Name of the operation.\n",
      "        :type operation_name: str\n",
      "        :return: None\n",
      "Waits for the named operation to complete - checks status of the\n",
      "        asynchronous call.\n",
      "\n",
      "        :param project_id: Project ID of the project that contains the instance.\n",
      "        :type project_id: str\n",
      "        :param operation_name: Name of the operation.\n",
      "        :type operation_name: str\n",
      "        :return: None\n",
      "{'project_id': '`project_id`', 'operation_name': '`operation_name`'}\n",
      "Waits for the named operation to complete - checks status of the\n",
      "        asynchronous call.\n",
      "\n",
      "        :param `project_id`: Project ID of the project that contains the instance.\n",
      "        :type `project_id`: str\n",
      "        :param `operation_name`: Name of the operation.\n",
      "        :type `operation_name`: str\n",
      "        :return: None\n",
      "Starts Cloud SQL Proxy.\n",
      "\n",
      "        You have to remember to stop the proxy if you started it!\n",
      "Starts Cloud SQL Proxy.\n",
      "\n",
      "        You have to remember to stop the proxy if you started it!\n",
      "Starts Cloud SQL Proxy.\n",
      "\n",
      "        You have to remember to stop the proxy if you started it!\n",
      "Stops running proxy.\n",
      "\n",
      "        You should stop the proxy after you stop using it.\n",
      "Stops running proxy.\n",
      "\n",
      "        You should stop the proxy after you stop using it.\n",
      "Stops running proxy.\n",
      "\n",
      "        You should stop the proxy after you stop using it.\n",
      "Returns version of the Cloud SQL Proxy.\n",
      "Returns version of the Cloud SQL Proxy.\n",
      "Returns version of the Cloud SQL Proxy.\n",
      "Create connection in the Connection table, according to whether it uses\n",
      "        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n",
      "\n",
      "        :param session: Session of the SQL Alchemy ORM (automatically generated with\n",
      "                        decorator).\n",
      "Create connection in the Connection table, according to whether it uses\n",
      "        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n",
      "\n",
      "        :param session: Session of the SQL Alchemy ORM (automatically generated with\n",
      "                        decorator).\n",
      "{'session': '`session`'}\n",
      "Create connection in the Connection table, according to whether it uses\n",
      "        proxy, TCP, UNIX sockets, SSL. Connection ID will be randomly generated.\n",
      "\n",
      "        :param `session`: Session of the SQL Alchemy ORM (automatically generated with\n",
      "                        decorator).\n",
      "Retrieves the dynamically created connection from the Connection table.\n",
      "\n",
      "        :param session: Session of the SQL Alchemy ORM (automatically generated with\n",
      "                        decorator).\n",
      "Retrieves the dynamically created connection from the Connection table.\n",
      "\n",
      "        :param session: Session of the SQL Alchemy ORM (automatically generated with\n",
      "                        decorator).\n",
      "{'session': '`session`'}\n",
      "Retrieves the dynamically created connection from the Connection table.\n",
      "\n",
      "        :param `session`: Session of the SQL Alchemy ORM (automatically generated with\n",
      "                        decorator).\n",
      "Delete the dynamically created connection from the Connection table.\n",
      "\n",
      "        :param session: Session of the SQL Alchemy ORM (automatically generated with\n",
      "                        decorator).\n",
      "Delete the dynamically created connection from the Connection table.\n",
      "\n",
      "        :param session: Session of the SQL Alchemy ORM (automatically generated with\n",
      "                        decorator).\n",
      "{'session': '`session`'}\n",
      "Delete the dynamically created connection from the Connection table.\n",
      "\n",
      "        :param `session`: Session of the SQL Alchemy ORM (automatically generated with\n",
      "                        decorator).\n",
      "Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n",
      "        lifecycle per task.\n",
      "\n",
      "        :return: The Cloud SQL Proxy runner.\n",
      "        :rtype: CloudSqlProxyRunner\n",
      "Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n",
      "        lifecycle per task.\n",
      "\n",
      "        :return: The Cloud SQL Proxy runner.\n",
      "        :rtype: CloudSqlProxyRunner\n",
      "Retrieve Cloud SQL Proxy runner. It is used to manage the proxy\n",
      "        lifecycle per task.\n",
      "\n",
      "        :return: The Cloud SQL Proxy runner.\n",
      "        :rtype: CloudSqlProxyRunner\n",
      "Retrieve database hook. This is the actual Postgres or MySQL database hook\n",
      "        that uses proxy or connects directly to the Google Cloud SQL database.\n",
      "Retrieve database hook. This is the actual Postgres or MySQL database hook\n",
      "        that uses proxy or connects directly to the Google Cloud SQL database.\n",
      "Retrieve database hook. This is the actual Postgres or MySQL database hook\n",
      "        that uses proxy or connects directly to the Google Cloud SQL database.\n",
      "Clean up database hook after it was used.\n",
      "Clean up database hook after it was used.\n",
      "Clean up database hook after it was used.\n",
      "Reserve free TCP port to be used by Cloud SQL Proxy\n",
      "Reserve free TCP port to be used by Cloud SQL Proxy\n",
      "Reserve free TCP port to be used by Cloud SQL Proxy\n",
      "Replaces invalid MLEngine job_id characters with '_'.\n",
      "\n",
      "    This also adds a leading 'z' in case job_id starts with an invalid\n",
      "    character.\n",
      "\n",
      "    Args:\n",
      "        job_id: A job_id str that may have invalid characters.\n",
      "\n",
      "    Returns:\n",
      "        A valid job_id representation.\n",
      "Replaces invalid MLEngine job_id characters with '_'.\n",
      "\n",
      "    This also adds a leading 'z' in case job_id starts with an invalid\n",
      "    character.\n",
      "\n",
      "    Args:\n",
      "        job_id: A job_id str that may have invalid characters.\n",
      "\n",
      "    Returns:\n",
      "        A valid job_id representation.\n",
      "{'job_id': '`job_id`'}\n",
      "Replaces invalid MLEngine `job_id` characters with '_'.\n",
      "\n",
      "    This also adds a leading 'z' in case `job_id` starts with an invalid\n",
      "    character.\n",
      "\n",
      "    Args:\n",
      "        `job_id`: A `job_id` str that may have invalid characters.\n",
      "\n",
      "    Returns:\n",
      "        A valid `job_id` representation.\n",
      "Extract error code from ftp exception\n",
      "Extract error code from ftp exception\n",
      "Extract error code from ftp exception\n",
      "Integrate plugins to the context\n",
      "Integrate plugins to the context\n",
      "Integrate plugins to the context\n",
      "Remove any existing DAG runs for the perf test DAGs.\n",
      "Remove any existing DAG runs for the perf test DAGs.\n",
      "Remove any existing DAG runs for the perf test DAGs.\n",
      "Remove any existing task instances for the perf test DAGs.\n",
      "Remove any existing task instances for the perf test DAGs.\n",
      "Remove any existing task instances for the perf test DAGs.\n",
      "Toggle the pause state of the DAGs in the test.\n",
      "Toggle the pause state of the DAGs in the test.\n",
      "Toggle the pause state of the DAGs in the test.\n",
      "Print operational metrics for the scheduler test.\n",
      "Print operational metrics for the scheduler test.\n",
      "Print operational metrics for the scheduler test.\n",
      "Override the scheduler heartbeat to determine when the test is complete\n",
      "Override the scheduler heartbeat to determine when the test is complete\n",
      "Override the scheduler heartbeat to determine when the test is complete\n",
      "Invoke Lambda Function\n",
      "Invoke Lambda Function\n",
      "Invoke Lambda Function\n",
      "Return the task object identified by the given dag_id and task_id.\n",
      "Return the task object identified by the given dag_id and task_id.\n",
      "Return the task object identified by the given dag_id and task_id.\n",
      "Creates Operators needed for model evaluation and returns.\n",
      "\n",
      "    It gets prediction over inputs via Cloud ML Engine BatchPrediction API by\n",
      "    calling MLEngineBatchPredictionOperator, then summarize and validate\n",
      "    the result via Cloud Dataflow using DataFlowPythonOperator.\n",
      "\n",
      "    For details and pricing about Batch prediction, please refer to the website\n",
      "    https://cloud.google.com/ml-engine/docs/how-tos/batch-predict\n",
      "    and for Cloud Dataflow, https://cloud.google.com/dataflow/docs/\n",
      "\n",
      "    It returns three chained operators for prediction, summary, and validation,\n",
      "    named as <prefix>-prediction, <prefix>-summary, and <prefix>-validation,\n",
      "    respectively.\n",
      "    (<prefix> should contain only alphanumeric characters or hyphen.)\n",
      "\n",
      "    The upstream and downstream can be set accordingly like:\n",
      "      pred, _, val = create_evaluate_ops(...)\n",
      "      pred.set_upstream(upstream_op)\n",
      "      ...\n",
      "      downstream_op.set_upstream(val)\n",
      "\n",
      "    Callers will provide two python callables, metric_fn and validate_fn, in\n",
      "    order to customize the evaluation behavior as they wish.\n",
      "    - metric_fn receives a dictionary per instance derived from json in the\n",
      "      batch prediction result. The keys might vary depending on the model.\n",
      "      It should return a tuple of metrics.\n",
      "    - validation_fn receives a dictionary of the averaged metrics that metric_fn\n",
      "      generated over all instances.\n",
      "      The key/value of the dictionary matches to what's given by\n",
      "      metric_fn_and_keys arg.\n",
      "      The dictionary contains an additional metric, 'count' to represent the\n",
      "      total number of instances received for evaluation.\n",
      "      The function would raise an exception to mark the task as failed, in a\n",
      "      case the validation result is not okay to proceed (i.e. to set the trained\n",
      "      version as default).\n",
      "\n",
      "    Typical examples are like this:\n",
      "\n",
      "    def get_metric_fn_and_keys():\n",
      "        import math  # imports should be outside of the metric_fn below.\n",
      "        def error_and_squared_error(inst):\n",
      "            label = float(inst['input_label'])\n",
      "            classes = float(inst['classes'])  # 0 or 1\n",
      "            err = abs(classes-label)\n",
      "            squared_err = math.pow(classes-label, 2)\n",
      "            return (err, squared_err)  # returns a tuple.\n",
      "        return error_and_squared_error, ['err', 'mse']  # key order must match.\n",
      "\n",
      "    def validate_err_and_count(summary):\n",
      "        if summary['err'] > 0.2:\n",
      "            raise ValueError('Too high err>0.2; summary=%s' % summary)\n",
      "        if summary['mse'] > 0.05:\n",
      "            raise ValueError('Too high mse>0.05; summary=%s' % summary)\n",
      "        if summary['count'] < 1000:\n",
      "            raise ValueError('Too few instances<1000; summary=%s' % summary)\n",
      "        return summary\n",
      "\n",
      "    For the details on the other BatchPrediction-related arguments (project_id,\n",
      "    job_id, region, data_format, input_paths, prediction_path, model_uri),\n",
      "    please refer to MLEngineBatchPredictionOperator too.\n",
      "\n",
      "    :param task_prefix: a prefix for the tasks. Only alphanumeric characters and\n",
      "        hyphen are allowed (no underscores), since this will be used as dataflow\n",
      "        job name, which doesn't allow other characters.\n",
      "    :type task_prefix: str\n",
      "\n",
      "    :param data_format: either of 'TEXT', 'TF_RECORD', 'TF_RECORD_GZIP'\n",
      "    :type data_format: str\n",
      "\n",
      "    :param input_paths: a list of input paths to be sent to BatchPrediction.\n",
      "    :type input_paths: list[str]\n",
      "\n",
      "    :param prediction_path: GCS path to put the prediction results in.\n",
      "    :type prediction_path: str\n",
      "\n",
      "    :param metric_fn_and_keys: a tuple of metric_fn and metric_keys:\n",
      "        - metric_fn is a function that accepts a dictionary (for an instance),\n",
      "          and returns a tuple of metric(s) that it calculates.\n",
      "        - metric_keys is a list of strings to denote the key of each metric.\n",
      "    :type metric_fn_and_keys: tuple of a function and a list[str]\n",
      "\n",
      "    :param validate_fn: a function to validate whether the averaged metric(s) is\n",
      "        good enough to push the model.\n",
      "    :type validate_fn: function\n",
      "\n",
      "    :param batch_prediction_job_id: the id to use for the Cloud ML Batch\n",
      "        prediction job. Passed directly to the MLEngineBatchPredictionOperator as\n",
      "        the job_id argument.\n",
      "    :type batch_prediction_job_id: str\n",
      "\n",
      "    :param project_id: the Google Cloud Platform project id in which to execute\n",
      "        Cloud ML Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n",
      "        `default_args['project_id']` will be used.\n",
      "    :type project_id: str\n",
      "\n",
      "    :param region: the Google Cloud Platform region in which to execute Cloud ML\n",
      "        Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n",
      "        `default_args['region']` will be used.\n",
      "    :type region: str\n",
      "\n",
      "    :param dataflow_options: options to run Dataflow jobs. If None, then the\n",
      "        `dag`'s `default_args['dataflow_default_options']` will be used.\n",
      "    :type dataflow_options: dictionary\n",
      "\n",
      "    :param model_uri: GCS path of the model exported by Tensorflow using\n",
      "        tensorflow.estimator.export_savedmodel(). It cannot be used with\n",
      "        model_name or version_name below. See MLEngineBatchPredictionOperator for\n",
      "        more detail.\n",
      "    :type model_uri: str\n",
      "\n",
      "    :param model_name: Used to indicate a model to use for prediction. Can be\n",
      "        used in combination with version_name, but cannot be used together with\n",
      "        model_uri. See MLEngineBatchPredictionOperator for more detail. If None,\n",
      "        then the `dag`'s `default_args['model_name']` will be used.\n",
      "    :type model_name: str\n",
      "\n",
      "    :param version_name: Used to indicate a model version to use for prediction,\n",
      "        in combination with model_name. Cannot be used together with model_uri.\n",
      "        See MLEngineBatchPredictionOperator for more detail. If None, then the\n",
      "        `dag`'s `default_args['version_name']` will be used.\n",
      "    :type version_name: str\n",
      "\n",
      "    :param dag: The `DAG` to use for all Operators.\n",
      "    :type dag: airflow.models.DAG\n",
      "\n",
      "    :returns: a tuple of three operators, (prediction, summary, validation)\n",
      "    :rtype: tuple(DataFlowPythonOperator, DataFlowPythonOperator,\n",
      "                  PythonOperator)\n",
      "Creates Operators needed for model evaluation and returns.\n",
      "\n",
      "    It gets prediction over inputs via Cloud ML Engine BatchPrediction API by\n",
      "    calling MLEngineBatchPredictionOperator, then summarize and validate\n",
      "    the result via Cloud Dataflow using DataFlowPythonOperator.\n",
      "\n",
      "    For details and pricing about Batch prediction, please refer to the website\n",
      "    https://cloud.google.com/ml-engine/docs/how-tos/batch-predict\n",
      "    and for Cloud Dataflow, https://cloud.google.com/dataflow/docs/\n",
      "\n",
      "    It returns three chained operators for prediction, summary, and validation,\n",
      "    named as <prefix>-prediction, <prefix>-summary, and <prefix>-validation,\n",
      "    respectively.\n",
      "    (<prefix> should contain only alphanumeric characters or hyphen.)\n",
      "\n",
      "    The upstream and downstream can be set accordingly like:\n",
      "      pred, _, val = create_evaluate_ops(...)\n",
      "      pred.set_upstream(upstream_op)\n",
      "      ...\n",
      "      downstream_op.set_upstream(val)\n",
      "\n",
      "    Callers will provide two python callables, metric_fn and validate_fn, in\n",
      "    order to customize the evaluation behavior as they wish.\n",
      "    - metric_fn receives a dictionary per instance derived from json in the\n",
      "      batch prediction result. The keys might vary depending on the model.\n",
      "      It should return a tuple of metrics.\n",
      "    - validation_fn receives a dictionary of the averaged metrics that metric_fn\n",
      "      generated over all instances.\n",
      "      The key/value of the dictionary matches to what's given by\n",
      "      metric_fn_and_keys arg.\n",
      "      The dictionary contains an additional metric, 'count' to represent the\n",
      "      total number of instances received for evaluation.\n",
      "      The function would raise an exception to mark the task as failed, in a\n",
      "      case the validation result is not okay to proceed (i.e. to set the trained\n",
      "      version as default).\n",
      "\n",
      "    Typical examples are like this:\n",
      "\n",
      "    def get_metric_fn_and_keys():\n",
      "        import math  # imports should be outside of the metric_fn below.\n",
      "        def error_and_squared_error(inst):\n",
      "            label = float(inst['input_label'])\n",
      "            classes = float(inst['classes'])  # 0 or 1\n",
      "            err = abs(classes-label)\n",
      "            squared_err = math.pow(classes-label, 2)\n",
      "            return (err, squared_err)  # returns a tuple.\n",
      "        return error_and_squared_error, ['err', 'mse']  # key order must match.\n",
      "\n",
      "    def validate_err_and_count(summary):\n",
      "        if summary['err'] > 0.2:\n",
      "            raise ValueError('Too high err>0.2; summary=%s' % summary)\n",
      "        if summary['mse'] > 0.05:\n",
      "            raise ValueError('Too high mse>0.05; summary=%s' % summary)\n",
      "        if summary['count'] < 1000:\n",
      "            raise ValueError('Too few instances<1000; summary=%s' % summary)\n",
      "        return summary\n",
      "\n",
      "    For the details on the other BatchPrediction-related arguments (project_id,\n",
      "    job_id, region, data_format, input_paths, prediction_path, model_uri),\n",
      "    please refer to MLEngineBatchPredictionOperator too.\n",
      "\n",
      "    :param task_prefix: a prefix for the tasks. Only alphanumeric characters and\n",
      "        hyphen are allowed (no underscores), since this will be used as dataflow\n",
      "        job name, which doesn't allow other characters.\n",
      "    :type task_prefix: str\n",
      "\n",
      "    :param data_format: either of 'TEXT', 'TF_RECORD', 'TF_RECORD_GZIP'\n",
      "    :type data_format: str\n",
      "\n",
      "    :param input_paths: a list of input paths to be sent to BatchPrediction.\n",
      "    :type input_paths: list[str]\n",
      "\n",
      "    :param prediction_path: GCS path to put the prediction results in.\n",
      "    :type prediction_path: str\n",
      "\n",
      "    :param metric_fn_and_keys: a tuple of metric_fn and metric_keys:\n",
      "        - metric_fn is a function that accepts a dictionary (for an instance),\n",
      "          and returns a tuple of metric(s) that it calculates.\n",
      "        - metric_keys is a list of strings to denote the key of each metric.\n",
      "    :type metric_fn_and_keys: tuple of a function and a list[str]\n",
      "\n",
      "    :param validate_fn: a function to validate whether the averaged metric(s) is\n",
      "        good enough to push the model.\n",
      "    :type validate_fn: function\n",
      "\n",
      "    :param batch_prediction_job_id: the id to use for the Cloud ML Batch\n",
      "        prediction job. Passed directly to the MLEngineBatchPredictionOperator as\n",
      "        the job_id argument.\n",
      "    :type batch_prediction_job_id: str\n",
      "\n",
      "    :param project_id: the Google Cloud Platform project id in which to execute\n",
      "        Cloud ML Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n",
      "        `default_args['project_id']` will be used.\n",
      "    :type project_id: str\n",
      "\n",
      "    :param region: the Google Cloud Platform region in which to execute Cloud ML\n",
      "        Batch Prediction and Dataflow jobs. If None, then the `dag`'s\n",
      "        `default_args['region']` will be used.\n",
      "    :type region: str\n",
      "\n",
      "    :param dataflow_options: options to run Dataflow jobs. If None, then the\n",
      "        `dag`'s `default_args['dataflow_default_options']` will be used.\n",
      "    :type dataflow_options: dictionary\n",
      "\n",
      "    :param model_uri: GCS path of the model exported by Tensorflow using\n",
      "        tensorflow.estimator.export_savedmodel(). It cannot be used with\n",
      "        model_name or version_name below. See MLEngineBatchPredictionOperator for\n",
      "        more detail.\n",
      "    :type model_uri: str\n",
      "\n",
      "    :param model_name: Used to indicate a model to use for prediction. Can be\n",
      "        used in combination with version_name, but cannot be used together with\n",
      "        model_uri. See MLEngineBatchPredictionOperator for more detail. If None,\n",
      "        then the `dag`'s `default_args['model_name']` will be used.\n",
      "    :type model_name: str\n",
      "\n",
      "    :param version_name: Used to indicate a model version to use for prediction,\n",
      "        in combination with model_name. Cannot be used together with model_uri.\n",
      "        See MLEngineBatchPredictionOperator for more detail. If None, then the\n",
      "        `dag`'s `default_args['version_name']` will be used.\n",
      "    :type version_name: str\n",
      "\n",
      "    :param dag: The `DAG` to use for all Operators.\n",
      "    :type dag: airflow.models.DAG\n",
      "\n",
      "    :returns: a tuple of three operators, (prediction, summary, validation)\n",
      "    :rtype: tuple(DataFlowPythonOperator, DataFlowPythonOperator,\n",
      "                  PythonOperator)\n",
      "{'task_prefix': '`task_prefix`', 'data_format': '`data_format`', 'input_paths': '`input_paths`', 'prediction_path': '`prediction_path`', 'metric_fn_and_keys': '`metric_fn_and_keys`', 'validate_fn': '`validate_fn`', 'batch_prediction_job_id': '`batch_prediction_job_id`', 'project_id': '`project_id`', 'region': '`region`', 'dataflow_options': '`dataflow_options`', 'model_uri': '`model_uri`', 'model_name': '`model_name`', 'version_name': '`version_name`', 'dag': '`dag`'}\n",
      "Creates Operators needed for model evaluation and returns.\n",
      "\n",
      "    It gets prediction over inputs via Cloud ML Engine BatchPrediction API by\n",
      "    calling MLEngineBatchPredictionOperator, then summarize and validate\n",
      "    the result via Cloud Dataflow using DataFlowPythonOperator.\n",
      "\n",
      "    For details and pricing about Batch prediction, please refer to the website\n",
      "    https://cloud.google.com/ml-engine/docs/how-tos/batch-predict\n",
      "    and for Cloud Dataflow, https://cloud.google.com/dataflow/docs/\n",
      "\n",
      "    It returns three chained operators for prediction, summary, and validation,\n",
      "    named as <prefix>-prediction, <prefix>-summary, and <prefix>-validation,\n",
      "    respectively.\n",
      "    (<prefix> should contain only alphanumeric characters or hyphen.)\n",
      "\n",
      "    The upstream and downstream can be set accordingly like:\n",
      "      pred, _, val = create_evaluate_ops(...)\n",
      "      pred.set_upstream(upstream_op)\n",
      "      ...\n",
      "      downstream_op.set_upstream(val)\n",
      "\n",
      "    Callers will provide two python callables, metric_fn and `validate_fn`, in\n",
      "    order to customize the evaluation behavior as they wish.\n",
      "    - metric_fn receives a dictionary per instance derived from json in the\n",
      "      batch prediction result. The keys might vary depending on the model.\n",
      "      It should return a tuple of metrics.\n",
      "    - validation_fn receives a dictionary of the averaged metrics that metric_fn\n",
      "      generated over all instances.\n",
      "      The key/value of the dictionary matches to what's given by\n",
      "      `metric_fn_and_keys` arg.\n",
      "      The dictionary contains an additional metric, 'count' to represent the\n",
      "      total number of instances received for evaluation.\n",
      "      The function would raise an exception to mark the task as failed, in a\n",
      "      case the validation result is not okay to proceed (i.e. to set the trained\n",
      "      version as default).\n",
      "\n",
      "    Typical examples are like this:\n",
      "\n",
      "    def get_`metric_fn_and_keys`():\n",
      "        import math  # imports should be outside of the metric_fn below.\n",
      "        def error_and_squared_error(inst):\n",
      "            label = float(inst['input_label'])\n",
      "            classes = float(inst['classes'])  # 0 or 1\n",
      "            err = abs(classes-label)\n",
      "            squared_err = math.pow(classes-label, 2)\n",
      "            return (err, squared_err)  # returns a tuple.\n",
      "        return error_and_squared_error, ['err', 'mse']  # key order must match.\n",
      "\n",
      "    def validate_err_and_count(summary):\n",
      "        if summary['err'] > 0.2:\n",
      "            raise ValueError('Too high err>0.2; summary=%s' % summary)\n",
      "        if summary['mse'] > 0.05:\n",
      "            raise ValueError('Too high mse>0.05; summary=%s' % summary)\n",
      "        if summary['count'] < 1000:\n",
      "            raise ValueError('Too few instances<1000; summary=%s' % summary)\n",
      "        return summary\n",
      "\n",
      "    For the details on the other BatchPrediction-related arguments (`project_id`,\n",
      "    job_id, `region`, `data_format`, `input_paths`, `prediction_path`, `model_uri`),\n",
      "    please refer to MLEngineBatchPredictionOperator too.\n",
      "\n",
      "    :param `task_prefix`: a prefix for the tasks. Only alphanumeric characters and\n",
      "        hyphen are allowed (no underscores), since this will be used as dataflow\n",
      "        job name, which doesn't allow other characters.\n",
      "    :type `task_prefix`: str\n",
      "\n",
      "    :param `data_format`: either of 'TEXT', 'TF_RECORD', 'TF_RECORD_GZIP'\n",
      "    :type `data_format`: str\n",
      "\n",
      "    :param `input_paths`: a list of input paths to be sent to BatchPrediction.\n",
      "    :type `input_paths`: list[str]\n",
      "\n",
      "    :param `prediction_path`: GCS path to put the prediction results in.\n",
      "    :type `prediction_path`: str\n",
      "\n",
      "    :param `metric_fn_and_keys`: a tuple of metric_fn and metric_keys:\n",
      "        - metric_fn is a function that accepts a dictionary (for an instance),\n",
      "          and returns a tuple of metric(s) that it calculates.\n",
      "        - metric_keys is a list of strings to denote the key of each metric.\n",
      "    :type `metric_fn_and_keys`: tuple of a function and a list[str]\n",
      "\n",
      "    :param `validate_fn`: a function to validate whether the averaged metric(s) is\n",
      "        good enough to push the model.\n",
      "    :type `validate_fn`: function\n",
      "\n",
      "    :param `batch_prediction_job_id`: the id to use for the Cloud ML Batch\n",
      "        prediction job. Passed directly to the MLEngineBatchPredictionOperator as\n",
      "        the job_id argument.\n",
      "    :type `batch_prediction_job_id`: str\n",
      "\n",
      "    :param `project_id`: the Google Cloud Platform project id in which to execute\n",
      "        Cloud ML Batch Prediction and Dataflow jobs. If None, then the ``dag``'s\n",
      "        `default_args['`project_id`']` will be used.\n",
      "    :type `project_id`: str\n",
      "\n",
      "    :param `region`: the Google Cloud Platform `region` in which to execute Cloud ML\n",
      "        Batch Prediction and Dataflow jobs. If None, then the ``dag``'s\n",
      "        `default_args['`region`']` will be used.\n",
      "    :type `region`: str\n",
      "\n",
      "    :param `dataflow_options`: options to run Dataflow jobs. If None, then the\n",
      "        ``dag``'s `default_args['dataflow_default_options']` will be used.\n",
      "    :type `dataflow_options`: dictionary\n",
      "\n",
      "    :param `model_uri`: GCS path of the model exported by Tensorflow using\n",
      "        tensorflow.estimator.export_savedmodel(). It cannot be used with\n",
      "        `model_name` or `version_name` below. See MLEngineBatchPredictionOperator for\n",
      "        more detail.\n",
      "    :type `model_uri`: str\n",
      "\n",
      "    :param `model_name`: Used to indicate a model to use for prediction. Can be\n",
      "        used in combination with `version_name`, but cannot be used together with\n",
      "        `model_uri`. See MLEngineBatchPredictionOperator for more detail. If None,\n",
      "        then the ``dag``'s `default_args['`model_name`']` will be used.\n",
      "    :type `model_name`: str\n",
      "\n",
      "    :param `version_name`: Used to indicate a model version to use for prediction,\n",
      "        in combination with `model_name`. Cannot be used together with `model_uri`.\n",
      "        See MLEngineBatchPredictionOperator for more detail. If None, then the\n",
      "        ``dag``'s `default_args['`version_name`']` will be used.\n",
      "    :type `version_name`: str\n",
      "\n",
      "    :param `dag`: The `DAG` to use for all Operators.\n",
      "    :type `dag`: airflow.models.DAG\n",
      "\n",
      "    :returns: a tuple of three operators, (prediction, summary, validation)\n",
      "    :rtype: tuple(DataFlowPythonOperator, DataFlowPythonOperator,\n",
      "                  PythonOperator)\n",
      "Creates the directory specified by path, creating intermediate directories\n",
      "    as necessary. If directory already exists, this is a no-op.\n",
      "\n",
      "    :param path: The directory to create\n",
      "    :type path: str\n",
      "    :param mode: The mode to give to the directory e.g. 0o755, ignores umask\n",
      "    :type mode: int\n",
      "Creates the directory specified by path, creating intermediate directories\n",
      "    as necessary. If directory already exists, this is a no-op.\n",
      "\n",
      "    :param path: The directory to create\n",
      "    :type path: str\n",
      "    :param mode: The mode to give to the directory e.g. 0o755, ignores umask\n",
      "    :type mode: int\n",
      "{'path': '`path`', 'mode': '`mode`'}\n",
      "Creates the directory specified by `path`, creating intermediate directories\n",
      "    as necessary. If directory already exists, this is a no-op.\n",
      "\n",
      "    :param `path`: The directory to create\n",
      "    :type `path`: str\n",
      "    :param `mode`: The `mode` to give to the directory e.g. 0o755, ignores umask\n",
      "    :type `mode`: int\n",
      "A small helper function to convert a string to a numeric value\n",
      "    if appropriate\n",
      "\n",
      "    :param s: the string to be converted\n",
      "    :type s: str\n",
      "A small helper function to convert a string to a numeric value\n",
      "    if appropriate\n",
      "\n",
      "    :param s: the string to be converted\n",
      "    :type s: str\n",
      "{'s': '`s`'}\n",
      "A `s`mall helper function to convert a `s`tring to a numeric value\n",
      "    if appropriate\n",
      "\n",
      "    :param `s`: the `s`tring to be converted\n",
      "    :type `s`: `s`tr\n",
      "Get the current date and time in UTC\n",
      "    :return:\n",
      "Get the current date and time in UTC\n",
      "    :return:\n",
      "Get the current date and time in UTC\n",
      "    :return:\n",
      "Gets the epoch in the users timezone\n",
      "    :return:\n",
      "Gets the epoch in the users timezone\n",
      "    :return:\n",
      "Gets the epoch in the users timezone\n",
      "    :return:\n",
      "Returns the datetime with the default timezone added if timezone\n",
      "    information was not associated\n",
      "    :param value: datetime\n",
      "    :return: datetime with tzinfo\n",
      "Returns the datetime with the default timezone added if timezone\n",
      "    information was not associated\n",
      "    :param value: datetime\n",
      "    :return: datetime with tzinfo\n",
      "{'value': '`value`'}\n",
      "Returns the datetime with the default timezone added if timezone\n",
      "    information was not associated\n",
      "    :param `value`: datetime\n",
      "    :return: datetime with tzinfo\n",
      "Make a naive datetime.datetime in a given time zone aware.\n",
      "\n",
      "    :param value: datetime\n",
      "    :param timezone: timezone\n",
      "    :return: localized datetime in settings.TIMEZONE or timezone\n",
      "Make a naive datetime.datetime in a given time zone aware.\n",
      "\n",
      "    :param value: datetime\n",
      "    :param timezone: timezone\n",
      "    :return: localized datetime in settings.TIMEZONE or timezone\n",
      "{'value': '`value`', 'timezone': '`timezone`'}\n",
      "Make a naive datetime.datetime in a given time zone aware.\n",
      "\n",
      "    :param `value`: datetime\n",
      "    :param `timezone`: `timezone`\n",
      "    :return: localized datetime in settings.TIMEZONE or `timezone`\n",
      "Make an aware datetime.datetime naive in a given time zone.\n",
      "\n",
      "    :param value: datetime\n",
      "    :param timezone: timezone\n",
      "    :return: naive datetime\n",
      "Make an aware datetime.datetime naive in a given time zone.\n",
      "\n",
      "    :param value: datetime\n",
      "    :param timezone: timezone\n",
      "    :return: naive datetime\n",
      "{'value': '`value`', 'timezone': '`timezone`'}\n",
      "Make an aware datetime.datetime naive in a given time zone.\n",
      "\n",
      "    :param `value`: datetime\n",
      "    :param `timezone`: `timezone`\n",
      "    :return: naive datetime\n",
      "Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified\n",
      "\n",
      "    :return: datetime.datetime\n",
      "Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified\n",
      "\n",
      "    :return: datetime.datetime\n",
      "Wrapper around datetime.datetime that adds settings.TIMEZONE if tzinfo not specified\n",
      "\n",
      "    :return: datetime.datetime\n",
      "Sets the environment variable `GOOGLE_APPLICATION_CREDENTIALS` with either:\n",
      "\n",
      "        - The path to the keyfile from the specified connection id\n",
      "        - A generated file's path if the user specified JSON in the connection id. The\n",
      "            file is assumed to be deleted after the process dies due to how mkstemp()\n",
      "            works.\n",
      "\n",
      "        The environment variable is used inside the gcloud command to determine correct\n",
      "        service account to use.\n",
      "Sets the environment variable `GOOGLE_APPLICATION_CREDENTIALS` with either:\n",
      "\n",
      "        - The path to the keyfile from the specified connection id\n",
      "        - A generated file's path if the user specified JSON in the connection id. The\n",
      "            file is assumed to be deleted after the process dies due to how mkstemp()\n",
      "            works.\n",
      "\n",
      "        The environment variable is used inside the gcloud command to determine correct\n",
      "        service account to use.\n",
      "Sets the environment variable `GOOGLE_APPLICATION_CREDENTIALS` with either:\n",
      "\n",
      "        - The path to the keyfile from the specified connection id\n",
      "        - A generated file's path if the user specified JSON in the connection id. The\n",
      "            file is assumed to be deleted after the process dies due to how mkstemp()\n",
      "            works.\n",
      "\n",
      "        The environment variable is used inside the gcloud command to determine correct\n",
      "        service account to use.\n",
      "Fetches a field from extras, and returns it. This is some Airflow\n",
      "        magic. The google_cloud_platform hook type adds custom UI elements\n",
      "        to the hook page, which allow admins to specify service_account,\n",
      "        key_path, etc. They get formatted as shown below.\n",
      "Fetches a field from extras, and returns it. This is some Airflow\n",
      "        magic. The google_cloud_platform hook type adds custom UI elements\n",
      "        to the hook page, which allow admins to specify service_account,\n",
      "        key_path, etc. They get formatted as shown below.\n",
      "Fetches a field from extras, and returns it. This is some Airflow\n",
      "        magic. The google_cloud_platform hook type adds custom UI elements\n",
      "        to the hook page, which allow admins to specify service_account,\n",
      "        key_path, etc. They get formatted as shown below.\n",
      "Establish a connection to druid broker.\n",
      "Establish a connection to druid broker.\n",
      "Establish a connection to druid broker.\n",
      "Returns http session for use with requests\n",
      "\n",
      "        :param headers: additional headers to be passed through as a dictionary\n",
      "        :type headers: dict\n",
      "Returns http session for use with requests\n",
      "\n",
      "        :param headers: additional headers to be passed through as a dictionary\n",
      "        :type headers: dict\n",
      "{'headers': '`headers`'}\n",
      "Returns http session for use with requests\n",
      "\n",
      "        :param `headers`: additional `headers` to be passed through as a dictionary\n",
      "        :type `headers`: dict\n",
      "Performs the request\n",
      "\n",
      "        :param endpoint: the endpoint to be called i.e. resource/v1/query?\n",
      "        :type endpoint: str\n",
      "        :param data: payload to be uploaded or request parameters\n",
      "        :type data: dict\n",
      "        :param headers: additional headers to be passed through as a dictionary\n",
      "        :type headers: dict\n",
      "        :param extra_options: additional options to be used when executing the request\n",
      "            i.e. {'check_response': False} to avoid checking raising exceptions on non\n",
      "            2XX or 3XX status codes\n",
      "        :type extra_options: dict\n",
      "Performs the request\n",
      "\n",
      "        :param endpoint: the endpoint to be called i.e. resource/v1/query?\n",
      "        :type endpoint: str\n",
      "        :param data: payload to be uploaded or request parameters\n",
      "        :type data: dict\n",
      "        :param headers: additional headers to be passed through as a dictionary\n",
      "        :type headers: dict\n",
      "        :param extra_options: additional options to be used when executing the request\n",
      "            i.e. {'check_response': False} to avoid checking raising exceptions on non\n",
      "            2XX or 3XX status codes\n",
      "        :type extra_options: dict\n",
      "{'endpoint': '`endpoint`', 'data': '`data`', 'headers': '`headers`', 'extra_options': '`extra_options`'}\n",
      "Performs the request\n",
      "\n",
      "        :param `endpoint`: the `endpoint` to be called i.e. resource/v1/query?\n",
      "        :type `endpoint`: str\n",
      "        :param `data`: payload to be uploaded or request parameters\n",
      "        :type `data`: dict\n",
      "        :param `headers`: additional `headers` to be passed through as a dictionary\n",
      "        :type `headers`: dict\n",
      "        :param `extra_options`: additional options to be used when executing the request\n",
      "            i.e. {'check_response': False} to avoid checking raising exceptions on non\n",
      "            2XX or 3XX status codes\n",
      "        :type `extra_options`: dict\n",
      "Checks the status code and raise an AirflowException exception on non 2XX or 3XX\n",
      "        status codes\n",
      "\n",
      "        :param response: A requests response object\n",
      "        :type response: requests.response\n",
      "Checks the status code and raise an AirflowException exception on non 2XX or 3XX\n",
      "        status codes\n",
      "\n",
      "        :param response: A requests response object\n",
      "        :type response: requests.response\n",
      "{'response': '`response`'}\n",
      "Checks the status code and raise an AirflowException exception on non 2XX or 3XX\n",
      "        status codes\n",
      "\n",
      "        :param `response`: A requests `response` object\n",
      "        :type `response`: requests.`response`\n",
      "Grabs extra options like timeout and actually runs the request,\n",
      "        checking for the result\n",
      "\n",
      "        :param session: the session to be used to execute the request\n",
      "        :type session: requests.Session\n",
      "        :param prepped_request: the prepared request generated in run()\n",
      "        :type prepped_request: session.prepare_request\n",
      "        :param extra_options: additional options to be used when executing the request\n",
      "            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX\n",
      "            or 3XX status codes\n",
      "        :type extra_options: dict\n",
      "Grabs extra options like timeout and actually runs the request,\n",
      "        checking for the result\n",
      "\n",
      "        :param session: the session to be used to execute the request\n",
      "        :type session: requests.Session\n",
      "        :param prepped_request: the prepared request generated in run()\n",
      "        :type prepped_request: session.prepare_request\n",
      "        :param extra_options: additional options to be used when executing the request\n",
      "            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX\n",
      "            or 3XX status codes\n",
      "        :type extra_options: dict\n",
      "{'session': '`session`', 'prepped_request': '`prepped_request`', 'extra_options': '`extra_options`'}\n",
      "Grabs extra options like timeout and actually runs the request,\n",
      "        checking for the result\n",
      "\n",
      "        :param `session`: the `session` to be used to execute the request\n",
      "        :type `session`: requests.Session\n",
      "        :param `prepped_request`: the prepared request generated in run()\n",
      "        :type `prepped_request`: `session`.prepare_request\n",
      "        :param `extra_options`: additional options to be used when executing the request\n",
      "            i.e. {'check_response': False} to avoid checking raising exceptions on non 2XX\n",
      "            or 3XX status codes\n",
      "        :type `extra_options`: dict\n",
      "Runs Hook.run() with a Tenacity decorator attached to it. This is useful for\n",
      "        connectors which might be disturbed by intermittent issues and should not\n",
      "        instantly fail.\n",
      "\n",
      "        :param _retry_args: Arguments which define the retry behaviour.\n",
      "            See Tenacity documentation at https://github.com/jd/tenacity\n",
      "        :type _retry_args: dict\n",
      "\n",
      "\n",
      "        :Example::\n",
      "\n",
      "            hook = HttpHook(http_conn_id='my_conn',method='GET')\n",
      "            retry_args = dict(\n",
      "                 wait=tenacity.wait_exponential(),\n",
      "                 stop=tenacity.stop_after_attempt(10),\n",
      "                 retry=requests.exceptions.ConnectionError\n",
      "             )\n",
      "             hook.run_with_advanced_retry(\n",
      "                     endpoint='v1/test',\n",
      "                     _retry_args=retry_args\n",
      "                 )\n",
      "Runs Hook.run() with a Tenacity decorator attached to it. This is useful for\n",
      "        connectors which might be disturbed by intermittent issues and should not\n",
      "        instantly fail.\n",
      "\n",
      "        :param _retry_args: Arguments which define the retry behaviour.\n",
      "            See Tenacity documentation at https://github.com/jd/tenacity\n",
      "        :type _retry_args: dict\n",
      "\n",
      "\n",
      "        :Example::\n",
      "\n",
      "            hook = HttpHook(http_conn_id='my_conn',method='GET')\n",
      "            retry_args = dict(\n",
      "                 wait=tenacity.wait_exponential(),\n",
      "                 stop=tenacity.stop_after_attempt(10),\n",
      "                 retry=requests.exceptions.ConnectionError\n",
      "             )\n",
      "             hook.run_with_advanced_retry(\n",
      "                     endpoint='v1/test',\n",
      "                     _retry_args=retry_args\n",
      "                 )\n",
      "{'_retry_args': '`_retry_args`'}\n",
      "Runs Hook.run() with a Tenacity decorator attached to it. This is useful for\n",
      "        connectors which might be disturbed by intermittent issues and should not\n",
      "        instantly fail.\n",
      "\n",
      "        :param `_retry_args`: Arguments which define the retry behaviour.\n",
      "            See Tenacity documentation at https://github.com/jd/tenacity\n",
      "        :type `_retry_args`: dict\n",
      "\n",
      "\n",
      "        :Example::\n",
      "\n",
      "            hook = HttpHook(http_conn_id='my_conn',method='GET')\n",
      "            retry_args = dict(\n",
      "                 wait=tenacity.wait_exponential(),\n",
      "                 stop=tenacity.stop_after_attempt(10),\n",
      "                 retry=requests.exceptions.ConnectionError\n",
      "             )\n",
      "             hook.run_with_advanced_retry(\n",
      "                     endpoint='v1/test',\n",
      "                     `_retry_args`=retry_args\n",
      "                 )\n",
      "Contextmanager that will create and teardown a session.\n",
      "Contextmanager that will create and teardown a session.\n",
      "Contextmanager that will create and teardown a session.\n",
      "Function decorator that provides a session if it isn't provided.\n",
      "    If you want to reuse a session or run the function as part of a\n",
      "    database transaction, you pass it to the function, if not this wrapper\n",
      "    will create one and close it for you.\n",
      "Function decorator that provides a session if it isn't provided.\n",
      "    If you want to reuse a session or run the function as part of a\n",
      "    database transaction, you pass it to the function, if not this wrapper\n",
      "    will create one and close it for you.\n",
      "Function decorator that provides a session if it isn't provided.\n",
      "    If you want to reuse a session or run the function as part of a\n",
      "    database transaction, you pass it to the function, if not this wrapper\n",
      "    will create one and close it for you.\n",
      "Clear out the database\n",
      "Clear out the database\n",
      "Clear out the database\n",
      "Upload a file to Azure Blob Storage.\n",
      "Upload a file to Azure Blob Storage.\n",
      "Upload a file to Azure Blob Storage.\n",
      "Returns a connection object\n",
      "Returns a connection object\n",
      "Returns a connection object\n",
      "Parses some DatabaseError to provide a better error message\n",
      "Parses some DatabaseError to provide a better error message\n",
      "Parses some DatabaseError to provide a better error message\n",
      "Get a set of records from Presto\n",
      "Get a set of records from Presto\n",
      "Get a set of records from Presto\n",
      "Get a pandas dataframe from a sql query.\n",
      "Get a pandas dataframe from a sql query.\n",
      "Get a pandas dataframe from a sql query.\n",
      "Execute the statement against Presto. Can be used to create views.\n",
      "Execute the statement against Presto. Can be used to create views.\n",
      "Execute the statement against Presto. Can be used to create views.\n",
      "A generic way to insert a set of tuples into a table.\n",
      "\n",
      "        :param table: Name of the target table\n",
      "        :type table: str\n",
      "        :param rows: The rows to insert into the table\n",
      "        :type rows: iterable of tuples\n",
      "        :param target_fields: The names of the columns to fill in the table\n",
      "        :type target_fields: iterable of strings\n",
      "A generic way to insert a set of tuples into a table.\n",
      "\n",
      "        :param table: Name of the target table\n",
      "        :type table: str\n",
      "        :param rows: The rows to insert into the table\n",
      "        :type rows: iterable of tuples\n",
      "        :param target_fields: The names of the columns to fill in the table\n",
      "        :type target_fields: iterable of strings\n",
      "{'table': '`table`', 'rows': '`rows`', 'target_fields': '`target_fields`'}\n",
      "A generic way to insert a set of tuples into a `table`.\n",
      "\n",
      "        :param `table`: Name of the target `table`\n",
      "        :type `table`: str\n",
      "        :param `rows`: The `rows` to insert into the `table`\n",
      "        :type `rows`: iterable of tuples\n",
      "        :param `target_fields`: The names of the columns to fill in the `table`\n",
      "        :type `target_fields`: iterable of strings\n",
      "Return a cosmos db client.\n",
      "Return a cosmos db client.\n",
      "Return a cosmos db client.\n",
      "Checks if a collection exists in CosmosDB.\n",
      "Checks if a collection exists in CosmosDB.\n",
      "Checks if a collection exists in CosmosDB.\n",
      "Creates a new collection in the CosmosDB database.\n",
      "Creates a new collection in the CosmosDB database.\n",
      "Creates a new collection in the CosmosDB database.\n",
      "Checks if a database exists in CosmosDB.\n",
      "Checks if a database exists in CosmosDB.\n",
      "Checks if a database exists in CosmosDB.\n",
      "Creates a new database in CosmosDB.\n",
      "Creates a new database in CosmosDB.\n",
      "Creates a new database in CosmosDB.\n",
      "Deletes an existing database in CosmosDB.\n",
      "Deletes an existing database in CosmosDB.\n",
      "Deletes an existing database in CosmosDB.\n",
      "Deletes an existing collection in the CosmosDB database.\n",
      "Deletes an existing collection in the CosmosDB database.\n",
      "Deletes an existing collection in the CosmosDB database.\n",
      "Inserts a new document (or updates an existing one) into an existing\n",
      "        collection in the CosmosDB database.\n",
      "Inserts a new document (or updates an existing one) into an existing\n",
      "        collection in the CosmosDB database.\n",
      "Inserts a new document (or updates an existing one) into an existing\n",
      "        collection in the CosmosDB database.\n",
      "Insert a list of new documents into an existing collection in the CosmosDB database.\n",
      "Insert a list of new documents into an existing collection in the CosmosDB database.\n",
      "Insert a list of new documents into an existing collection in the CosmosDB database.\n",
      "Delete an existing document out of a collection in the CosmosDB database.\n",
      "Delete an existing document out of a collection in the CosmosDB database.\n",
      "Delete an existing document out of a collection in the CosmosDB database.\n",
      "Get a document from an existing collection in the CosmosDB database.\n",
      "Get a document from an existing collection in the CosmosDB database.\n",
      "Get a document from an existing collection in the CosmosDB database.\n",
      "Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n",
      "Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n",
      "Get a list of documents from an existing collection in the CosmosDB database via SQL query.\n",
      "Return python code of a given dag_id.\n",
      "Return python code of a given dag_id.\n",
      "Return python code of a given dag_id.\n",
      "Returns the Cloud Function with the given name.\n",
      "\n",
      "        :param name: Name of the function.\n",
      "        :type name: str\n",
      "        :return: A Cloud Functions object representing the function.\n",
      "        :rtype: dict\n",
      "Returns the Cloud Function with the given name.\n",
      "\n",
      "        :param name: Name of the function.\n",
      "        :type name: str\n",
      "        :return: A Cloud Functions object representing the function.\n",
      "        :rtype: dict\n",
      "{'name': '`name`'}\n",
      "Returns the Cloud Function with the given `name`.\n",
      "\n",
      "        :param `name`: Name of the function.\n",
      "        :type `name`: str\n",
      "        :return: A Cloud Functions object representing the function.\n",
      "        :rtype: dict\n",
      "Creates a new function in Cloud Function in the location specified in the body.\n",
      "\n",
      "        :param location: The location of the function.\n",
      "        :type location: str\n",
      "        :param body: The body required by the Cloud Functions insert API.\n",
      "        :type body: dict\n",
      "        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n",
      "            If set to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "Creates a new function in Cloud Function in the location specified in the body.\n",
      "\n",
      "        :param location: The location of the function.\n",
      "        :type location: str\n",
      "        :param body: The body required by the Cloud Functions insert API.\n",
      "        :type body: dict\n",
      "        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n",
      "            If set to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: None\n",
      "{'location': '`location`', 'body': '`body`', 'project_id': '`project_id`'}\n",
      "Creates a new function in Cloud Function in the `location` specified in the `body`.\n",
      "\n",
      "        :param `location`: The `location` of the function.\n",
      "        :type `location`: str\n",
      "        :param `body`: The `body` required by the Cloud Functions insert API.\n",
      "        :type `body`: dict\n",
      "        :param `project_id`: Optional, Google Cloud Project `project_id` where the function belongs.\n",
      "            If set to None or missing, the default `project_id` from the GCP connection is used.\n",
      "        :type `project_id`: str\n",
      "        :return: None\n",
      "Updates Cloud Functions according to the specified update mask.\n",
      "\n",
      "        :param name: The name of the function.\n",
      "        :type name: str\n",
      "        :param body: The body required by the cloud function patch API.\n",
      "        :type body: dict\n",
      "        :param update_mask: The update mask - array of fields that should be patched.\n",
      "        :type update_mask: [str]\n",
      "        :return: None\n",
      "Updates Cloud Functions according to the specified update mask.\n",
      "\n",
      "        :param name: The name of the function.\n",
      "        :type name: str\n",
      "        :param body: The body required by the cloud function patch API.\n",
      "        :type body: dict\n",
      "        :param update_mask: The update mask - array of fields that should be patched.\n",
      "        :type update_mask: [str]\n",
      "        :return: None\n",
      "{'name': '`name`', 'body': '`body`', 'update_mask': '`update_mask`'}\n",
      "Updates Cloud Functions according to the specified update mask.\n",
      "\n",
      "        :param `name`: The `name` of the function.\n",
      "        :type `name`: str\n",
      "        :param `body`: The `body` required by the cloud function patch API.\n",
      "        :type `body`: dict\n",
      "        :param `update_mask`: The update mask - array of fields that should be patched.\n",
      "        :type `update_mask`: [str]\n",
      "        :return: None\n",
      "Uploads zip file with sources.\n",
      "\n",
      "        :param location: The location where the function is created.\n",
      "        :type location: str\n",
      "        :param zip_path: The path of the valid .zip file to upload.\n",
      "        :type zip_path: str\n",
      "        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n",
      "            If set to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: The upload URL that was returned by generateUploadUrl method.\n",
      "Uploads zip file with sources.\n",
      "\n",
      "        :param location: The location where the function is created.\n",
      "        :type location: str\n",
      "        :param zip_path: The path of the valid .zip file to upload.\n",
      "        :type zip_path: str\n",
      "        :param project_id: Optional, Google Cloud Project project_id where the function belongs.\n",
      "            If set to None or missing, the default project_id from the GCP connection is used.\n",
      "        :type project_id: str\n",
      "        :return: The upload URL that was returned by generateUploadUrl method.\n",
      "{'location': '`location`', 'zip_path': '`zip_path`', 'project_id': '`project_id`'}\n",
      "Uploads zip file with sources.\n",
      "\n",
      "        :param `location`: The `location` where the function is created.\n",
      "        :type `location`: str\n",
      "        :param `zip_path`: The path of the valid .zip file to upload.\n",
      "        :type `zip_path`: str\n",
      "        :param `project_id`: Optional, Google Cloud Project `project_id` where the function belongs.\n",
      "            If set to None or missing, the default `project_id` from the GCP connection is used.\n",
      "        :type `project_id`: str\n",
      "        :return: The upload URL that was returned by generateUploadUrl method.\n",
      "Deletes the specified Cloud Function.\n",
      "\n",
      "        :param name: The name of the function.\n",
      "        :type name: str\n",
      "        :return: None\n",
      "Deletes the specified Cloud Function.\n",
      "\n",
      "        :param name: The name of the function.\n",
      "        :type name: str\n",
      "        :return: None\n",
      "{'name': '`name`'}\n",
      "Deletes the specified Cloud Function.\n",
      "\n",
      "        :param `name`: The `name` of the function.\n",
      "        :type `name`: str\n",
      "        :return: None\n",
      "Waits for the named operation to complete - checks status of the\n",
      "        asynchronous call.\n",
      "\n",
      "        :param operation_name: The name of the operation.\n",
      "        :type operation_name: str\n",
      "        :return: The response returned by the operation.\n",
      "        :rtype: dict\n",
      "        :exception: AirflowException in case error is returned.\n",
      "Waits for the named operation to complete - checks status of the\n",
      "        asynchronous call.\n",
      "\n",
      "        :param operation_name: The name of the operation.\n",
      "        :type operation_name: str\n",
      "        :return: The response returned by the operation.\n",
      "        :rtype: dict\n",
      "        :exception: AirflowException in case error is returned.\n",
      "{'operation_name': '`operation_name`'}\n",
      "Waits for the named operation to complete - checks status of the\n",
      "        asynchronous call.\n",
      "\n",
      "        :param `operation_name`: The name of the operation.\n",
      "        :type `operation_name`: str\n",
      "        :return: The response returned by the operation.\n",
      "        :rtype: dict\n",
      "        :exception: AirflowException in case error is returned.\n",
      "Publishes messages to a Pub/Sub topic.\n",
      "\n",
      "        :param project: the GCP project ID in which to publish\n",
      "        :type project: str\n",
      "        :param topic: the Pub/Sub topic to which to publish; do not\n",
      "            include the ``projects/{project}/topics/`` prefix.\n",
      "        :type topic: str\n",
      "        :param messages: messages to publish; if the data field in a\n",
      "            message is set, it should already be base64 encoded.\n",
      "        :type messages: list of PubSub messages; see\n",
      "            http://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage\n",
      "Publishes messages to a Pub/Sub topic.\n",
      "\n",
      "        :param project: the GCP project ID in which to publish\n",
      "        :type project: str\n",
      "        :param topic: the Pub/Sub topic to which to publish; do not\n",
      "            include the ``projects/{project}/topics/`` prefix.\n",
      "        :type topic: str\n",
      "        :param messages: messages to publish; if the data field in a\n",
      "            message is set, it should already be base64 encoded.\n",
      "        :type messages: list of PubSub messages; see\n",
      "            http://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage\n",
      "{'project': '`project`', 'topic': '`topic`', 'messages': '`messages`'}\n",
      "Publishes `messages` to a Pub/Sub `topic`.\n",
      "\n",
      "        :param `project`: the GCP `project` ID in which to publish\n",
      "        :type `project`: str\n",
      "        :param `topic`: the Pub/Sub `topic` to which to publish; do not\n",
      "            include the ```project`s/{`project`}/`topic`s/`` prefix.\n",
      "        :type `topic`: str\n",
      "        :param `messages`: `messages` to publish; if the data field in a\n",
      "            message is set, it should already be base64 encoded.\n",
      "        :type `messages`: list of PubSub `messages`; see\n",
      "            http://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage\n",
      "Creates a Pub/Sub topic, if it does not already exist.\n",
      "\n",
      "        :param project: the GCP project ID in which to create\n",
      "            the topic\n",
      "        :type project: str\n",
      "        :param topic: the Pub/Sub topic name to create; do not\n",
      "            include the ``projects/{project}/topics/`` prefix.\n",
      "        :type topic: str\n",
      "        :param fail_if_exists: if set, raise an exception if the topic\n",
      "            already exists\n",
      "        :type fail_if_exists: bool\n",
      "Creates a Pub/Sub topic, if it does not already exist.\n",
      "\n",
      "        :param project: the GCP project ID in which to create\n",
      "            the topic\n",
      "        :type project: str\n",
      "        :param topic: the Pub/Sub topic name to create; do not\n",
      "            include the ``projects/{project}/topics/`` prefix.\n",
      "        :type topic: str\n",
      "        :param fail_if_exists: if set, raise an exception if the topic\n",
      "            already exists\n",
      "        :type fail_if_exists: bool\n",
      "{'project': '`project`', 'topic': '`topic`', 'fail_if_exists': '`fail_if_exists`'}\n",
      "Creates a Pub/Sub `topic`, if it does not already exist.\n",
      "\n",
      "        :param `project`: the GCP `project` ID in which to create\n",
      "            the `topic`\n",
      "        :type `project`: str\n",
      "        :param `topic`: the Pub/Sub `topic` name to create; do not\n",
      "            include the ```project`s/{`project`}/`topic`s/`` prefix.\n",
      "        :type `topic`: str\n",
      "        :param `fail_if_exists`: if set, raise an exception if the `topic`\n",
      "            already exists\n",
      "        :type `fail_if_exists`: bool\n",
      "Deletes a Pub/Sub topic if it exists.\n",
      "\n",
      "        :param project: the GCP project ID in which to delete the topic\n",
      "        :type project: str\n",
      "        :param topic: the Pub/Sub topic name to delete; do not\n",
      "            include the ``projects/{project}/topics/`` prefix.\n",
      "        :type topic: str\n",
      "        :param fail_if_not_exists: if set, raise an exception if the topic\n",
      "            does not exist\n",
      "        :type fail_if_not_exists: bool\n",
      "Deletes a Pub/Sub topic if it exists.\n",
      "\n",
      "        :param project: the GCP project ID in which to delete the topic\n",
      "        :type project: str\n",
      "        :param topic: the Pub/Sub topic name to delete; do not\n",
      "            include the ``projects/{project}/topics/`` prefix.\n",
      "        :type topic: str\n",
      "        :param fail_if_not_exists: if set, raise an exception if the topic\n",
      "            does not exist\n",
      "        :type fail_if_not_exists: bool\n",
      "{'project': '`project`', 'topic': '`topic`', 'fail_if_not_exists': '`fail_if_not_exists`'}\n",
      "Deletes a Pub/Sub `topic` if it exists.\n",
      "\n",
      "        :param `project`: the GCP `project` ID in which to delete the `topic`\n",
      "        :type `project`: str\n",
      "        :param `topic`: the Pub/Sub `topic` name to delete; do not\n",
      "            include the ```project`s/{`project`}/`topic`s/`` prefix.\n",
      "        :type `topic`: str\n",
      "        :param `fail_if_not_exists`: if set, raise an exception if the `topic`\n",
      "            does not exist\n",
      "        :type `fail_if_not_exists`: bool\n",
      "Creates a Pub/Sub subscription, if it does not already exist.\n",
      "\n",
      "        :param topic_project: the GCP project ID of the topic that the\n",
      "            subscription will be bound to.\n",
      "        :type topic_project: str\n",
      "        :param topic: the Pub/Sub topic name that the subscription will be bound\n",
      "            to create; do not include the ``projects/{project}/subscriptions/``\n",
      "            prefix.\n",
      "        :type topic: str\n",
      "        :param subscription: the Pub/Sub subscription name. If empty, a random\n",
      "            name will be generated using the uuid module\n",
      "        :type subscription: str\n",
      "        :param subscription_project: the GCP project ID where the subscription\n",
      "            will be created. If unspecified, ``topic_project`` will be used.\n",
      "        :type subscription_project: str\n",
      "        :param ack_deadline_secs: Number of seconds that a subscriber has to\n",
      "            acknowledge each message pulled from the subscription\n",
      "        :type ack_deadline_secs: int\n",
      "        :param fail_if_exists: if set, raise an exception if the topic\n",
      "            already exists\n",
      "        :type fail_if_exists: bool\n",
      "        :return: subscription name which will be the system-generated value if\n",
      "            the ``subscription`` parameter is not supplied\n",
      "        :rtype: str\n",
      "Creates a Pub/Sub subscription, if it does not already exist.\n",
      "\n",
      "        :param topic_project: the GCP project ID of the topic that the\n",
      "            subscription will be bound to.\n",
      "        :type topic_project: str\n",
      "        :param topic: the Pub/Sub topic name that the subscription will be bound\n",
      "            to create; do not include the ``projects/{project}/subscriptions/``\n",
      "            prefix.\n",
      "        :type topic: str\n",
      "        :param subscription: the Pub/Sub subscription name. If empty, a random\n",
      "            name will be generated using the uuid module\n",
      "        :type subscription: str\n",
      "        :param subscription_project: the GCP project ID where the subscription\n",
      "            will be created. If unspecified, ``topic_project`` will be used.\n",
      "        :type subscription_project: str\n",
      "        :param ack_deadline_secs: Number of seconds that a subscriber has to\n",
      "            acknowledge each message pulled from the subscription\n",
      "        :type ack_deadline_secs: int\n",
      "        :param fail_if_exists: if set, raise an exception if the topic\n",
      "            already exists\n",
      "        :type fail_if_exists: bool\n",
      "        :return: subscription name which will be the system-generated value if\n",
      "            the ``subscription`` parameter is not supplied\n",
      "        :rtype: str\n",
      "{'topic_project': '`topic_project`', 'topic': '`topic`', 'subscription': '`subscription`', 'subscription_project': '`subscription_project`', 'ack_deadline_secs': '`ack_deadline_secs`', 'fail_if_exists': '`fail_if_exists`'}\n",
      "Creates a Pub/Sub `subscription`, if it does not already exist.\n",
      "\n",
      "        :param `topic_project`: the GCP project ID of the `topic` that the\n",
      "            `subscription` will be bound to.\n",
      "        :type `topic_project`: str\n",
      "        :param `topic`: the Pub/Sub `topic` name that the `subscription` will be bound\n",
      "            to create; do not include the ``projects/{project}/`subscription`s/``\n",
      "            prefix.\n",
      "        :type `topic`: str\n",
      "        :param `subscription`: the Pub/Sub `subscription` name. If empty, a random\n",
      "            name will be generated using the uuid module\n",
      "        :type `subscription`: str\n",
      "        :param `subscription`_project: the GCP project ID where the `subscription`\n",
      "            will be created. If unspecified, ```topic_project``` will be used.\n",
      "        :type `subscription`_project: str\n",
      "        :param `ack_deadline_secs`: Number of seconds that a subscriber has to\n",
      "            acknowledge each message pulled from the `subscription`\n",
      "        :type `ack_deadline_secs`: int\n",
      "        :param `fail_if_exists`: if set, raise an exception if the `topic`\n",
      "            already exists\n",
      "        :type `fail_if_exists`: bool\n",
      "        :return: `subscription` name which will be the system-generated value if\n",
      "            the ```subscription``` parameter is not supplied\n",
      "        :rtype: str\n",
      "Deletes a Pub/Sub subscription, if it exists.\n",
      "\n",
      "        :param project: the GCP project ID where the subscription exists\n",
      "        :type project: str\n",
      "        :param subscription: the Pub/Sub subscription name to delete; do not\n",
      "            include the ``projects/{project}/subscriptions/`` prefix.\n",
      "        :type subscription: str\n",
      "        :param fail_if_not_exists: if set, raise an exception if the topic\n",
      "            does not exist\n",
      "        :type fail_if_not_exists: bool\n",
      "Deletes a Pub/Sub subscription, if it exists.\n",
      "\n",
      "        :param project: the GCP project ID where the subscription exists\n",
      "        :type project: str\n",
      "        :param subscription: the Pub/Sub subscription name to delete; do not\n",
      "            include the ``projects/{project}/subscriptions/`` prefix.\n",
      "        :type subscription: str\n",
      "        :param fail_if_not_exists: if set, raise an exception if the topic\n",
      "            does not exist\n",
      "        :type fail_if_not_exists: bool\n",
      "{'project': '`project`', 'subscription': '`subscription`', 'fail_if_not_exists': '`fail_if_not_exists`'}\n",
      "Deletes a Pub/Sub `subscription`, if it exists.\n",
      "\n",
      "        :param `project`: the GCP `project` ID where the `subscription` exists\n",
      "        :type `project`: str\n",
      "        :param `subscription`: the Pub/Sub `subscription` name to delete; do not\n",
      "            include the ```project`s/{`project`}/`subscription`s/`` prefix.\n",
      "        :type `subscription`: str\n",
      "        :param `fail_if_not_exists`: if set, raise an exception if the topic\n",
      "            does not exist\n",
      "        :type `fail_if_not_exists`: bool\n",
      "Pulls up to ``max_messages`` messages from Pub/Sub subscription.\n",
      "\n",
      "        :param project: the GCP project ID where the subscription exists\n",
      "        :type project: str\n",
      "        :param subscription: the Pub/Sub subscription name to pull from; do not\n",
      "            include the 'projects/{project}/topics/' prefix.\n",
      "        :type subscription: str\n",
      "        :param max_messages: The maximum number of messages to return from\n",
      "            the Pub/Sub API.\n",
      "        :type max_messages: int\n",
      "        :param return_immediately: If set, the Pub/Sub API will immediately\n",
      "            return if no messages are available. Otherwise, the request will\n",
      "            block for an undisclosed, but bounded period of time\n",
      "        :type return_immediately: bool\n",
      "        :return: A list of Pub/Sub ReceivedMessage objects each containing\n",
      "            an ``ackId`` property and a ``message`` property, which includes\n",
      "            the base64-encoded message content. See\n",
      "            https://cloud.google.com/pubsub/docs/reference/rest/v1/projects.subscriptions/pull#ReceivedMessage\n",
      "Pulls up to ``max_messages`` messages from Pub/Sub subscription.\n",
      "\n",
      "        :param project: the GCP project ID where the subscription exists\n",
      "        :type project: str\n",
      "        :param subscription: the Pub/Sub subscription name to pull from; do not\n",
      "            include the 'projects/{project}/topics/' prefix.\n",
      "        :type subscription: str\n",
      "        :param max_messages: The maximum number of messages to return from\n",
      "            the Pub/Sub API.\n",
      "        :type max_messages: int\n",
      "        :param return_immediately: If set, the Pub/Sub API will immediately\n",
      "            return if no messages are available. Otherwise, the request will\n",
      "            block for an undisclosed, but bounded period of time\n",
      "        :type return_immediately: bool\n",
      "        :return: A list of Pub/Sub ReceivedMessage objects each containing\n",
      "            an ``ackId`` property and a ``message`` property, which includes\n",
      "            the base64-encoded message content. See\n",
      "            https://cloud.google.com/pubsub/docs/reference/rest/v1/projects.subscriptions/pull#ReceivedMessage\n",
      "{'project': '`project`', 'subscription': '`subscription`', 'max_messages': '`max_messages`', 'return_immediately': '`return_immediately`'}\n",
      "Pulls up to ```max_messages``` messages from Pub/Sub `subscription`.\n",
      "\n",
      "        :param `project`: the GCP `project` ID where the `subscription` exists\n",
      "        :type `project`: str\n",
      "        :param `subscription`: the Pub/Sub `subscription` name to pull from; do not\n",
      "            include the '`project`s/{`project`}/topics/' prefix.\n",
      "        :type `subscription`: str\n",
      "        :param `max_messages`: The maximum number of messages to return from\n",
      "            the Pub/Sub API.\n",
      "        :type `max_messages`: int\n",
      "        :param `return_immediately`: If set, the Pub/Sub API will immediately\n",
      "            return if no messages are available. Otherwise, the request will\n",
      "            block for an undisclosed, but bounded period of time\n",
      "        :type `return_immediately`: bool\n",
      "        :return: A list of Pub/Sub ReceivedMessage objects each containing\n",
      "            an ``ackId`` property and a ``message`` property, which includes\n",
      "            the base64-encoded message content. See\n",
      "            https://cloud.google.com/pubsub/docs/reference/rest/v1/`project`s.`subscription`s/pull#ReceivedMessage\n",
      "Pulls up to ``max_messages`` messages from Pub/Sub subscription.\n",
      "\n",
      "        :param project: the GCP project name or ID in which to create\n",
      "            the topic\n",
      "        :type project: str\n",
      "        :param subscription: the Pub/Sub subscription name to delete; do not\n",
      "            include the 'projects/{project}/topics/' prefix.\n",
      "        :type subscription: str\n",
      "        :param ack_ids: List of ReceivedMessage ackIds from a previous pull\n",
      "            response\n",
      "        :type ack_ids: list\n",
      "Pulls up to ``max_messages`` messages from Pub/Sub subscription.\n",
      "\n",
      "        :param project: the GCP project name or ID in which to create\n",
      "            the topic\n",
      "        :type project: str\n",
      "        :param subscription: the Pub/Sub subscription name to delete; do not\n",
      "            include the 'projects/{project}/topics/' prefix.\n",
      "        :type subscription: str\n",
      "        :param ack_ids: List of ReceivedMessage ackIds from a previous pull\n",
      "            response\n",
      "        :type ack_ids: list\n",
      "{'project': '`project`', 'subscription': '`subscription`', 'ack_ids': '`ack_ids`'}\n",
      "Pulls up to ``max_messages`` messages from Pub/Sub `subscription`.\n",
      "\n",
      "        :param `project`: the GCP `project` name or ID in which to create\n",
      "            the topic\n",
      "        :type `project`: str\n",
      "        :param `subscription`: the Pub/Sub `subscription` name to delete; do not\n",
      "            include the '`project`s/{`project`}/topics/' prefix.\n",
      "        :type `subscription`: str\n",
      "        :param `ack_ids`: List of ReceivedMessage ackIds from a previous pull\n",
      "            response\n",
      "        :type `ack_ids`: list\n",
      "Wrapper around the private _get_dep_statuses method that contains some global\n",
      "        checks for all dependencies.\n",
      "\n",
      "        :param ti: the task instance to get the dependency status for\n",
      "        :type ti: airflow.models.TaskInstance\n",
      "        :param session: database session\n",
      "        :type session: sqlalchemy.orm.session.Session\n",
      "        :param dep_context: the context for which this dependency should be evaluated for\n",
      "        :type dep_context: DepContext\n",
      "Wrapper around the private _get_dep_statuses method that contains some global\n",
      "        checks for all dependencies.\n",
      "\n",
      "        :param ti: the task instance to get the dependency status for\n",
      "        :type ti: airflow.models.TaskInstance\n",
      "        :param session: database session\n",
      "        :type session: sqlalchemy.orm.session.Session\n",
      "        :param dep_context: the context for which this dependency should be evaluated for\n",
      "        :type dep_context: DepContext\n",
      "{'ti': '`ti`', 'session': '`session`', 'dep_context': '`dep_context`'}\n",
      "Wrapper around the private _get_dep_statuses method that contains some global\n",
      "        checks for all dependencies.\n",
      "\n",
      "        :param `ti`: the task instance to get the dependency status for\n",
      "        :type `ti`: airflow.models.TaskInstance\n",
      "        :param `session`: database `session`\n",
      "        :type `session`: sqlalchemy.orm.`session`.Session\n",
      "        :param `dep_context`: the context for which this dependency should be evaluated for\n",
      "        :type `dep_context`: DepContext\n",
      "Returns whether or not this dependency is met for a given task instance. A\n",
      "        dependency is considered met if all of the dependency statuses it reports are\n",
      "        passing.\n",
      "\n",
      "        :param ti: the task instance to see if this dependency is met for\n",
      "        :type ti: airflow.models.TaskInstance\n",
      "        :param session: database session\n",
      "        :type session: sqlalchemy.orm.session.Session\n",
      "        :param dep_context: The context this dependency is being checked under that stores\n",
      "            state that can be used by this dependency.\n",
      "        :type dep_context: BaseDepContext\n",
      "Returns whether or not this dependency is met for a given task instance. A\n",
      "        dependency is considered met if all of the dependency statuses it reports are\n",
      "        passing.\n",
      "\n",
      "        :param ti: the task instance to see if this dependency is met for\n",
      "        :type ti: airflow.models.TaskInstance\n",
      "        :param session: database session\n",
      "        :type session: sqlalchemy.orm.session.Session\n",
      "        :param dep_context: The context this dependency is being checked under that stores\n",
      "            state that can be used by this dependency.\n",
      "        :type dep_context: BaseDepContext\n",
      "{'ti': '`ti`', 'session': '`session`', 'dep_context': '`dep_context`'}\n",
      "Returns whether or not this dependency is met for a given task instance. A\n",
      "        dependency is considered met if all of the dependency statuses it reports are\n",
      "        passing.\n",
      "\n",
      "        :param `ti`: the task instance to see if this dependency is met for\n",
      "        :type `ti`: airflow.models.TaskInstance\n",
      "        :param `session`: database `session`\n",
      "        :type `session`: sqlalchemy.orm.`session`.Session\n",
      "        :param `dep_context`: The context this dependency is being checked under that stores\n",
      "            state that can be used by this dependency.\n",
      "        :type `dep_context`: BaseDepContext\n",
      "Returns an iterable of strings that explain why this dependency wasn't met.\n",
      "\n",
      "        :param ti: the task instance to see if this dependency is met for\n",
      "        :type ti: airflow.models.TaskInstance\n",
      "        :param session: database session\n",
      "        :type session: sqlalchemy.orm.session.Session\n",
      "        :param dep_context: The context this dependency is being checked under that stores\n",
      "            state that can be used by this dependency.\n",
      "        :type dep_context: BaseDepContext\n",
      "Returns an iterable of strings that explain why this dependency wasn't met.\n",
      "\n",
      "        :param ti: the task instance to see if this dependency is met for\n",
      "        :type ti: airflow.models.TaskInstance\n",
      "        :param session: database session\n",
      "        :type session: sqlalchemy.orm.session.Session\n",
      "        :param dep_context: The context this dependency is being checked under that stores\n",
      "            state that can be used by this dependency.\n",
      "        :type dep_context: BaseDepContext\n",
      "{'ti': '`ti`', 'session': '`session`', 'dep_context': '`dep_context`'}\n",
      "Returns an iterable of strings that explain why this dependency wasn't met.\n",
      "\n",
      "        :param `ti`: the task instance to see if this dependency is met for\n",
      "        :type `ti`: airflow.models.TaskInstance\n",
      "        :param `session`: database `session`\n",
      "        :type `session`: sqlalchemy.orm.`session`.Session\n",
      "        :param `dep_context`: The context this dependency is being checked under that stores\n",
      "            state that can be used by this dependency.\n",
      "        :type `dep_context`: BaseDepContext\n",
      "Parses a config file for s3 credentials. Can currently\n",
      "    parse boto, s3cmd.conf and AWS SDK config formats\n",
      "\n",
      "    :param config_file_name: path to the config file\n",
      "    :type config_file_name: str\n",
      "    :param config_format: config type. One of \"boto\", \"s3cmd\" or \"aws\".\n",
      "        Defaults to \"boto\"\n",
      "    :type config_format: str\n",
      "    :param profile: profile name in AWS type config file\n",
      "    :type profile: str\n",
      "Parses a config file for s3 credentials. Can currently\n",
      "    parse boto, s3cmd.conf and AWS SDK config formats\n",
      "\n",
      "    :param config_file_name: path to the config file\n",
      "    :type config_file_name: str\n",
      "    :param config_format: config type. One of \"boto\", \"s3cmd\" or \"aws\".\n",
      "        Defaults to \"boto\"\n",
      "    :type config_format: str\n",
      "    :param profile: profile name in AWS type config file\n",
      "    :type profile: str\n",
      "{'config_file_name': '`config_file_name`', 'config_format': '`config_format`', 'profile': '`profile`'}\n",
      "Parses a config file for s3 credentials. Can currently\n",
      "    parse boto, s3cmd.conf and AWS SDK config formats\n",
      "\n",
      "    :param `config_file_name`: path to the config file\n",
      "    :type `config_file_name`: str\n",
      "    :param `config_format`: config type. One of \"boto\", \"s3cmd\" or \"aws\".\n",
      "        Defaults to \"boto\"\n",
      "    :type `config_format`: str\n",
      "    :param `profile`: `profile` name in AWS type config file\n",
      "    :type `profile`: str\n",
      "Get the underlying `botocore.Credentials` object.\n",
      "\n",
      "        This contains the following authentication attributes: access_key, secret_key and token.\n",
      "Get the underlying `botocore.Credentials` object.\n",
      "\n",
      "        This contains the following authentication attributes: access_key, secret_key and token.\n",
      "Get the underlying `botocore.Credentials` object.\n",
      "\n",
      "        This contains the following authentication attributes: access_key, secret_key and token.\n",
      "If the IAM role is a role name, get the Amazon Resource Name (ARN) for the role.\n",
      "        If IAM role is already an IAM role ARN, no change is made.\n",
      "\n",
      "        :param role: IAM role name or ARN\n",
      "        :return: IAM role ARN\n",
      "If the IAM role is a role name, get the Amazon Resource Name (ARN) for the role.\n",
      "        If IAM role is already an IAM role ARN, no change is made.\n",
      "\n",
      "        :param role: IAM role name or ARN\n",
      "        :return: IAM role ARN\n",
      "{'role': '`role`'}\n",
      "If the IAM `role` is a `role` name, get the Amazon Resource Name (ARN) for the `role`.\n",
      "        If IAM `role` is already an IAM `role` ARN, no change is made.\n",
      "\n",
      "        :param `role`: IAM `role` name or ARN\n",
      "        :return: IAM `role` ARN\n",
      "Returns verticaql connection object\n",
      "Returns verticaql connection object\n",
      "Returns verticaql connection object\n",
      "Walks the tree of loggers and tries to set the context for each handler\n",
      "    :param logger: logger\n",
      "    :param value: value to set\n",
      "Walks the tree of loggers and tries to set the context for each handler\n",
      "    :param logger: logger\n",
      "    :param value: value to set\n",
      "{'logger': '`logger`', 'value': '`value`'}\n",
      "Walks the tree of `logger`s and tries to set the context for each handler\n",
      "    :param `logger`: `logger`\n",
      "    :param `value`: `value` to set\n",
      "Do whatever it takes to actually log the specified logging record\n",
      "        :param message: message to log\n",
      "Do whatever it takes to actually log the specified logging record\n",
      "        :param message: message to log\n",
      "{'message': '`message`'}\n",
      "Do whatever it takes to actually log the specified logging record\n",
      "        :param `message`: `message` to log\n",
      "Ensure all logging output has been flushed\n",
      "Ensure all logging output has been flushed\n",
      "Ensure all logging output has been flushed\n",
      "If the path contains a folder with a .zip suffix, then\n",
      "    the folder is treated as a zip archive and path to zip is returned.\n",
      "If the path contains a folder with a .zip suffix, then\n",
      "    the folder is treated as a zip archive and path to zip is returned.\n",
      "If the path contains a folder with a .zip suffix, then\n",
      "    the folder is treated as a zip archive and path to zip is returned.\n",
      "Traverse a directory and look for Python files.\n",
      "\n",
      "    :param directory: the directory to traverse\n",
      "    :type directory: unicode\n",
      "    :param safe_mode: whether to use a heuristic to determine whether a file\n",
      "        contains Airflow DAG definitions\n",
      "    :return: a list of paths to Python files in the specified directory\n",
      "    :rtype: list[unicode]\n",
      "Traverse a directory and look for Python files.\n",
      "\n",
      "    :param directory: the directory to traverse\n",
      "    :type directory: unicode\n",
      "    :param safe_mode: whether to use a heuristic to determine whether a file\n",
      "        contains Airflow DAG definitions\n",
      "    :return: a list of paths to Python files in the specified directory\n",
      "    :rtype: list[unicode]\n",
      "{'directory': '`directory`', 'safe_mode': '`safe_mode`'}\n",
      "Traverse a `directory` and look for Python files.\n",
      "\n",
      "    :param `directory`: the `directory` to traverse\n",
      "    :type `directory`: unicode\n",
      "    :param `safe_mode`: whether to use a heuristic to determine whether a file\n",
      "        contains Airflow DAG definitions\n",
      "    :return: a list of paths to Python files in the specified `directory`\n",
      "    :rtype: list[unicode]\n",
      "Construct a TaskInstance from the database based on the primary key\n",
      "\n",
      "        :param session: DB session.\n",
      "        :param lock_for_update: if True, indicates that the database should\n",
      "            lock the TaskInstance (issuing a FOR UPDATE clause) until the\n",
      "            session is committed.\n",
      "Construct a TaskInstance from the database based on the primary key\n",
      "\n",
      "        :param session: DB session.\n",
      "        :param lock_for_update: if True, indicates that the database should\n",
      "            lock the TaskInstance (issuing a FOR UPDATE clause) until the\n",
      "            session is committed.\n",
      "{'session': '`session`', 'lock_for_update': '`lock_for_update`'}\n",
      "Construct a TaskInstance from the database based on the primary key\n",
      "\n",
      "        :param `session`: DB `session`.\n",
      "        :param `lock_for_update`: if True, indicates that the database should\n",
      "            lock the TaskInstance (issuing a FOR UPDATE clause) until the\n",
      "            `session` is committed.\n",
      ":param dag_id: DAG ID\n",
      "        :type dag_id: unicode\n",
      "        :return: if the given DAG ID exists in the bag, return the BaseDag\n",
      "        corresponding to that ID. Otherwise, throw an Exception\n",
      "        :rtype: airflow.utils.dag_processing.SimpleDag\n",
      ":param dag_id: DAG ID\n",
      "        :type dag_id: unicode\n",
      "        :return: if the given DAG ID exists in the bag, return the BaseDag\n",
      "        corresponding to that ID. Otherwise, throw an Exception\n",
      "        :rtype: airflow.utils.dag_processing.SimpleDag\n",
      "{'dag_id': '`dag_id`'}\n",
      ":param `dag_id`: DAG ID\n",
      "        :type `dag_id`: unicode\n",
      "        :return: if the given DAG ID exists in the bag, return the BaseDag\n",
      "        corresponding to that ID. Otherwise, throw an Exception\n",
      "        :rtype: airflow.utils.dag_processing.SimpleDag\n",
      "Launch DagFileProcessorManager processor and start DAG parsing loop in manager.\n",
      "Launch DagFileProcessorManager processor and start DAG parsing loop in manager.\n",
      "Launch DagFileProcessorManager processor and start DAG parsing loop in manager.\n",
      "Harvest DAG parsing results from result queue and sync metadata from stat queue.\n",
      "        :return: List of parsing result in SimpleDag format.\n",
      "Harvest DAG parsing results from result queue and sync metadata from stat queue.\n",
      "        :return: List of parsing result in SimpleDag format.\n",
      "Harvest DAG parsing results from result queue and sync metadata from stat queue.\n",
      "        :return: List of parsing result in SimpleDag format.\n",
      "Heartbeat DAG file processor and start it if it is not alive.\n",
      "        :return:\n",
      "Heartbeat DAG file processor and start it if it is not alive.\n",
      "        :return:\n",
      "Heartbeat DAG file processor and start it if it is not alive.\n",
      "        :return:\n",
      "Sync metadata from stat queue and only keep the latest stat.\n",
      "        :return:\n",
      "Sync metadata from stat queue and only keep the latest stat.\n",
      "        :return:\n",
      "Sync metadata from stat queue and only keep the latest stat.\n",
      "        :return:\n",
      "Send termination signal to DAG parsing processor manager\n",
      "        and expect it to terminate all DAG file processors.\n",
      "Send termination signal to DAG parsing processor manager\n",
      "        and expect it to terminate all DAG file processors.\n",
      "Send termination signal to DAG parsing processor manager\n",
      "        and expect it to terminate all DAG file processors.\n",
      "Terminate (and then kill) the manager process launched.\n",
      "        :return:\n",
      "Terminate (and then kill) the manager process launched.\n",
      "        :return:\n",
      "Terminate (and then kill) the manager process launched.\n",
      "        :return:\n",
      "Helper method to clean up DAG file processors to avoid leaving orphan processes.\n",
      "Helper method to clean up DAG file processors to avoid leaving orphan processes.\n",
      "Helper method to clean up DAG file processors to avoid leaving orphan processes.\n",
      "Use multiple processes to parse and generate tasks for the\n",
      "        DAGs in parallel. By processing them in separate processes,\n",
      "        we can get parallelism and isolation from potentially harmful\n",
      "        user code.\n",
      "Use multiple processes to parse and generate tasks for the\n",
      "        DAGs in parallel. By processing them in separate processes,\n",
      "        we can get parallelism and isolation from potentially harmful\n",
      "        user code.\n",
      "Use multiple processes to parse and generate tasks for the\n",
      "        DAGs in parallel. By processing them in separate processes,\n",
      "        we can get parallelism and isolation from potentially harmful\n",
      "        user code.\n",
      "Parse DAG files repeatedly in a standalone loop.\n",
      "Parse DAG files repeatedly in a standalone loop.\n",
      "Parse DAG files repeatedly in a standalone loop.\n",
      "Parse DAG files in a loop controlled by DagParsingSignal.\n",
      "        Actual DAG parsing loop will run once upon receiving one\n",
      "        agent heartbeat message and will report done when finished the loop.\n",
      "Parse DAG files in a loop controlled by DagParsingSignal.\n",
      "        Actual DAG parsing loop will run once upon receiving one\n",
      "        agent heartbeat message and will report done when finished the loop.\n",
      "Parse DAG files in a loop controlled by DagParsingSignal.\n",
      "        Actual DAG parsing loop will run once upon receiving one\n",
      "        agent heartbeat message and will report done when finished the loop.\n",
      "Refresh file paths from dag dir if we haven't done it for too long.\n",
      "Refresh file paths from dag dir if we haven't done it for too long.\n",
      "Refresh file paths from dag dir if we haven't done it for too long.\n",
      "Occasionally print out stats about how fast the files are getting processed\n",
      "Occasionally print out stats about how fast the files are getting processed\n",
      "Occasionally print out stats about how fast the files are getting processed\n",
      "Clears import errors for files that no longer exist.\n",
      "\n",
      "        :param session: session for ORM operations\n",
      "        :type session: sqlalchemy.orm.session.Session\n",
      "Clears import errors for files that no longer exist.\n",
      "\n",
      "        :param session: session for ORM operations\n",
      "        :type session: sqlalchemy.orm.session.Session\n",
      "{'session': '`session`'}\n",
      "Clears import errors for files that no longer exist.\n",
      "\n",
      "        :param `session`: `session` for ORM operations\n",
      "        :type `session`: sqlalchemy.orm.`session`.Session\n",
      "Print out stats about how files are getting processed.\n",
      "\n",
      "        :param known_file_paths: a list of file paths that may contain Airflow\n",
      "            DAG definitions\n",
      "        :type known_file_paths: list[unicode]\n",
      "        :return: None\n",
      "Print out stats about how files are getting processed.\n",
      "\n",
      "        :param known_file_paths: a list of file paths that may contain Airflow\n",
      "            DAG definitions\n",
      "        :type known_file_paths: list[unicode]\n",
      "        :return: None\n",
      "{'known_file_paths': '`known_file_paths`'}\n",
      "Print out stats about how files are getting processed.\n",
      "\n",
      "        :param `known_file_paths`: a list of file paths that may contain Airflow\n",
      "            DAG definitions\n",
      "        :type `known_file_paths`: list[unicode]\n",
      "        :return: None\n",
      ":param file_path: the path to the file that's being processed\n",
      "        :type file_path: unicode\n",
      "        :return: the PID of the process processing the given file or None if\n",
      "            the specified file is not being processed\n",
      "        :rtype: int\n",
      ":param file_path: the path to the file that's being processed\n",
      "        :type file_path: unicode\n",
      "        :return: the PID of the process processing the given file or None if\n",
      "            the specified file is not being processed\n",
      "        :rtype: int\n",
      "{'file_path': '`file_path`'}\n",
      ":param `file_path`: the path to the file that's being processed\n",
      "        :type `file_path`: unicode\n",
      "        :return: the PID of the process processing the given file or None if\n",
      "            the specified file is not being processed\n",
      "        :rtype: int\n",
      ":param file_path: the path to the file that's being processed\n",
      "        :type file_path: unicode\n",
      "        :return: the current runtime (in seconds) of the process that's\n",
      "            processing the specified file or None if the file is not currently\n",
      "            being processed\n",
      ":param file_path: the path to the file that's being processed\n",
      "        :type file_path: unicode\n",
      "        :return: the current runtime (in seconds) of the process that's\n",
      "            processing the specified file or None if the file is not currently\n",
      "            being processed\n",
      "{'file_path': '`file_path`'}\n",
      ":param `file_path`: the path to the file that's being processed\n",
      "        :type `file_path`: unicode\n",
      "        :return: the current runtime (in seconds) of the process that's\n",
      "            processing the specified file or None if the file is not currently\n",
      "            being processed\n",
      ":param file_path: the path to the file that's being processed\n",
      "        :type file_path: unicode\n",
      "        :return: the start time of the process that's processing the\n",
      "            specified file or None if the file is not currently being processed\n",
      "        :rtype: datetime\n",
      ":param file_path: the path to the file that's being processed\n",
      "        :type file_path: unicode\n",
      "        :return: the start time of the process that's processing the\n",
      "            specified file or None if the file is not currently being processed\n",
      "        :rtype: datetime\n",
      "{'file_path': '`file_path`'}\n",
      ":param `file_path`: the path to the file that's being processed\n",
      "        :type `file_path`: unicode\n",
      "        :return: the start time of the process that's processing the\n",
      "            specified file or None if the file is not currently being processed\n",
      "        :rtype: datetime\n",
      "Update this with a new set of paths to DAG definition files.\n",
      "\n",
      "        :param new_file_paths: list of paths to DAG definition files\n",
      "        :type new_file_paths: list[unicode]\n",
      "        :return: None\n",
      "Update this with a new set of paths to DAG definition files.\n",
      "\n",
      "        :param new_file_paths: list of paths to DAG definition files\n",
      "        :type new_file_paths: list[unicode]\n",
      "        :return: None\n",
      "{'new_file_paths': '`new_file_paths`'}\n",
      "Update this with a new set of paths to DAG definition files.\n",
      "\n",
      "        :param `new_file_paths`: list of paths to DAG definition files\n",
      "        :type `new_file_paths`: list[unicode]\n",
      "        :return: None\n",
      "Sleeps until all the processors are done.\n",
      "Sleeps until all the processors are done.\n",
      "Sleeps until all the processors are done.\n",
      "This should be periodically called by the manager loop. This method will\n",
      "        kick off new processes to process DAG definition files and read the\n",
      "        results from the finished processors.\n",
      "\n",
      "        :return: a list of SimpleDags that were produced by processors that\n",
      "            have finished since the last time this was called\n",
      "        :rtype: list[airflow.utils.dag_processing.SimpleDag]\n",
      "This should be periodically called by the manager loop. This method will\n",
      "        kick off new processes to process DAG definition files and read the\n",
      "        results from the finished processors.\n",
      "\n",
      "        :return: a list of SimpleDags that were produced by processors that\n",
      "            have finished since the last time this was called\n",
      "        :rtype: list[airflow.utils.dag_processing.SimpleDag]\n",
      "This should be periodically called by the manager loop. This method will\n",
      "        kick off new processes to process DAG definition files and read the\n",
      "        results from the finished processors.\n",
      "\n",
      "        :return: a list of SimpleDags that were produced by processors that\n",
      "            have finished since the last time this was called\n",
      "        :rtype: list[airflow.utils.dag_processing.SimpleDag]\n",
      "Find zombie task instances, which are tasks haven't heartbeated for too long.\n",
      "        :return: Zombie task instances in SimpleTaskInstance format.\n",
      "Find zombie task instances, which are tasks haven't heartbeated for too long.\n",
      "        :return: Zombie task instances in SimpleTaskInstance format.\n",
      "Find zombie task instances, which are tasks haven't heartbeated for too long.\n",
      "        :return: Zombie task instances in SimpleTaskInstance format.\n",
      ":return: whether all file paths have been processed max_runs times\n",
      ":return: whether all file paths have been processed max_runs times\n",
      ":return: whether all file paths have been processed max_runs times\n",
      "Kill all child processes on exit since we don't want to leave\n",
      "        them as orphaned.\n",
      "Kill all child processes on exit since we don't want to leave\n",
      "        them as orphaned.\n",
      "Kill all child processes on exit since we don't want to leave\n",
      "        them as orphaned.\n",
      "Opens a ssh connection to the remote host.\n",
      "\n",
      "        :rtype: paramiko.client.SSHClient\n",
      "Opens a ssh connection to the remote host.\n",
      "\n",
      "        :rtype: paramiko.client.SSHClient\n",
      "Opens a ssh connection to the remote host.\n",
      "\n",
      "        :rtype: paramiko.client.SSHClient\n",
      "Creates a tunnel between two hosts. Like ssh -L <LOCAL_PORT>:host:<REMOTE_PORT>.\n",
      "\n",
      "        :param remote_port: The remote port to create a tunnel to\n",
      "        :type remote_port: int\n",
      "        :param remote_host: The remote host to create a tunnel to (default localhost)\n",
      "        :type remote_host: str\n",
      "        :param local_port:  The local port to attach the tunnel to\n",
      "        :type local_port: int\n",
      "\n",
      "        :return: sshtunnel.SSHTunnelForwarder object\n",
      "Creates a tunnel between two hosts. Like ssh -L <LOCAL_PORT>:host:<REMOTE_PORT>.\n",
      "\n",
      "        :param remote_port: The remote port to create a tunnel to\n",
      "        :type remote_port: int\n",
      "        :param remote_host: The remote host to create a tunnel to (default localhost)\n",
      "        :type remote_host: str\n",
      "        :param local_port:  The local port to attach the tunnel to\n",
      "        :type local_port: int\n",
      "\n",
      "        :return: sshtunnel.SSHTunnelForwarder object\n",
      "{'remote_port': '`remote_port`', 'remote_host': '`remote_host`', 'local_port': '`local_port`'}\n",
      "Creates a tunnel between two hosts. Like ssh -L <LOCAL_PORT>:host:<REMOTE_PORT>.\n",
      "\n",
      "        :param `remote_port`: The remote port to create a tunnel to\n",
      "        :type `remote_port`: int\n",
      "        :param `remote_host`: The remote host to create a tunnel to (default localhost)\n",
      "        :type `remote_host`: str\n",
      "        :param `local_port`:  The local port to attach the tunnel to\n",
      "        :type `local_port`: int\n",
      "\n",
      "        :return: sshtunnel.SSHTunnelForwarder object\n",
      "Creates a transfer job that runs periodically.\n",
      "\n",
      "        :param body: (Required) A request body, as described in\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body\n",
      "        :type body: dict\n",
      "        :return: transfer job.\n",
      "            See:\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob\n",
      "        :rtype: dict\n",
      "Creates a transfer job that runs periodically.\n",
      "\n",
      "        :param body: (Required) A request body, as described in\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body\n",
      "        :type body: dict\n",
      "        :return: transfer job.\n",
      "            See:\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob\n",
      "        :rtype: dict\n",
      "{'body': '`body`'}\n",
      "Creates a transfer job that runs periodically.\n",
      "\n",
      "        :param `body`: (Required) A request `body`, as described in\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-`body`\n",
      "        :type `body`: dict\n",
      "        :return: transfer job.\n",
      "            See:\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob\n",
      "        :rtype: dict\n",
      "Gets the latest state of a long-running operation in Google Storage\n",
      "        Transfer Service.\n",
      "\n",
      "        :param job_name: (Required) Name of the job to be fetched\n",
      "        :type job_name: str\n",
      "        :param project_id: (Optional) the ID of the project that owns the Transfer\n",
      "            Job. If set to None or missing, the default project_id from the GCP\n",
      "            connection is used.\n",
      "        :type project_id: str\n",
      "        :return: Transfer Job\n",
      "        :rtype: dict\n",
      "Gets the latest state of a long-running operation in Google Storage\n",
      "        Transfer Service.\n",
      "\n",
      "        :param job_name: (Required) Name of the job to be fetched\n",
      "        :type job_name: str\n",
      "        :param project_id: (Optional) the ID of the project that owns the Transfer\n",
      "            Job. If set to None or missing, the default project_id from the GCP\n",
      "            connection is used.\n",
      "        :type project_id: str\n",
      "        :return: Transfer Job\n",
      "        :rtype: dict\n",
      "{'job_name': '`job_name`', 'project_id': '`project_id`'}\n",
      "Gets the latest state of a long-running operation in Google Storage\n",
      "        Transfer Service.\n",
      "\n",
      "        :param `job_name`: (Required) Name of the job to be fetched\n",
      "        :type `job_name`: str\n",
      "        :param `project_id`: (Optional) the ID of the project that owns the Transfer\n",
      "            Job. If set to None or missing, the default `project_id` from the GCP\n",
      "            connection is used.\n",
      "        :type `project_id`: str\n",
      "        :return: Transfer Job\n",
      "        :rtype: dict\n",
      "Lists long-running operations in Google Storage Transfer\n",
      "        Service that match the specified filter.\n",
      "\n",
      "        :param filter: (Required) A request filter, as described in\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/list#body.QUERY_PARAMETERS.filter\n",
      "        :type filter: dict\n",
      "        :return: List of Transfer Jobs\n",
      "        :rtype: list[dict]\n",
      "Lists long-running operations in Google Storage Transfer\n",
      "        Service that match the specified filter.\n",
      "\n",
      "        :param filter: (Required) A request filter, as described in\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/list#body.QUERY_PARAMETERS.filter\n",
      "        :type filter: dict\n",
      "        :return: List of Transfer Jobs\n",
      "        :rtype: list[dict]\n",
      "{'filter': '`filter`'}\n",
      "Lists long-running operations in Google Storage Transfer\n",
      "        Service that match the specified `filter`.\n",
      "\n",
      "        :param `filter`: (Required) A request `filter`, as described in\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/list#body.QUERY_PARAMETERS.`filter`\n",
      "        :type `filter`: dict\n",
      "        :return: List of Transfer Jobs\n",
      "        :rtype: list[dict]\n",
      "Updates a transfer job that runs periodically.\n",
      "\n",
      "        :param job_name: (Required) Name of the job to be updated\n",
      "        :type job_name: str\n",
      "        :param body: A request body, as described in\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body\n",
      "        :type body: dict\n",
      "        :return: If successful, TransferJob.\n",
      "        :rtype: dict\n",
      "Updates a transfer job that runs periodically.\n",
      "\n",
      "        :param job_name: (Required) Name of the job to be updated\n",
      "        :type job_name: str\n",
      "        :param body: A request body, as described in\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-body\n",
      "        :type body: dict\n",
      "        :return: If successful, TransferJob.\n",
      "        :rtype: dict\n",
      "{'job_name': '`job_name`', 'body': '`body`'}\n",
      "Updates a transfer job that runs periodically.\n",
      "\n",
      "        :param `job_name`: (Required) Name of the job to be updated\n",
      "        :type `job_name`: str\n",
      "        :param `body`: A request `body`, as described in\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/patch#request-`body`\n",
      "        :type `body`: dict\n",
      "        :return: If successful, TransferJob.\n",
      "        :rtype: dict\n",
      "Deletes a transfer job. This is a soft delete. After a transfer job is\n",
      "        deleted, the job and all the transfer executions are subject to garbage\n",
      "        collection. Transfer jobs become eligible for garbage collection\n",
      "        30 days after soft delete.\n",
      "\n",
      "        :param job_name: (Required) Name of the job to be deleted\n",
      "        :type job_name: str\n",
      "        :param project_id: (Optional) the ID of the project that owns the Transfer\n",
      "            Job. If set to None or missing, the default project_id from the GCP\n",
      "            connection is used.\n",
      "        :type project_id: str\n",
      "        :rtype: None\n",
      "Deletes a transfer job. This is a soft delete. After a transfer job is\n",
      "        deleted, the job and all the transfer executions are subject to garbage\n",
      "        collection. Transfer jobs become eligible for garbage collection\n",
      "        30 days after soft delete.\n",
      "\n",
      "        :param job_name: (Required) Name of the job to be deleted\n",
      "        :type job_name: str\n",
      "        :param project_id: (Optional) the ID of the project that owns the Transfer\n",
      "            Job. If set to None or missing, the default project_id from the GCP\n",
      "            connection is used.\n",
      "        :type project_id: str\n",
      "        :rtype: None\n",
      "{'job_name': '`job_name`', 'project_id': '`project_id`'}\n",
      "Deletes a transfer job. This is a soft delete. After a transfer job is\n",
      "        deleted, the job and all the transfer executions are subject to garbage\n",
      "        collection. Transfer jobs become eligible for garbage collection\n",
      "        30 days after soft delete.\n",
      "\n",
      "        :param `job_name`: (Required) Name of the job to be deleted\n",
      "        :type `job_name`: str\n",
      "        :param `project_id`: (Optional) the ID of the project that owns the Transfer\n",
      "            Job. If set to None or missing, the default `project_id` from the GCP\n",
      "            connection is used.\n",
      "        :type `project_id`: str\n",
      "        :rtype: None\n",
      "Cancels an transfer operation in Google Storage Transfer Service.\n",
      "\n",
      "        :param operation_name: Name of the transfer operation.\n",
      "        :type operation_name: str\n",
      "        :rtype: None\n",
      "Cancels an transfer operation in Google Storage Transfer Service.\n",
      "\n",
      "        :param operation_name: Name of the transfer operation.\n",
      "        :type operation_name: str\n",
      "        :rtype: None\n",
      "{'operation_name': '`operation_name`'}\n",
      "Cancels an transfer operation in Google Storage Transfer Service.\n",
      "\n",
      "        :param `operation_name`: Name of the transfer operation.\n",
      "        :type `operation_name`: str\n",
      "        :rtype: None\n",
      "Gets an transfer operation in Google Storage Transfer Service.\n",
      "\n",
      "        :param operation_name: (Required) Name of the transfer operation.\n",
      "        :type operation_name: str\n",
      "        :return: transfer operation\n",
      "            See:\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/Operation\n",
      "        :rtype: dict\n",
      "Gets an transfer operation in Google Storage Transfer Service.\n",
      "\n",
      "        :param operation_name: (Required) Name of the transfer operation.\n",
      "        :type operation_name: str\n",
      "        :return: transfer operation\n",
      "            See:\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/Operation\n",
      "        :rtype: dict\n",
      "{'operation_name': '`operation_name`'}\n",
      "Gets an transfer operation in Google Storage Transfer Service.\n",
      "\n",
      "        :param `operation_name`: (Required) Name of the transfer operation.\n",
      "        :type `operation_name`: str\n",
      "        :return: transfer operation\n",
      "            See:\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/Operation\n",
      "        :rtype: dict\n",
      "Gets an transfer operation in Google Storage Transfer Service.\n",
      "\n",
      "        :param filter: (Required) A request filter, as described in\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/list#body.QUERY_PARAMETERS.filter\n",
      "            With one additional improvement:\n",
      "\n",
      "            * project_id is optional if you have a project id defined\n",
      "              in the connection\n",
      "              See: :ref:`howto/connection:gcp`\n",
      "\n",
      "        :type filter: dict\n",
      "        :return: transfer operation\n",
      "        :rtype: list[dict]\n",
      "Gets an transfer operation in Google Storage Transfer Service.\n",
      "\n",
      "        :param filter: (Required) A request filter, as described in\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/list#body.QUERY_PARAMETERS.filter\n",
      "            With one additional improvement:\n",
      "\n",
      "            * project_id is optional if you have a project id defined\n",
      "              in the connection\n",
      "              See: :ref:`howto/connection:gcp`\n",
      "\n",
      "        :type filter: dict\n",
      "        :return: transfer operation\n",
      "        :rtype: list[dict]\n",
      "{'filter': '`filter`'}\n",
      "Gets an transfer operation in Google Storage Transfer Service.\n",
      "\n",
      "        :param `filter`: (Required) A request `filter`, as described in\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs/list#body.QUERY_PARAMETERS.`filter`\n",
      "            With one additional improvement:\n",
      "\n",
      "            * project_id is optional if you have a project id defined\n",
      "              in the connection\n",
      "              See: :ref:`howto/connection:gcp`\n",
      "\n",
      "        :type `filter`: dict\n",
      "        :return: transfer operation\n",
      "        :rtype: list[dict]\n",
      "Pauses an transfer operation in Google Storage Transfer Service.\n",
      "\n",
      "        :param operation_name: (Required) Name of the transfer operation.\n",
      "        :type operation_name: str\n",
      "        :rtype: None\n",
      "Pauses an transfer operation in Google Storage Transfer Service.\n",
      "\n",
      "        :param operation_name: (Required) Name of the transfer operation.\n",
      "        :type operation_name: str\n",
      "        :rtype: None\n",
      "{'operation_name': '`operation_name`'}\n",
      "Pauses an transfer operation in Google Storage Transfer Service.\n",
      "\n",
      "        :param `operation_name`: (Required) Name of the transfer operation.\n",
      "        :type `operation_name`: str\n",
      "        :rtype: None\n",
      "Resumes an transfer operation in Google Storage Transfer Service.\n",
      "\n",
      "        :param operation_name: (Required) Name of the transfer operation.\n",
      "        :type operation_name: str\n",
      "        :rtype: None\n",
      "Resumes an transfer operation in Google Storage Transfer Service.\n",
      "\n",
      "        :param operation_name: (Required) Name of the transfer operation.\n",
      "        :type operation_name: str\n",
      "        :rtype: None\n",
      "{'operation_name': '`operation_name`'}\n",
      "Resumes an transfer operation in Google Storage Transfer Service.\n",
      "\n",
      "        :param `operation_name`: (Required) Name of the transfer operation.\n",
      "        :type `operation_name`: str\n",
      "        :rtype: None\n",
      "Waits until the job reaches the expected state.\n",
      "\n",
      "        :param job: Transfer job\n",
      "            See:\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob\n",
      "        :type job: dict\n",
      "        :param expected_statuses: State that is expected\n",
      "            See:\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferOperations#Status\n",
      "        :type expected_statuses: set[str]\n",
      "        :param timeout:\n",
      "        :type timeout: time in which the operation must end in seconds\n",
      "        :rtype: None\n",
      "Waits until the job reaches the expected state.\n",
      "\n",
      "        :param job: Transfer job\n",
      "            See:\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob\n",
      "        :type job: dict\n",
      "        :param expected_statuses: State that is expected\n",
      "            See:\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferOperations#Status\n",
      "        :type expected_statuses: set[str]\n",
      "        :param timeout:\n",
      "        :type timeout: time in which the operation must end in seconds\n",
      "        :rtype: None\n",
      "{'job': '`job`', 'expected_statuses': '`expected_statuses`', 'timeout': '`timeout`'}\n",
      "Waits until the `job` reaches the expected state.\n",
      "\n",
      "        :param `job`: Transfer `job`\n",
      "            See:\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferJobs#TransferJob\n",
      "        :type `job`: dict\n",
      "        :param `expected_statuses`: State that is expected\n",
      "            See:\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferOperations#Status\n",
      "        :type `expected_statuses`: set[str]\n",
      "        :param `timeout`:\n",
      "        :type `timeout`: time in which the operation must end in seconds\n",
      "        :rtype: None\n",
      "Checks whether the operation list has an operation with the\n",
      "        expected status, then returns true\n",
      "        If it encounters operations in FAILED or ABORTED state\n",
      "        throw :class:`airflow.exceptions.AirflowException`.\n",
      "\n",
      "        :param operations: (Required) List of transfer operations to check.\n",
      "        :type operations: list[dict]\n",
      "        :param expected_statuses: (Required) status that is expected\n",
      "            See:\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferOperations#Status\n",
      "        :type expected_statuses: set[str]\n",
      "        :return: If there is an operation with the expected state\n",
      "            in the operation list, returns true,\n",
      "        :raises: airflow.exceptions.AirflowException If it encounters operations\n",
      "            with a state in the list,\n",
      "        :rtype: bool\n",
      "Checks whether the operation list has an operation with the\n",
      "        expected status, then returns true\n",
      "        If it encounters operations in FAILED or ABORTED state\n",
      "        throw :class:`airflow.exceptions.AirflowException`.\n",
      "\n",
      "        :param operations: (Required) List of transfer operations to check.\n",
      "        :type operations: list[dict]\n",
      "        :param expected_statuses: (Required) status that is expected\n",
      "            See:\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferOperations#Status\n",
      "        :type expected_statuses: set[str]\n",
      "        :return: If there is an operation with the expected state\n",
      "            in the operation list, returns true,\n",
      "        :raises: airflow.exceptions.AirflowException If it encounters operations\n",
      "            with a state in the list,\n",
      "        :rtype: bool\n",
      "{'operations': '`operations`', 'expected_statuses': '`expected_statuses`'}\n",
      "Checks whether the operation list has an operation with the\n",
      "        expected status, then returns true\n",
      "        If it encounters `operations` in FAILED or ABORTED state\n",
      "        throw :class:`airflow.exceptions.AirflowException`.\n",
      "\n",
      "        :param `operations`: (Required) List of transfer `operations` to check.\n",
      "        :type `operations`: list[dict]\n",
      "        :param `expected_statuses`: (Required) status that is expected\n",
      "            See:\n",
      "            https://cloud.google.com/storage-transfer/docs/reference/rest/v1/transferOperations#Status\n",
      "        :type `expected_statuses`: set[str]\n",
      "        :return: If there is an operation with the expected state\n",
      "            in the operation list, returns true,\n",
      "        :raises: airflow.exceptions.AirflowException If it encounters `operations`\n",
      "            with a state in the list,\n",
      "        :rtype: bool\n",
      "Returns all task reschedules for the task instance and try number,\n",
      "        in ascending order.\n",
      "\n",
      "        :param task_instance: the task instance to find task reschedules for\n",
      "        :type task_instance: airflow.models.TaskInstance\n",
      "Returns all task reschedules for the task instance and try number,\n",
      "        in ascending order.\n",
      "\n",
      "        :param task_instance: the task instance to find task reschedules for\n",
      "        :type task_instance: airflow.models.TaskInstance\n",
      "{'task_instance': '`task_instance`'}\n",
      "Returns all task reschedules for the task instance and try number,\n",
      "        in ascending order.\n",
      "\n",
      "        :param `task_instance`: the task instance to find task reschedules for\n",
      "        :type `task_instance`: airflow.models.TaskInstance\n",
      "Kubernetes only supports lowercase alphanumeric characters and \"-\" and \".\" in\n",
      "        the pod name\n",
      "        However, there are special rules about how \"-\" and \".\" can be used so let's\n",
      "        only keep\n",
      "        alphanumeric chars  see here for detail:\n",
      "        https://kubernetes.io/docs/concepts/overview/working-with-objects/names/\n",
      "\n",
      "        :param string: The requested Pod name\n",
      "        :return: ``str`` Pod name stripped of any unsafe characters\n",
      "Kubernetes only supports lowercase alphanumeric characters and \"-\" and \".\" in\n",
      "        the pod name\n",
      "        However, there are special rules about how \"-\" and \".\" can be used so let's\n",
      "        only keep\n",
      "        alphanumeric chars  see here for detail:\n",
      "        https://kubernetes.io/docs/concepts/overview/working-with-objects/names/\n",
      "\n",
      "        :param string: The requested Pod name\n",
      "        :return: ``str`` Pod name stripped of any unsafe characters\n",
      "{'string': '`string`'}\n",
      "Kubernetes only supports lowercase alphanumeric characters and \"-\" and \".\" in\n",
      "        the pod name\n",
      "        However, there are special rules about how \"-\" and \".\" can be used so let's\n",
      "        only keep\n",
      "        alphanumeric chars  see here for detail:\n",
      "        https://kubernetes.io/docs/concepts/overview/working-with-objects/names/\n",
      "\n",
      "        :param `string`: The requested Pod name\n",
      "        :return: ``str`` Pod name stripped of any unsafe characters\n",
      "Kubernetes pod names must be <= 253 chars and must pass the following regex for\n",
      "        validation\n",
      "        \"^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$\"\n",
      "\n",
      "        :param safe_dag_id: a dag_id with only alphanumeric characters\n",
      "        :param safe_task_id: a task_id with only alphanumeric characters\n",
      "        :param random_uuid: a uuid\n",
      "        :return: ``str`` valid Pod name of appropriate length\n",
      "Kubernetes pod names must be <= 253 chars and must pass the following regex for\n",
      "        validation\n",
      "        \"^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$\"\n",
      "\n",
      "        :param safe_dag_id: a dag_id with only alphanumeric characters\n",
      "        :param safe_task_id: a task_id with only alphanumeric characters\n",
      "        :param random_uuid: a uuid\n",
      "        :return: ``str`` valid Pod name of appropriate length\n",
      "{'safe_dag_id': '`safe_dag_id`', 'safe_task_id': '`safe_task_id`', 'random_uuid': '`random_uuid`'}\n",
      "Kubernetes pod names must be <= 253 chars and must pass the following regex for\n",
      "        validation\n",
      "        \"^[a-z0-9]([-a-z0-9]*[a-z0-9])?(\\\\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*$\"\n",
      "\n",
      "        :param `safe_dag_id`: a dag_id with only alphanumeric characters\n",
      "        :param `safe_task_id`: a task_id with only alphanumeric characters\n",
      "        :param `random_uuid`: a uuid\n",
      "        :return: ``str`` valid Pod name of appropriate length\n",
      "Valid label values must be 63 characters or less and must be empty or begin and\n",
      "        end with an alphanumeric character ([a-z0-9A-Z]) with dashes (-), underscores (_),\n",
      "        dots (.), and alphanumerics between.\n",
      "\n",
      "        If the label value is then greater than 63 chars once made safe, or differs in any\n",
      "        way from the original value sent to this function, then we need to truncate to\n",
      "        53chars, and append it with a unique hash.\n",
      "Valid label values must be 63 characters or less and must be empty or begin and\n",
      "        end with an alphanumeric character ([a-z0-9A-Z]) with dashes (-), underscores (_),\n",
      "        dots (.), and alphanumerics between.\n",
      "\n",
      "        If the label value is then greater than 63 chars once made safe, or differs in any\n",
      "        way from the original value sent to this function, then we need to truncate to\n",
      "        53chars, and append it with a unique hash.\n",
      "Valid label values must be 63 characters or less and must be empty or begin and\n",
      "        end with an alphanumeric character ([a-z0-9A-Z]) with dashes (-), underscores (_),\n",
      "        dots (.), and alphanumerics between.\n",
      "\n",
      "        If the label value is then greater than 63 chars once made safe, or differs in any\n",
      "        way from the original value sent to this function, then we need to truncate to\n",
      "        53chars, and append it with a unique hash.\n",
      "If the airflow scheduler restarts with pending \"Queued\" tasks, the tasks may or\n",
      "        may not\n",
      "        have been launched Thus, on starting up the scheduler let's check every\n",
      "        \"Queued\" task to\n",
      "        see if it has been launched (ie: if there is a corresponding pod on kubernetes)\n",
      "\n",
      "        If it has been launched then do nothing, otherwise reset the state to \"None\" so\n",
      "        the task\n",
      "        will be rescheduled\n",
      "\n",
      "        This will not be necessary in a future version of airflow in which there is\n",
      "        proper support\n",
      "        for State.LAUNCHED\n",
      "If the airflow scheduler restarts with pending \"Queued\" tasks, the tasks may or\n",
      "        may not\n",
      "        have been launched Thus, on starting up the scheduler let's check every\n",
      "        \"Queued\" task to\n",
      "        see if it has been launched (ie: if there is a corresponding pod on kubernetes)\n",
      "\n",
      "        If it has been launched then do nothing, otherwise reset the state to \"None\" so\n",
      "        the task\n",
      "        will be rescheduled\n",
      "\n",
      "        This will not be necessary in a future version of airflow in which there is\n",
      "        proper support\n",
      "        for State.LAUNCHED\n",
      "If the airflow scheduler restarts with pending \"Queued\" tasks, the tasks may or\n",
      "        may not\n",
      "        have been launched Thus, on starting up the scheduler let's check every\n",
      "        \"Queued\" task to\n",
      "        see if it has been launched (ie: if there is a corresponding pod on kubernetes)\n",
      "\n",
      "        If it has been launched then do nothing, otherwise reset the state to \"None\" so\n",
      "        the task\n",
      "        will be rescheduled\n",
      "\n",
      "        This will not be necessary in a future version of airflow in which there is\n",
      "        proper support\n",
      "        for State.LAUNCHED\n",
      "Returns the number of slots open at the moment\n",
      "Returns the number of slots open at the moment\n",
      "Returns the number of slots open at the moment\n",
      "Expands (potentially nested) env vars by repeatedly applying\n",
      "    `expandvars` and `expanduser` until interpolation stops having\n",
      "    any effect.\n",
      "Expands (potentially nested) env vars by repeatedly applying\n",
      "    `expandvars` and `expanduser` until interpolation stops having\n",
      "    any effect.\n",
      "Expands (potentially nested) env vars by repeatedly applying\n",
      "    `expandvars` and `expanduser` until interpolation stops having\n",
      "    any effect.\n",
      "Runs command and returns stdout\n",
      "Runs command and returns stdout\n",
      "Runs command and returns stdout\n",
      "Generates a configuration from the provided template + variables defined in\n",
      "    current scope\n",
      "    :param template: a config content templated with {{variables}}\n",
      "Generates a configuration from the provided template + variables defined in\n",
      "    current scope\n",
      "    :param template: a config content templated with {{variables}}\n",
      "{'template': '`template`'}\n",
      "Generates a configuration from the provided `template` + variables defined in\n",
      "    current scope\n",
      "    :param `template`: a config content `template`d with {{variables}}\n",
      "Remove an option if it exists in config from a file or\n",
      "        default config. If both of config have the same option, this removes\n",
      "        the option in both configs unless remove_default=False.\n",
      "Remove an option if it exists in config from a file or\n",
      "        default config. If both of config have the same option, this removes\n",
      "        the option in both configs unless remove_default=False.\n",
      "Remove an option if it exists in config from a file or\n",
      "        default config. If both of config have the same option, this removes\n",
      "        the option in both configs unless remove_default=False.\n",
      "Returns the section as a dict. Values are converted to int, float, bool\n",
      "        as required.\n",
      "\n",
      "        :param section: section from the config\n",
      "        :rtype: dict\n",
      "Returns the section as a dict. Values are converted to int, float, bool\n",
      "        as required.\n",
      "\n",
      "        :param section: section from the config\n",
      "        :rtype: dict\n",
      "{'section': '`section`'}\n",
      "Returns the `section` as a dict. Values are converted to int, float, bool\n",
      "        as required.\n",
      "\n",
      "        :param `section`: `section` from the config\n",
      "        :rtype: dict\n",
      "Returns the current configuration as an OrderedDict of OrderedDicts.\n",
      "        :param display_source: If False, the option value is returned. If True,\n",
      "            a tuple of (option_value, source) is returned. Source is either\n",
      "            'airflow.cfg', 'default', 'env var', or 'cmd'.\n",
      "        :type display_source: bool\n",
      "        :param display_sensitive: If True, the values of options set by env\n",
      "            vars and bash commands will be displayed. If False, those options\n",
      "            are shown as '< hidden >'\n",
      "        :type display_sensitive: bool\n",
      "        :param raw: Should the values be output as interpolated values, or the\n",
      "            \"raw\" form that can be fed back in to ConfigParser\n",
      "        :type raw: bool\n",
      "Returns the current configuration as an OrderedDict of OrderedDicts.\n",
      "        :param display_source: If False, the option value is returned. If True,\n",
      "            a tuple of (option_value, source) is returned. Source is either\n",
      "            'airflow.cfg', 'default', 'env var', or 'cmd'.\n",
      "        :type display_source: bool\n",
      "        :param display_sensitive: If True, the values of options set by env\n",
      "            vars and bash commands will be displayed. If False, those options\n",
      "            are shown as '< hidden >'\n",
      "        :type display_sensitive: bool\n",
      "        :param raw: Should the values be output as interpolated values, or the\n",
      "            \"raw\" form that can be fed back in to ConfigParser\n",
      "        :type raw: bool\n",
      "{'display_source': '`display_source`', 'display_sensitive': '`display_sensitive`', 'raw': '`raw`'}\n",
      "Returns the current configuration as an OrderedDict of OrderedDicts.\n",
      "        :param `display_source`: If False, the option value is returned. If True,\n",
      "            a tuple of (option_value, source) is returned. Source is either\n",
      "            'airflow.cfg', 'default', 'env var', or 'cmd'.\n",
      "        :type `display_source`: bool\n",
      "        :param `display_sensitive`: If True, the values of options set by env\n",
      "            vars and bash commands will be displayed. If False, those options\n",
      "            are shown as '< hidden >'\n",
      "        :type `display_sensitive`: bool\n",
      "        :param `raw`: Should the values be output as interpolated values, or the\n",
      "            \"`raw`\" form that can be fed back in to ConfigParser\n",
      "        :type `raw`: bool\n",
      "Allocate IDs for incomplete keys.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/allocateIds\n",
      "\n",
      "        :param partial_keys: a list of partial keys.\n",
      "        :type partial_keys: list\n",
      "        :return: a list of full keys.\n",
      "        :rtype: list\n",
      "Allocate IDs for incomplete keys.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/allocateIds\n",
      "\n",
      "        :param partial_keys: a list of partial keys.\n",
      "        :type partial_keys: list\n",
      "        :return: a list of full keys.\n",
      "        :rtype: list\n",
      "{'partial_keys': '`partial_keys`'}\n",
      "Allocate IDs for incomplete keys.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/allocateIds\n",
      "\n",
      "        :param `partial_keys`: a list of partial keys.\n",
      "        :type `partial_keys`: list\n",
      "        :return: a list of full keys.\n",
      "        :rtype: list\n",
      "Begins a new transaction.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/beginTransaction\n",
      "\n",
      "        :return: a transaction handle.\n",
      "        :rtype: str\n",
      "Begins a new transaction.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/beginTransaction\n",
      "\n",
      "        :return: a transaction handle.\n",
      "        :rtype: str\n",
      "Begins a new transaction.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/beginTransaction\n",
      "\n",
      "        :return: a transaction handle.\n",
      "        :rtype: str\n",
      "Commit a transaction, optionally creating, deleting or modifying some entities.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/commit\n",
      "\n",
      "        :param body: the body of the commit request.\n",
      "        :type body: dict\n",
      "        :return: the response body of the commit request.\n",
      "        :rtype: dict\n",
      "Commit a transaction, optionally creating, deleting or modifying some entities.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/commit\n",
      "\n",
      "        :param body: the body of the commit request.\n",
      "        :type body: dict\n",
      "        :return: the response body of the commit request.\n",
      "        :rtype: dict\n",
      "{'body': '`body`'}\n",
      "Commit a transaction, optionally creating, deleting or modifying some entities.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/commit\n",
      "\n",
      "        :param `body`: the `body` of the commit request.\n",
      "        :type `body`: dict\n",
      "        :return: the response `body` of the commit request.\n",
      "        :rtype: dict\n",
      "Lookup some entities by key.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/lookup\n",
      "\n",
      "        :param keys: the keys to lookup.\n",
      "        :type keys: list\n",
      "        :param read_consistency: the read consistency to use. default, strong or eventual.\n",
      "                                 Cannot be used with a transaction.\n",
      "        :type read_consistency: str\n",
      "        :param transaction: the transaction to use, if any.\n",
      "        :type transaction: str\n",
      "        :return: the response body of the lookup request.\n",
      "        :rtype: dict\n",
      "Lookup some entities by key.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/lookup\n",
      "\n",
      "        :param keys: the keys to lookup.\n",
      "        :type keys: list\n",
      "        :param read_consistency: the read consistency to use. default, strong or eventual.\n",
      "                                 Cannot be used with a transaction.\n",
      "        :type read_consistency: str\n",
      "        :param transaction: the transaction to use, if any.\n",
      "        :type transaction: str\n",
      "        :return: the response body of the lookup request.\n",
      "        :rtype: dict\n",
      "{'keys': '`keys`', 'read_consistency': '`read_consistency`', 'transaction': '`transaction`'}\n",
      "Lookup some entities by key.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/lookup\n",
      "\n",
      "        :param `keys`: the `keys` to lookup.\n",
      "        :type `keys`: list\n",
      "        :param `read_consistency`: the read consistency to use. default, strong or eventual.\n",
      "                                 Cannot be used with a `transaction`.\n",
      "        :type `read_consistency`: str\n",
      "        :param `transaction`: the `transaction` to use, if any.\n",
      "        :type `transaction`: str\n",
      "        :return: the response body of the lookup request.\n",
      "        :rtype: dict\n",
      "Roll back a transaction.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/rollback\n",
      "\n",
      "        :param transaction: the transaction to roll back.\n",
      "        :type transaction: str\n",
      "Roll back a transaction.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/rollback\n",
      "\n",
      "        :param transaction: the transaction to roll back.\n",
      "        :type transaction: str\n",
      "{'transaction': '`transaction`'}\n",
      "Roll back a `transaction`.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/rollback\n",
      "\n",
      "        :param `transaction`: the `transaction` to roll back.\n",
      "        :type `transaction`: str\n",
      "Run a query for entities.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/runQuery\n",
      "\n",
      "        :param body: the body of the query request.\n",
      "        :type body: dict\n",
      "        :return: the batch of query results.\n",
      "        :rtype: dict\n",
      "Run a query for entities.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/runQuery\n",
      "\n",
      "        :param body: the body of the query request.\n",
      "        :type body: dict\n",
      "        :return: the batch of query results.\n",
      "        :rtype: dict\n",
      "{'body': '`body`'}\n",
      "Run a query for entities.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/rest/v1/projects/runQuery\n",
      "\n",
      "        :param `body`: the `body` of the query request.\n",
      "        :type `body`: dict\n",
      "        :return: the batch of query results.\n",
      "        :rtype: dict\n",
      "Gets the latest state of a long-running operation.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/get\n",
      "\n",
      "        :param name: the name of the operation resource.\n",
      "        :type name: str\n",
      "        :return: a resource operation instance.\n",
      "        :rtype: dict\n",
      "Gets the latest state of a long-running operation.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/get\n",
      "\n",
      "        :param name: the name of the operation resource.\n",
      "        :type name: str\n",
      "        :return: a resource operation instance.\n",
      "        :rtype: dict\n",
      "{'name': '`name`'}\n",
      "Gets the latest state of a long-running operation.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/get\n",
      "\n",
      "        :param `name`: the `name` of the operation resource.\n",
      "        :type `name`: str\n",
      "        :return: a resource operation instance.\n",
      "        :rtype: dict\n",
      "Deletes the long-running operation.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/delete\n",
      "\n",
      "        :param name: the name of the operation resource.\n",
      "        :type name: str\n",
      "        :return: none if successful.\n",
      "        :rtype: dict\n",
      "Deletes the long-running operation.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/delete\n",
      "\n",
      "        :param name: the name of the operation resource.\n",
      "        :type name: str\n",
      "        :return: none if successful.\n",
      "        :rtype: dict\n",
      "{'name': '`name`'}\n",
      "Deletes the long-running operation.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/data/rest/v1/projects.operations/delete\n",
      "\n",
      "        :param `name`: the `name` of the operation resource.\n",
      "        :type `name`: str\n",
      "        :return: none if successful.\n",
      "        :rtype: dict\n",
      "Poll backup operation state until it's completed.\n",
      "\n",
      "        :param name: the name of the operation resource\n",
      "        :type name: str\n",
      "        :param polling_interval_in_seconds: The number of seconds to wait before calling another request.\n",
      "        :type polling_interval_in_seconds: int\n",
      "        :return: a resource operation instance.\n",
      "        :rtype: dict\n",
      "Poll backup operation state until it's completed.\n",
      "\n",
      "        :param name: the name of the operation resource\n",
      "        :type name: str\n",
      "        :param polling_interval_in_seconds: The number of seconds to wait before calling another request.\n",
      "        :type polling_interval_in_seconds: int\n",
      "        :return: a resource operation instance.\n",
      "        :rtype: dict\n",
      "{'name': '`name`', 'polling_interval_in_seconds': '`polling_interval_in_seconds`'}\n",
      "Poll backup operation state until it's completed.\n",
      "\n",
      "        :param `name`: the `name` of the operation resource\n",
      "        :type `name`: str\n",
      "        :param `polling_interval_in_seconds`: The number of seconds to wait before calling another request.\n",
      "        :type `polling_interval_in_seconds`: int\n",
      "        :return: a resource operation instance.\n",
      "        :rtype: dict\n",
      "Export entities from Cloud Datastore to Cloud Storage for backup.\n",
      "\n",
      "        .. note::\n",
      "            Keep in mind that this requests the Admin API not the Data API.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/export\n",
      "\n",
      "        :param bucket: The name of the Cloud Storage bucket.\n",
      "        :type bucket: str\n",
      "        :param namespace: The Cloud Storage namespace path.\n",
      "        :type namespace: str\n",
      "        :param entity_filter: Description of what data from the project is included in the export.\n",
      "        :type entity_filter: dict\n",
      "        :param labels: Client-assigned labels.\n",
      "        :type labels: dict of str\n",
      "        :return: a resource operation instance.\n",
      "        :rtype: dict\n",
      "Export entities from Cloud Datastore to Cloud Storage for backup.\n",
      "\n",
      "        .. note::\n",
      "            Keep in mind that this requests the Admin API not the Data API.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/export\n",
      "\n",
      "        :param bucket: The name of the Cloud Storage bucket.\n",
      "        :type bucket: str\n",
      "        :param namespace: The Cloud Storage namespace path.\n",
      "        :type namespace: str\n",
      "        :param entity_filter: Description of what data from the project is included in the export.\n",
      "        :type entity_filter: dict\n",
      "        :param labels: Client-assigned labels.\n",
      "        :type labels: dict of str\n",
      "        :return: a resource operation instance.\n",
      "        :rtype: dict\n",
      "{'bucket': '`bucket`', 'namespace': '`namespace`', 'entity_filter': '`entity_filter`', 'labels': '`labels`'}\n",
      "Export entities from Cloud Datastore to Cloud Storage for backup.\n",
      "\n",
      "        .. note::\n",
      "            Keep in mind that this requests the Admin API not the Data API.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/export\n",
      "\n",
      "        :param `bucket`: The name of the Cloud Storage `bucket`.\n",
      "        :type `bucket`: str\n",
      "        :param `namespace`: The Cloud Storage `namespace` path.\n",
      "        :type `namespace`: str\n",
      "        :param `entity_filter`: Description of what data from the project is included in the export.\n",
      "        :type `entity_filter`: dict\n",
      "        :param `labels`: Client-assigned `labels`.\n",
      "        :type `labels`: dict of str\n",
      "        :return: a resource operation instance.\n",
      "        :rtype: dict\n",
      "Import a backup from Cloud Storage to Cloud Datastore.\n",
      "\n",
      "        .. note::\n",
      "            Keep in mind that this requests the Admin API not the Data API.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/import\n",
      "\n",
      "        :param bucket: The name of the Cloud Storage bucket.\n",
      "        :type bucket: str\n",
      "        :param file: the metadata file written by the projects.export operation.\n",
      "        :type file: str\n",
      "        :param namespace: The Cloud Storage namespace path.\n",
      "        :type namespace: str\n",
      "        :param entity_filter: specify which kinds/namespaces are to be imported.\n",
      "        :type entity_filter: dict\n",
      "        :param labels: Client-assigned labels.\n",
      "        :type labels: dict of str\n",
      "        :return: a resource operation instance.\n",
      "        :rtype: dict\n",
      "Import a backup from Cloud Storage to Cloud Datastore.\n",
      "\n",
      "        .. note::\n",
      "            Keep in mind that this requests the Admin API not the Data API.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/import\n",
      "\n",
      "        :param bucket: The name of the Cloud Storage bucket.\n",
      "        :type bucket: str\n",
      "        :param file: the metadata file written by the projects.export operation.\n",
      "        :type file: str\n",
      "        :param namespace: The Cloud Storage namespace path.\n",
      "        :type namespace: str\n",
      "        :param entity_filter: specify which kinds/namespaces are to be imported.\n",
      "        :type entity_filter: dict\n",
      "        :param labels: Client-assigned labels.\n",
      "        :type labels: dict of str\n",
      "        :return: a resource operation instance.\n",
      "        :rtype: dict\n",
      "{'bucket': '`bucket`', 'file': '`file`', 'namespace': '`namespace`', 'entity_filter': '`entity_filter`', 'labels': '`labels`'}\n",
      "Import a backup from Cloud Storage to Cloud Datastore.\n",
      "\n",
      "        .. note::\n",
      "            Keep in mind that this requests the Admin API not the Data API.\n",
      "\n",
      "        .. seealso::\n",
      "            https://cloud.google.com/datastore/docs/reference/admin/rest/v1/projects/import\n",
      "\n",
      "        :param `bucket`: The name of the Cloud Storage `bucket`.\n",
      "        :type `bucket`: str\n",
      "        :param `file`: the metadata `file` written by the projects.export operation.\n",
      "        :type `file`: str\n",
      "        :param `namespace`: The Cloud Storage `namespace` path.\n",
      "        :type `namespace`: str\n",
      "        :param `entity_filter`: specify which kinds/`namespace`s are to be imported.\n",
      "        :type `entity_filter`: dict\n",
      "        :param `labels`: Client-assigned `labels`.\n",
      "        :type `labels`: dict of str\n",
      "        :return: a resource operation instance.\n",
      "        :rtype: dict\n",
      "Publish a message to a topic or an endpoint.\n",
      "\n",
      "        :param target_arn: either a TopicArn or an EndpointArn\n",
      "        :type target_arn: str\n",
      "        :param message: the default message you want to send\n",
      "        :param message: str\n",
      "Publish a message to a topic or an endpoint.\n",
      "\n",
      "        :param target_arn: either a TopicArn or an EndpointArn\n",
      "        :type target_arn: str\n",
      "        :param message: the default message you want to send\n",
      "        :param message: str\n",
      "{'target_arn': '`target_arn`', 'message': '`message`'}\n",
      "Publish a `message` to a topic or an endpoint.\n",
      "\n",
      "        :param `target_arn`: either a TopicArn or an EndpointArn\n",
      "        :type `target_arn`: str\n",
      "        :param `message`: the default `message` you want to send\n",
      "        :param `message`: str\n",
      "Fetch the hostname using the callable from the config or using\n",
      "    `socket.getfqdn` as a fallback.\n",
      "Fetch the hostname using the callable from the config or using\n",
      "    `socket.getfqdn` as a fallback.\n",
      "Fetch the hostname using the callable from the config or using\n",
      "    `socket.getfqdn` as a fallback.\n",
      "Retrieves connection to Cloud Natural Language service.\n",
      "\n",
      "        :return: Cloud Natural Language service object\n",
      "        :rtype: google.cloud.language_v1.LanguageServiceClient\n",
      "Retrieves connection to Cloud Natural Language service.\n",
      "\n",
      "        :return: Cloud Natural Language service object\n",
      "        :rtype: google.cloud.language_v1.LanguageServiceClient\n",
      "Retrieves connection to Cloud Natural Language service.\n",
      "\n",
      "        :return: Cloud Natural Language service object\n",
      "        :rtype: google.cloud.language_v1.LanguageServiceClient\n",
      "Finds named entities in the text along with entity types,\n",
      "        salience, mentions for each entity, and other properties.\n",
      "\n",
      "        :param document: Input document.\n",
      "            If a dict is provided, it must be of the same form as the protobuf message Document\n",
      "        :type document: dict or class google.cloud.language_v1.types.Document\n",
      "        :param encoding_type: The encoding type used by the API to calculate offsets.\n",
      "        :type encoding_type: google.cloud.language_v1.types.EncodingType\n",
      "        :param retry: A retry object used to retry requests. If None is specified, requests will not be\n",
      "            retried.\n",
      "        :type retry: google.api_core.retry.Retry\n",
      "        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if\n",
      "            retry is specified, the timeout applies to each individual attempt.\n",
      "        :type timeout: float\n",
      "        :param metadata: Additional metadata that is provided to the method.\n",
      "        :type metadata: sequence[tuple[str, str]]]\n",
      "        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse\n",
      "Finds named entities in the text along with entity types,\n",
      "        salience, mentions for each entity, and other properties.\n",
      "\n",
      "        :param document: Input document.\n",
      "            If a dict is provided, it must be of the same form as the protobuf message Document\n",
      "        :type document: dict or class google.cloud.language_v1.types.Document\n",
      "        :param encoding_type: The encoding type used by the API to calculate offsets.\n",
      "        :type encoding_type: google.cloud.language_v1.types.EncodingType\n",
      "        :param retry: A retry object used to retry requests. If None is specified, requests will not be\n",
      "            retried.\n",
      "        :type retry: google.api_core.retry.Retry\n",
      "        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if\n",
      "            retry is specified, the timeout applies to each individual attempt.\n",
      "        :type timeout: float\n",
      "        :param metadata: Additional metadata that is provided to the method.\n",
      "        :type metadata: sequence[tuple[str, str]]]\n",
      "        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse\n",
      "{'document': '`document`', 'encoding_type': '`encoding_type`', 'retry': '`retry`', 'timeout': '`timeout`', 'metadata': '`metadata`'}\n",
      "Finds named entities in the text along with entity types,\n",
      "        salience, mentions for each entity, and other properties.\n",
      "\n",
      "        :param `document`: Input `document`.\n",
      "            If a dict is provided, it must be of the same form as the protobuf message Document\n",
      "        :type `document`: dict or class google.cloud.language_v1.types.Document\n",
      "        :param `encoding_type`: The encoding type used by the API to calculate offsets.\n",
      "        :type `encoding_type`: google.cloud.language_v1.types.EncodingType\n",
      "        :param `retry`: A `retry` object used to `retry` requests. If None is specified, requests will not be\n",
      "            retried.\n",
      "        :type `retry`: google.api_core.`retry`.Retry\n",
      "        :param `timeout`: The amount of time, in seconds, to wait for the request to complete. Note that if\n",
      "            `retry` is specified, the `timeout` applies to each individual attempt.\n",
      "        :type `timeout`: float\n",
      "        :param `metadata`: Additional `metadata` that is provided to the method.\n",
      "        :type `metadata`: sequence[tuple[str, str]]]\n",
      "        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse\n",
      "A convenience method that provides all the features that analyzeSentiment,\n",
      "        analyzeEntities, and analyzeSyntax provide in one call.\n",
      "\n",
      "        :param document: Input document.\n",
      "            If a dict is provided, it must be of the same form as the protobuf message Document\n",
      "        :type document: dict or google.cloud.language_v1.types.Document\n",
      "        :param features: The enabled features.\n",
      "            If a dict is provided, it must be of the same form as the protobuf message Features\n",
      "        :type features: dict or google.cloud.language_v1.enums.Features\n",
      "        :param encoding_type: The encoding type used by the API to calculate offsets.\n",
      "        :type encoding_type: google.cloud.language_v1.types.EncodingType\n",
      "        :param retry: A retry object used to retry requests. If None is specified, requests will not be\n",
      "            retried.\n",
      "        :type retry: google.api_core.retry.Retry\n",
      "        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if\n",
      "            retry is specified, the timeout applies to each individual attempt.\n",
      "        :type timeout: float\n",
      "        :param metadata: Additional metadata that is provided to the method.\n",
      "        :type metadata: sequence[tuple[str, str]]]\n",
      "        :rtype: google.cloud.language_v1.types.AnnotateTextResponse\n",
      "A convenience method that provides all the features that analyzeSentiment,\n",
      "        analyzeEntities, and analyzeSyntax provide in one call.\n",
      "\n",
      "        :param document: Input document.\n",
      "            If a dict is provided, it must be of the same form as the protobuf message Document\n",
      "        :type document: dict or google.cloud.language_v1.types.Document\n",
      "        :param features: The enabled features.\n",
      "            If a dict is provided, it must be of the same form as the protobuf message Features\n",
      "        :type features: dict or google.cloud.language_v1.enums.Features\n",
      "        :param encoding_type: The encoding type used by the API to calculate offsets.\n",
      "        :type encoding_type: google.cloud.language_v1.types.EncodingType\n",
      "        :param retry: A retry object used to retry requests. If None is specified, requests will not be\n",
      "            retried.\n",
      "        :type retry: google.api_core.retry.Retry\n",
      "        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if\n",
      "            retry is specified, the timeout applies to each individual attempt.\n",
      "        :type timeout: float\n",
      "        :param metadata: Additional metadata that is provided to the method.\n",
      "        :type metadata: sequence[tuple[str, str]]]\n",
      "        :rtype: google.cloud.language_v1.types.AnnotateTextResponse\n",
      "{'document': '`document`', 'features': '`features`', 'encoding_type': '`encoding_type`', 'retry': '`retry`', 'timeout': '`timeout`', 'metadata': '`metadata`'}\n",
      "A convenience method that provides all the `features` that analyzeSentiment,\n",
      "        analyzeEntities, and analyzeSyntax provide in one call.\n",
      "\n",
      "        :param `document`: Input `document`.\n",
      "            If a dict is provided, it must be of the same form as the protobuf message Document\n",
      "        :type `document`: dict or google.cloud.language_v1.types.Document\n",
      "        :param `features`: The enabled `features`.\n",
      "            If a dict is provided, it must be of the same form as the protobuf message Features\n",
      "        :type `features`: dict or google.cloud.language_v1.enums.Features\n",
      "        :param `encoding_type`: The encoding type used by the API to calculate offsets.\n",
      "        :type `encoding_type`: google.cloud.language_v1.types.EncodingType\n",
      "        :param `retry`: A `retry` object used to `retry` requests. If None is specified, requests will not be\n",
      "            retried.\n",
      "        :type `retry`: google.api_core.`retry`.Retry\n",
      "        :param `timeout`: The amount of time, in seconds, to wait for the request to complete. Note that if\n",
      "            `retry` is specified, the `timeout` applies to each individual attempt.\n",
      "        :type `timeout`: float\n",
      "        :param `metadata`: Additional `metadata` that is provided to the method.\n",
      "        :type `metadata`: sequence[tuple[str, str]]]\n",
      "        :rtype: google.cloud.language_v1.types.AnnotateTextResponse\n",
      "Classifies a document into categories.\n",
      "\n",
      "        :param document: Input document.\n",
      "            If a dict is provided, it must be of the same form as the protobuf message Document\n",
      "        :type document: dict or class google.cloud.language_v1.types.Document\n",
      "        :param retry: A retry object used to retry requests. If None is specified, requests will not be\n",
      "            retried.\n",
      "        :type retry: google.api_core.retry.Retry\n",
      "        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if\n",
      "            retry is specified, the timeout applies to each individual attempt.\n",
      "        :type timeout: float\n",
      "        :param metadata: Additional metadata that is provided to the method.\n",
      "        :type metadata: sequence[tuple[str, str]]]\n",
      "        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse\n",
      "Classifies a document into categories.\n",
      "\n",
      "        :param document: Input document.\n",
      "            If a dict is provided, it must be of the same form as the protobuf message Document\n",
      "        :type document: dict or class google.cloud.language_v1.types.Document\n",
      "        :param retry: A retry object used to retry requests. If None is specified, requests will not be\n",
      "            retried.\n",
      "        :type retry: google.api_core.retry.Retry\n",
      "        :param timeout: The amount of time, in seconds, to wait for the request to complete. Note that if\n",
      "            retry is specified, the timeout applies to each individual attempt.\n",
      "        :type timeout: float\n",
      "        :param metadata: Additional metadata that is provided to the method.\n",
      "        :type metadata: sequence[tuple[str, str]]]\n",
      "        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse\n",
      "{'document': '`document`', 'retry': '`retry`', 'timeout': '`timeout`', 'metadata': '`metadata`'}\n",
      "Classifies a `document` into categories.\n",
      "\n",
      "        :param `document`: Input `document`.\n",
      "            If a dict is provided, it must be of the same form as the protobuf message Document\n",
      "        :type `document`: dict or class google.cloud.language_v1.types.Document\n",
      "        :param `retry`: A `retry` object used to `retry` requests. If None is specified, requests will not be\n",
      "            retried.\n",
      "        :type `retry`: google.api_core.`retry`.Retry\n",
      "        :param `timeout`: The amount of time, in seconds, to wait for the request to complete. Note that if\n",
      "            `retry` is specified, the `timeout` applies to each individual attempt.\n",
      "        :type `timeout`: float\n",
      "        :param `metadata`: Additional `metadata` that is provided to the method.\n",
      "        :type `metadata`: sequence[tuple[str, str]]]\n",
      "        :rtype: google.cloud.language_v1.types.AnalyzeEntitiesResponse\n",
      "Return the task object identified by the given dag_id and task_id.\n",
      "Return the task object identified by the given dag_id and task_id.\n",
      "Return the task object identified by the given dag_id and task_id.\n",
      "Gets template fields for specific operator class.\n",
      "\n",
      "    :param fullname: Full path to operator class.\n",
      "        For example: ``airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetCreateOperator``\n",
      "    :return: List of template field\n",
      "    :rtype: list[str]\n",
      "Gets template fields for specific operator class.\n",
      "\n",
      "    :param fullname: Full path to operator class.\n",
      "        For example: ``airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetCreateOperator``\n",
      "    :return: List of template field\n",
      "    :rtype: list[str]\n",
      "{'fullname': '`fullname`'}\n",
      "Gets template fields for specific operator class.\n",
      "\n",
      "    :param `fullname`: Full path to operator class.\n",
      "        For example: ``airflow.contrib.operators.gcp_vision_operator.CloudVisionProductSetCreateOperator``\n",
      "    :return: List of template field\n",
      "    :rtype: list[str]\n",
      "A role that allows you to include a list of template fields in the middle of the text. This is especially\n",
      "    useful when writing guides describing how to use the operator.\n",
      "    The result is a list of fields where each field is shorted in the literal block.\n",
      "\n",
      "    Sample usage::\n",
      "\n",
      "    :template-fields:`airflow.contrib.operators.gcp_natural_language_operator.CloudLanguageAnalyzeSentimentOperator`\n",
      "\n",
      "    For further information look at:\n",
      "\n",
      "    * [http://docutils.sourceforge.net/docs/howto/rst-roles.html](Creating reStructuredText Interpreted\n",
      "      Text Roles)\n",
      "A role that allows you to include a list of template fields in the middle of the text. This is especially\n",
      "    useful when writing guides describing how to use the operator.\n",
      "    The result is a list of fields where each field is shorted in the literal block.\n",
      "\n",
      "    Sample usage::\n",
      "\n",
      "    :template-fields:`airflow.contrib.operators.gcp_natural_language_operator.CloudLanguageAnalyzeSentimentOperator`\n",
      "\n",
      "    For further information look at:\n",
      "\n",
      "    * [http://docutils.sourceforge.net/docs/howto/rst-roles.html](Creating reStructuredText Interpreted\n",
      "      Text Roles)\n",
      "A role that allows you to include a list of template fields in the middle of the text. This is especially\n",
      "    useful when writing guides describing how to use the operator.\n",
      "    The result is a list of fields where each field is shorted in the literal block.\n",
      "\n",
      "    Sample usage::\n",
      "\n",
      "    :template-fields:`airflow.contrib.operators.gcp_natural_language_operator.CloudLanguageAnalyzeSentimentOperator`\n",
      "\n",
      "    For further information look at:\n",
      "\n",
      "    * [http://docutils.sourceforge.net/docs/howto/rst-roles.html](Creating reStructuredText Interpreted\n",
      "      Text Roles)\n",
      "Properly close pooled database connections\n",
      "Properly close pooled database connections\n",
      "Properly close pooled database connections\n",
      "Ensures that certain subfolders of AIRFLOW_HOME are on the classpath\n",
      "Ensures that certain subfolders of AIRFLOW_HOME are on the classpath\n",
      "Ensures that certain subfolders of AIRFLOW_HOME are on the classpath\n",
      "Gets the returned Celery result from the Airflow task\n",
      "        ID provided to the sensor, and returns True if the\n",
      "        celery result has been finished execution.\n",
      "\n",
      "        :param context: Airflow's execution context\n",
      "        :type context: dict\n",
      "        :return: True if task has been executed, otherwise False\n",
      "        :rtype: bool\n",
      "Gets the returned Celery result from the Airflow task\n",
      "        ID provided to the sensor, and returns True if the\n",
      "        celery result has been finished execution.\n",
      "\n",
      "        :param context: Airflow's execution context\n",
      "        :type context: dict\n",
      "        :return: True if task has been executed, otherwise False\n",
      "        :rtype: bool\n",
      "{'context': '`context`'}\n",
      "Gets the returned Celery result from the Airflow task\n",
      "        ID provided to the sensor, and returns True if the\n",
      "        celery result has been finished execution.\n",
      "\n",
      "        :param `context`: Airflow's execution `context`\n",
      "        :type `context`: dict\n",
      "        :return: True if task has been executed, otherwise False\n",
      "        :rtype: bool\n",
      "Return true if the ticket cache contains \"conf\" information as is found\n",
      "    in ticket caches of Kerberos 1.8.1 or later. This is incompatible with the\n",
      "    Sun Java Krb5LoginModule in Java6, so we need to take an action to work\n",
      "    around it.\n",
      "Return true if the ticket cache contains \"conf\" information as is found\n",
      "    in ticket caches of Kerberos 1.8.1 or later. This is incompatible with the\n",
      "    Sun Java Krb5LoginModule in Java6, so we need to take an action to work\n",
      "    around it.\n",
      "Return true if the ticket cache contains \"conf\" information as is found\n",
      "    in ticket caches of Kerberos 1.8.1 or later. This is incompatible with the\n",
      "    Sun Java Krb5LoginModule in Java6, so we need to take an action to work\n",
      "    around it.\n",
      "Transforms a SQLAlchemy model instance into a dictionary\n",
      "Transforms a SQLAlchemy model instance into a dictionary\n",
      "Transforms a SQLAlchemy model instance into a dictionary\n",
      "Yield successive chunks of a given size from a list of items\n",
      "Yield successive chunks of a given size from a list of items\n",
      "Yield successive chunks of a given size from a list of items\n",
      "Reduce the given list of items by splitting it into chunks\n",
      "    of the given size and passing each chunk through the reducer\n",
      "Reduce the given list of items by splitting it into chunks\n",
      "    of the given size and passing each chunk through the reducer\n",
      "Reduce the given list of items by splitting it into chunks\n",
      "    of the given size and passing each chunk through the reducer\n",
      "Given a number of tasks, builds a dependency chain.\n",
      "\n",
      "    chain(task_1, task_2, task_3, task_4)\n",
      "\n",
      "    is equivalent to\n",
      "\n",
      "    task_1.set_downstream(task_2)\n",
      "    task_2.set_downstream(task_3)\n",
      "    task_3.set_downstream(task_4)\n",
      "Given a number of tasks, builds a dependency chain.\n",
      "\n",
      "    chain(task_1, task_2, task_3, task_4)\n",
      "\n",
      "    is equivalent to\n",
      "\n",
      "    task_1.set_downstream(task_2)\n",
      "    task_2.set_downstream(task_3)\n",
      "    task_3.set_downstream(task_4)\n",
      "Given a number of tasks, builds a dependency chain.\n",
      "\n",
      "    chain(task_1, task_2, task_3, task_4)\n",
      "\n",
      "    is equivalent to\n",
      "\n",
      "    task_1.set_downstream(task_2)\n",
      "    task_2.set_downstream(task_3)\n",
      "    task_3.set_downstream(task_4)\n",
      "Returns a pretty ascii table from tuples\n",
      "\n",
      "    If namedtuple are used, the table will have headers\n",
      "Returns a pretty ascii table from tuples\n",
      "\n",
      "    If namedtuple are used, the table will have headers\n",
      "Returns a pretty ascii table from tuples\n",
      "\n",
      "    If namedtuple are used, the table will have headers\n",
      "Tries really hard to terminate all children (including grandchildren). Will send\n",
      "    sig (SIGTERM) to the process group of pid. If any process is alive after timeout\n",
      "    a SIGKILL will be send.\n",
      "\n",
      "    :param log: log handler\n",
      "    :param pid: pid to kill\n",
      "    :param sig: signal type\n",
      "    :param timeout: how much time a process has to terminate\n",
      "Tries really hard to terminate all children (including grandchildren). Will send\n",
      "    sig (SIGTERM) to the process group of pid. If any process is alive after timeout\n",
      "    a SIGKILL will be send.\n",
      "\n",
      "    :param log: log handler\n",
      "    :param pid: pid to kill\n",
      "    :param sig: signal type\n",
      "    :param timeout: how much time a process has to terminate\n",
      "{'log': '`log`', 'pid': '`pid`', 'sig': '`sig`', 'timeout': '`timeout`'}\n",
      "Tries really hard to terminate all children (including grandchildren). Will send\n",
      "    `sig` (SIGTERM) to the process group of `pid`. If any process is alive after `timeout`\n",
      "    a SIGKILL will be send.\n",
      "\n",
      "    :param `log`: `log` handler\n",
      "    :param `pid`: `pid` to kill\n",
      "    :param `sig`: `sig`nal type\n",
      "    :param `timeout`: how much time a process has to terminate\n",
      "Given task instance, try_number, filename_template, return the rendered log\n",
      "    filename\n",
      "\n",
      "    :param ti: task instance\n",
      "    :param try_number: try_number of the task\n",
      "    :param filename_template: filename template, which can be jinja template or\n",
      "        python string template\n",
      "Given task instance, try_number, filename_template, return the rendered log\n",
      "    filename\n",
      "\n",
      "    :param ti: task instance\n",
      "    :param try_number: try_number of the task\n",
      "    :param filename_template: filename template, which can be jinja template or\n",
      "        python string template\n",
      "{'ti': '`ti`', 'try_number': '`try_number`', 'filename_template': '`filename_template`'}\n",
      "Given task instance, `try_number`, `filename_template`, return the rendered log\n",
      "    filename\n",
      "\n",
      "    :param `ti`: task instance\n",
      "    :param `try_number`: `try_number` of the task\n",
      "    :param `filename_template`: filename template, which can be jinja template or\n",
      "        python string template\n",
      "Return the task object identified by the given dag_id and task_id.\n",
      "Return the task object identified by the given dag_id and task_id.\n",
      "Return the task object identified by the given dag_id and task_id.\n",
      "Integrate plugins to the context\n",
      "Integrate plugins to the context\n",
      "Integrate plugins to the context\n",
      "Returns a Google Cloud Dataproc service object.\n",
      "Returns a Google Cloud Dataproc service object.\n",
      "Returns a Google Cloud Dataproc service object.\n",
      "Awaits for Google Cloud Dataproc Operation to complete.\n",
      "Awaits for Google Cloud Dataproc Operation to complete.\n",
      "Awaits for Google Cloud Dataproc Operation to complete.\n",
      "Coerces content or all values of content if it is a dict to a string. The\n",
      "    function will throw if content contains non-string or non-numeric types.\n",
      "\n",
      "    The reason why we have this function is because the ``self.json`` field must be a\n",
      "    dict with only string values. This is because ``render_template`` will fail\n",
      "    for numerical values.\n",
      "Coerces content or all values of content if it is a dict to a string. The\n",
      "    function will throw if content contains non-string or non-numeric types.\n",
      "\n",
      "    The reason why we have this function is because the ``self.json`` field must be a\n",
      "    dict with only string values. This is because ``render_template`` will fail\n",
      "    for numerical values.\n",
      "Coerces content or all values of content if it is a dict to a string. The\n",
      "    function will throw if content contains non-string or non-numeric types.\n",
      "\n",
      "    The reason why we have this function is because the ``self.json`` field must be a\n",
      "    dict with only string values. This is because ``render_template`` will fail\n",
      "    for numerical values.\n",
      "Handles the Airflow + Databricks lifecycle logic for a Databricks operator\n",
      "\n",
      "    :param operator: Databricks operator being handled\n",
      "    :param context: Airflow context\n",
      "Handles the Airflow + Databricks lifecycle logic for a Databricks operator\n",
      "\n",
      "    :param operator: Databricks operator being handled\n",
      "    :param context: Airflow context\n",
      "{'operator': '`operator`', 'context': '`context`'}\n",
      "Handles the Airflow + Databricks lifecycle logic for a Databricks `operator`\n",
      "\n",
      "    :param `operator`: Databricks `operator` being handled\n",
      "    :param `context`: Airflow `context`\n",
      "Run an pig script using the pig cli\n",
      "\n",
      "        >>> ph = PigCliHook()\n",
      "        >>> result = ph.run_cli(\"ls /;\")\n",
      "        >>> (\"hdfs://\" in result)\n",
      "        True\n",
      "Run an pig script using the pig cli\n",
      "\n",
      "        >>> ph = PigCliHook()\n",
      "        >>> result = ph.run_cli(\"ls /;\")\n",
      "        >>> (\"hdfs://\" in result)\n",
      "        True\n",
      "Run an pig script using the pig cli\n",
      "\n",
      "        >>> ph = PigCliHook()\n",
      "        >>> result = ph.run_cli(\"ls /;\")\n",
      "        >>> (\"hdfs://\" in result)\n",
      "        True\n",
      "Fetch and return the state of the given celery task. The scope of this function is\n",
      "    global so that it can be called by subprocesses in the pool.\n",
      "\n",
      "    :param celery_task: a tuple of the Celery task key and the async Celery object used\n",
      "        to fetch the task's state\n",
      "    :type celery_task: tuple(str, celery.result.AsyncResult)\n",
      "    :return: a tuple of the Celery task key and the Celery state of the task\n",
      "    :rtype: tuple[str, str]\n",
      "Fetch and return the state of the given celery task. The scope of this function is\n",
      "    global so that it can be called by subprocesses in the pool.\n",
      "\n",
      "    :param celery_task: a tuple of the Celery task key and the async Celery object used\n",
      "        to fetch the task's state\n",
      "    :type celery_task: tuple(str, celery.result.AsyncResult)\n",
      "    :return: a tuple of the Celery task key and the Celery state of the task\n",
      "    :rtype: tuple[str, str]\n",
      "{'celery_task': '`celery_task`'}\n",
      "Fetch and return the state of the given celery task. The scope of this function is\n",
      "    global so that it can be called by subprocesses in the pool.\n",
      "\n",
      "    :param `celery_task`: a tuple of the Celery task key and the async Celery object used\n",
      "        to fetch the task's state\n",
      "    :type `celery_task`: tuple(str, celery.result.AsyncResult)\n",
      "    :return: a tuple of the Celery task key and the Celery state of the task\n",
      "    :rtype: tuple[str, str]\n",
      "How many Celery tasks should each worker process send.\n",
      "\n",
      "        :return: Number of tasks that should be sent per process\n",
      "        :rtype: int\n",
      "How many Celery tasks should each worker process send.\n",
      "\n",
      "        :return: Number of tasks that should be sent per process\n",
      "        :rtype: int\n",
      "How many Celery tasks should each worker process send.\n",
      "\n",
      "        :return: Number of tasks that should be sent per process\n",
      "        :rtype: int\n",
      "How many Celery tasks should be sent to each worker process.\n",
      "\n",
      "        :return: Number of tasks that should be used per process\n",
      "        :rtype: int\n",
      "How many Celery tasks should be sent to each worker process.\n",
      "\n",
      "        :return: Number of tasks that should be used per process\n",
      "        :rtype: int\n",
      "How many Celery tasks should be sent to each worker process.\n",
      "\n",
      "        :return: Number of tasks that should be used per process\n",
      "        :rtype: int\n",
      "Like a Python builtin dict object, setdefault returns the current value\n",
      "        for a key, and if it isn't there, stores the default value and returns it.\n",
      "\n",
      "        :param key: Dict key for this Variable\n",
      "        :type key: str\n",
      "        :param default: Default value to set and return if the variable\n",
      "            isn't already in the DB\n",
      "        :type default: Mixed\n",
      "        :param deserialize_json: Store this as a JSON encoded value in the DB\n",
      "            and un-encode it when retrieving a value\n",
      "        :return: Mixed\n",
      "Like a Python builtin dict object, setdefault returns the current value\n",
      "        for a key, and if it isn't there, stores the default value and returns it.\n",
      "\n",
      "        :param key: Dict key for this Variable\n",
      "        :type key: str\n",
      "        :param default: Default value to set and return if the variable\n",
      "            isn't already in the DB\n",
      "        :type default: Mixed\n",
      "        :param deserialize_json: Store this as a JSON encoded value in the DB\n",
      "            and un-encode it when retrieving a value\n",
      "        :return: Mixed\n",
      "{'key': '`key`', 'default': '`default`', 'deserialize_json': '`deserialize_json`'}\n",
      "Like a Python builtin dict object, set`default` returns the current value\n",
      "        for a `key`, and if it isn't there, stores the `default` value and returns it.\n",
      "\n",
      "        :param `key`: Dict `key` for this Variable\n",
      "        :type `key`: str\n",
      "        :param `default`: Default value to set and return if the variable\n",
      "            isn't already in the DB\n",
      "        :type `default`: Mixed\n",
      "        :param `deserialize_json`: Store this as a JSON encoded value in the DB\n",
      "            and un-encode it when retrieving a value\n",
      "        :return: Mixed\n",
      "Returns a Google MLEngine service object.\n",
      "Returns a Google MLEngine service object.\n",
      "Returns a Google MLEngine service object.\n",
      "Launches a MLEngine job and wait for it to reach a terminal state.\n",
      "\n",
      "        :param project_id: The Google Cloud project id within which MLEngine\n",
      "            job will be launched.\n",
      "        :type project_id: str\n",
      "\n",
      "        :param job: MLEngine Job object that should be provided to the MLEngine\n",
      "            API, such as: ::\n",
      "\n",
      "                {\n",
      "                  'jobId': 'my_job_id',\n",
      "                  'trainingInput': {\n",
      "                    'scaleTier': 'STANDARD_1',\n",
      "                    ...\n",
      "                  }\n",
      "                }\n",
      "\n",
      "        :type job: dict\n",
      "\n",
      "        :param use_existing_job_fn: In case that a MLEngine job with the same\n",
      "            job_id already exist, this method (if provided) will decide whether\n",
      "            we should use this existing job, continue waiting for it to finish\n",
      "            and returning the job object. It should accepts a MLEngine job\n",
      "            object, and returns a boolean value indicating whether it is OK to\n",
      "            reuse the existing job. If 'use_existing_job_fn' is not provided,\n",
      "            we by default reuse the existing MLEngine job.\n",
      "        :type use_existing_job_fn: function\n",
      "\n",
      "        :return: The MLEngine job object if the job successfully reach a\n",
      "            terminal state (which might be FAILED or CANCELLED state).\n",
      "        :rtype: dict\n",
      "Launches a MLEngine job and wait for it to reach a terminal state.\n",
      "\n",
      "        :param project_id: The Google Cloud project id within which MLEngine\n",
      "            job will be launched.\n",
      "        :type project_id: str\n",
      "\n",
      "        :param job: MLEngine Job object that should be provided to the MLEngine\n",
      "            API, such as: ::\n",
      "\n",
      "                {\n",
      "                  'jobId': 'my_job_id',\n",
      "                  'trainingInput': {\n",
      "                    'scaleTier': 'STANDARD_1',\n",
      "                    ...\n",
      "                  }\n",
      "                }\n",
      "\n",
      "        :type job: dict\n",
      "\n",
      "        :param use_existing_job_fn: In case that a MLEngine job with the same\n",
      "            job_id already exist, this method (if provided) will decide whether\n",
      "            we should use this existing job, continue waiting for it to finish\n",
      "            and returning the job object. It should accepts a MLEngine job\n",
      "            object, and returns a boolean value indicating whether it is OK to\n",
      "            reuse the existing job. If 'use_existing_job_fn' is not provided,\n",
      "            we by default reuse the existing MLEngine job.\n",
      "        :type use_existing_job_fn: function\n",
      "\n",
      "        :return: The MLEngine job object if the job successfully reach a\n",
      "            terminal state (which might be FAILED or CANCELLED state).\n",
      "        :rtype: dict\n",
      "{'project_id': '`project_id`', 'job': '`job`', 'use_existing_job_fn': '`use_existing_job_fn`'}\n",
      "Launches a MLEngine `job` and wait for it to reach a terminal state.\n",
      "\n",
      "        :param `project_id`: The Google Cloud project id within which MLEngine\n",
      "            `job` will be launched.\n",
      "        :type `project_id`: str\n",
      "\n",
      "        :param `job`: MLEngine Job object that should be provided to the MLEngine\n",
      "            API, such as: ::\n",
      "\n",
      "                {\n",
      "                  '`job`Id': 'my_`job`_id',\n",
      "                  'trainingInput': {\n",
      "                    'scaleTier': 'STANDARD_1',\n",
      "                    ...\n",
      "                  }\n",
      "                }\n",
      "\n",
      "        :type `job`: dict\n",
      "\n",
      "        :param `use_existing_job_fn`: In case that a MLEngine `job` with the same\n",
      "            `job`_id already exist, this method (if provided) will decide whether\n",
      "            we should use this existing `job`, continue waiting for it to finish\n",
      "            and returning the `job` object. It should accepts a MLEngine `job`\n",
      "            object, and returns a boolean value indicating whether it is OK to\n",
      "            reuse the existing `job`. If '`use_existing_job_fn`' is not provided,\n",
      "            we by default reuse the existing MLEngine `job`.\n",
      "        :type `use_existing_job_fn`: function\n",
      "\n",
      "        :return: The MLEngine `job` object if the `job` successfully reach a\n",
      "            terminal state (which might be FAILED or CANCELLED state).\n",
      "        :rtype: dict\n",
      "Gets a MLEngine job based on the job name.\n",
      "\n",
      "        :return: MLEngine job object if succeed.\n",
      "        :rtype: dict\n",
      "\n",
      "        Raises:\n",
      "            googleapiclient.errors.HttpError: if HTTP error is returned from server\n",
      "Gets a MLEngine job based on the job name.\n",
      "\n",
      "        :return: MLEngine job object if succeed.\n",
      "        :rtype: dict\n",
      "\n",
      "        Raises:\n",
      "            googleapiclient.errors.HttpError: if HTTP error is returned from server\n",
      "Gets a MLEngine job based on the job name.\n",
      "\n",
      "        :return: MLEngine job object if succeed.\n",
      "        :rtype: dict\n",
      "\n",
      "        Raises:\n",
      "            googleapiclient.errors.HttpError: if HTTP error is returned from server\n",
      "Waits for the Job to reach a terminal state.\n",
      "\n",
      "        This method will periodically check the job state until the job reach\n",
      "        a terminal state.\n",
      "\n",
      "        Raises:\n",
      "            googleapiclient.errors.HttpError: if HTTP error is returned when getting\n",
      "            the job\n",
      "Waits for the Job to reach a terminal state.\n",
      "\n",
      "        This method will periodically check the job state until the job reach\n",
      "        a terminal state.\n",
      "\n",
      "        Raises:\n",
      "            googleapiclient.errors.HttpError: if HTTP error is returned when getting\n",
      "            the job\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-967e6f868e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpydf_doc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mdoc_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiple_replace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-967e6f868e35>\u001b[0m in \u001b[0;36mmultiple_replace\u001b[0;34m(doc_str)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mreplacement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdoc_str_parsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_name\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_str_parsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/docstring_parser/parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(text, style)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparse_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSTYLES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParseError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mexc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/docstring_parser/google.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparsed\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \"\"\"\n\u001b[0;32m--> 274\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mGoogleParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/docstring_parser/google.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0mpart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m                 \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/docstring_parser/google.py\u001b[0m in \u001b[0;36m_build_meta\u001b[0;34m(self, text, title)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# Split spec and description\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mbefore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "pydf_doc = pydf['docstring']\n",
    "from docstring_parser import parse\n",
    "\n",
    "def create_dict(string):\n",
    "    replacement = {}\n",
    "    doc_str = parse(x)\n",
    "    params = [param.arg_name for param in doc_str.params]\n",
    "    for param in params:\n",
    "        replacement[param] = '`'+param+'`'\n",
    "    return replacement\n",
    "\n",
    "def multiple_replace(doc_str):\n",
    "    replacement = {}\n",
    "    print(doc_str)\n",
    "    doc_str_parsed = parse(doc_str)\n",
    "    params = [param.arg_name for param in doc_str_parsed.params]\n",
    "    for param in params:\n",
    "        replacement[param] = '`'+param+'`'\n",
    "    if replacement == {}:\n",
    "        return doc_str\n",
    "    else:\n",
    "        print(replacement)\n",
    "        # Create a regular expression  from the dictionary keys\n",
    "        regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, replacement.keys())))\n",
    "\n",
    "        # For each match, look-up corresponding value in dictionary\n",
    "        return regex.sub(lambda mo: replacement[mo.string[mo.start():mo.end()]], doc_str) \n",
    "\n",
    "for x in pydf_doc.iloc[:5000]:\n",
    "    print(x)\n",
    "    doc_str = multiple_replace(x)\n",
    "    print(doc_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docstring</th>\n",
       "      <th>docstring_tokens</th>\n",
       "      <th>code</th>\n",
       "      <th>code_tokens</th>\n",
       "      <th>partition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Extracts video ID from URL.</td>\n",
       "      <td>[Extracts, video, ID, from, URL, .]</td>\n",
       "      <td>def get_vid_from_url(url):\\n        \"\"\"Extract...</td>\n",
       "      <td>[def, get_vid_from_url, (, url, ), :, return, ...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>str-&gt;list\\n    Convert XML to URL List.\\n    F...</td>\n",
       "      <td>[str, -, &gt;, list, Convert, XML, to, URL, List,...</td>\n",
       "      <td>def sina_xml_to_url_list(xml_data):\\n    \"\"\"st...</td>\n",
       "      <td>[def, sina_xml_to_url_list, (, xml_data, ), :,...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From http://cdn37.atwikiimg.com/sitescript/pub...</td>\n",
       "      <td>[From, http, :, //, cdn37, ., atwikiimg, ., co...</td>\n",
       "      <td>def makeMimi(upid):\\n    \"\"\"From http://cdn37....</td>\n",
       "      <td>[def, makeMimi, (, upid, ), :, strSeed, =, \"gG...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           docstring  \\\n",
       "0                        Extracts video ID from URL.   \n",
       "1  str->list\\n    Convert XML to URL List.\\n    F...   \n",
       "2  From http://cdn37.atwikiimg.com/sitescript/pub...   \n",
       "\n",
       "                                    docstring_tokens  \\\n",
       "0                [Extracts, video, ID, from, URL, .]   \n",
       "1  [str, -, >, list, Convert, XML, to, URL, List,...   \n",
       "2  [From, http, :, //, cdn37, ., atwikiimg, ., co...   \n",
       "\n",
       "                                                code  \\\n",
       "0  def get_vid_from_url(url):\\n        \"\"\"Extract...   \n",
       "1  def sina_xml_to_url_list(xml_data):\\n    \"\"\"st...   \n",
       "2  def makeMimi(upid):\\n    \"\"\"From http://cdn37....   \n",
       "\n",
       "                                         code_tokens partition  \n",
       "0  [def, get_vid_from_url, (, url, ), :, return, ...      test  \n",
       "1  [def, sina_xml_to_url_list, (, xml_data, ), :,...      test  \n",
       "2  [def, makeMimi, (, upid, ), :, strSeed, =, \"gG...      test  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pydf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def sina_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n",
      "    \"\"\"Downloads Sina videos by URL.\n",
      "    \"\"\"\n",
      "    if 'news.sina.com.cn/zxt' in url:\n",
      "        sina_zxt(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\n",
      "        return\n",
      "\n",
      "    vid = match1(url, r'vid=(\\d+)')\n",
      "    if vid is None:\n",
      "        video_page = get_content(url)\n",
      "        vid = hd_vid = match1(video_page, r'hd_vid\\s*:\\s*\\'([^\\']+)\\'')\n",
      "        if hd_vid == '0':\n",
      "            vids = match1(video_page, r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'').split('|')\n",
      "            vid = vids[-1]\n",
      "\n",
      "    if vid is None:\n",
      "        vid = match1(video_page, r'vid:\"?(\\d+)\"?')\n",
      "    if vid:\n",
      "        #title = match1(video_page, r'title\\s*:\\s*\\'([^\\']+)\\'')\n",
      "        sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\n",
      "    else:\n",
      "        vkey = match1(video_page, r'vkey\\s*:\\s*\"([^\"]+)\"')\n",
      "        if vkey is None:\n",
      "            vid = match1(url, r'#(\\d+)')\n",
      "            sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\n",
      "            return\n",
      "        title = match1(video_page, r'title\\s*:\\s*\"([^\"]+)\"')\n",
      "        sina_download_by_vkey(vkey, title=title, output_dir=output_dir, merge=merge, info_only=info_only)\n"
     ]
    }
   ],
   "source": [
    "index_example = 10\n",
    "\n",
    "code_example = pydf.iloc[index_example]['code']\n",
    "print(code_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def metadata(self, item, filter_classified=False):\n",
      "        \"\"\"Add metadata to an item.\n",
      "\n",
      "        It adds metadata to a given item such as how and\n",
      "        when it was fetched. The contents from the original item will\n",
      "        be stored under the 'data' keyword.\n",
      "\n",
      "        :param item: an item fetched by a backend\n",
      "        :param filter_classified: sets if classified fields were filtered\n",
      "        \"\"\"\n",
      "        item = {\n",
      "            'backend_name': self.__class__.__name__,\n",
      "            'backend_version': self.version,\n",
      "            'perceval_version': __version__,\n",
      "            'timestamp': datetime_utcnow().timestamp(),\n",
      "            'origin': self.origin,\n",
      "            'uuid': uuid(self.origin, self.metadata_id(item)),\n",
      "            'updated_on': self.metadata_updated_on(item),\n",
      "            'classified_fields_filtered': self.classified_fields if filter_classified else None,\n",
      "            'category': self.metadata_category(item),\n",
      "            'tag': self.tag,\n",
      "            'data': item,\n",
      "        }\n",
      "\n",
      "        return item\n",
      "\n",
      "\n",
      "Add metadata to an item.\n",
      "\n",
      "        It adds metadata to a given item such as how and\n",
      "        when it was fetched. The contents from the original item will\n",
      "        be stored under the 'data' keyword.\n",
      "\n",
      "        :param item: an item fetched by a backend\n",
      "        :param filter_classified: sets if classified fields were filtered\n"
     ]
    }
   ],
   "source": [
    "index = 5000\n",
    "\n",
    "print(pydf.iloc[index]['code'])\n",
    "print()\n",
    "print()\n",
    "print(pydf.iloc[index]['docstring'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    412178\n",
       "valid     23107\n",
       "test      22176\n",
       "Name: partition, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pydf.partition.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pydf['code_len'] = pydf.code_tokens.apply(lambda x: len(x))\n",
    "pydf['query_len'] = pydf.docstring_tokens.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg docstring len: 16\n",
      "Avg code len: 117\n"
     ]
    }
   ],
   "source": [
    "print('Avg docstring len: %d' % np.average(pydf['query_len']), file=sys.stderr)\n",
    "print('Avg code len: %d' % np.average(pydf['code_len']), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.80</th>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.95</th>\n",
       "      <td>48.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      query_len\n",
       "0.50       10.0\n",
       "0.70       15.0\n",
       "0.80       20.0\n",
       "0.90       33.0\n",
       "0.95       48.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_len_summary = pydf['query_len'].quantile([.5, .7, .8, .9, .95])\n",
    "display(pd.DataFrame(query_len_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.50</th>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.70</th>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.80</th>\n",
       "      <td>155.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.90</th>\n",
       "      <td>237.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.95</th>\n",
       "      <td>341.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      code_len\n",
       "0.50      72.0\n",
       "0.70     114.0\n",
       "0.80     155.0\n",
       "0.90     237.0\n",
       "0.95     341.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "code_len_summary = pydf['code_len'].quantile([.5, .7, .8, .9, .95])\n",
    "display(pd.DataFrame(code_len_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get action sequence & reconstruct code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_sequence len of the example 369\n",
      "\n",
      " Reconstructed code : \n",
      " \n",
      " def sina_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n",
      "    \"\"\"Downloads Sina videos by URL.\n",
      "    \"\"\"\n",
      "    if 'news.sina.com.cn/zxt' in url:\n",
      "        sina_zxt(url, output_dir=output_dir, merge=merge, info_only=\n",
      "            info_only, **kwargs)\n",
      "        return\n",
      "    vid = match1(url, 'vid=(\\\\d+)')\n",
      "    if vid is None:\n",
      "        video_page = get_content(url)\n",
      "        vid = hd_vid = match1(video_page, \"hd_vid\\\\s*:\\\\s*\\\\'([^\\\\']+)\\\\'\")\n",
      "        if hd_vid == '0':\n",
      "            vids = match1(video_page, \"[^\\\\w]vid\\\\s*:\\\\s*\\\\'([^\\\\']+)\\\\'\"\n",
      "                ).split('|')\n",
      "            vid = vids[-1]\n",
      "    if vid is None:\n",
      "        vid = match1(video_page, 'vid:\"?(\\\\d+)\"?')\n",
      "    if vid:\n",
      "        sina_download_by_vid(vid, output_dir=output_dir, merge=merge,\n",
      "            info_only=info_only)\n",
      "    else:\n",
      "        vkey = match1(video_page, 'vkey\\\\s*:\\\\s*\"([^\"]+)\"')\n",
      "        if vkey is None:\n",
      "            vid = match1(url, '#(\\\\d+)')\n",
      "            sina_download_by_vid(vid, output_dir=output_dir, merge=merge,\n",
      "                info_only=info_only)\n",
      "            return\n",
      "        title = match1(video_page, 'title\\\\s*:\\\\s*\"([^\"]+)\"')\n",
      "        sina_download_by_vkey(vkey, title=title, output_dir=output_dir,\n",
      "            merge=merge, info_only=info_only)\n"
     ]
    }
   ],
   "source": [
    "py_ast = ast.parse(code_example)\n",
    "\n",
    "action_sequence = [(a, flag) for a, flag in  ast2seq(py_ast, act_dict, [], [], [])[0]]\n",
    "\n",
    "print('action_sequence len of the example', len(action_sequence))\n",
    "\n",
    "ast_reconstructed = seq2ast(make_iterlists(deque(action_sequence)))\n",
    "\n",
    "print('\\n Reconstructed code : \\n \\n', astor.to_source(ast_reconstructed).rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stats on action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ast(code):\n",
    "    try: \n",
    "        return(ast.parse(code))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "pydf['ast'] = pydf.code.apply(get_ast)\n",
    "pydf['action'] = pydf.ast.apply(ast2seq, action_dict=act_dict, parent_type=[], parent_field=[], parent_cardinality=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_lens = [len(e[0]) for e in pydf['action']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Max action len: 51906\n",
      "Avg action len: 219\n",
      "Actions larger than 100: 302963\n",
      "Actions larger than 200: 150254\n",
      "Actions larger than 300: 85524\n",
      "Actions larger than 1000: 8429\n"
     ]
    }
   ],
   "source": [
    "print('Max action len: %d' % max(action_lens), file=sys.stderr)\n",
    "print('Avg action len: %d' % np.average(action_lens), file=sys.stderr)\n",
    "print('Actions larger than 100: %d' % len(list(filter(lambda x: x > 100, action_lens))), file=sys.stderr)\n",
    "print('Actions larger than 200: %d' % len(list(filter(lambda x: x > 200, action_lens))), file=sys.stderr)\n",
    "print('Actions larger than 300: %d' % len(list(filter(lambda x: x > 300, action_lens))), file=sys.stderr)\n",
    "print('Actions larger than 1000: %d' % len(list(filter(lambda x: x > 1000, action_lens))), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docstring from the dataset does not contain comments inside the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloads Sina videos by URL.\n"
     ]
    }
   ],
   "source": [
    "print(pydf.iloc[index_example]['docstring'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get comments inside the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"#title = match1(video_page, r'title\\\\s*:\\\\s*\\\\'([^\\\\']+)\\\\'')\"]\n"
     ]
    }
   ],
   "source": [
    "comments = []\n",
    "\n",
    "code_encoded = code_example.encode('utf-8')\n",
    "code_bytes = BytesIO(code_encoded) # Bytes form to use tokenize \n",
    "code_tokenized = tokenize(code_bytes.readline)\n",
    "\n",
    "for type_id, string_value, _, _, _ in code_tokenized:\n",
    "    if type_id == 55:\n",
    "        comments.append(string_value)\n",
    "        \n",
    "print(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete docstrings from source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Processed source code output ******\n",
      "=========================================\n",
      "def sina_download(url, output_dir='.', merge=True, info_only=False, **kwargs):\n",
      "    if 'news.sina.com.cn/zxt' in url:\n",
      "        sina_zxt(url, output_dir=output_dir, merge=merge, info_only=\n",
      "            info_only, **kwargs)\n",
      "        return\n",
      "    vid = match1(url, 'vid=(\\\\d+)')\n",
      "    if vid is None:\n",
      "        video_page = get_content(url)\n",
      "        vid = hd_vid = match1(video_page, \"hd_vid\\\\s*:\\\\s*\\\\'([^\\\\']+)\\\\'\")\n",
      "        if hd_vid == '0':\n",
      "            vids = match1(video_page, \"[^\\\\w]vid\\\\s*:\\\\s*\\\\'([^\\\\']+)\\\\'\"\n",
      "                ).split('|')\n",
      "            vid = vids[-1]\n",
      "    if vid is None:\n",
      "        vid = match1(video_page, 'vid:\"?(\\\\d+)\"?')\n",
      "    if vid:\n",
      "        sina_download_by_vid(vid, output_dir=output_dir, merge=merge,\n",
      "            info_only=info_only)\n",
      "    else:\n",
      "        vkey = match1(video_page, 'vkey\\\\s*:\\\\s*\"([^\"]+)\"')\n",
      "        if vkey is None:\n",
      "            vid = match1(url, '#(\\\\d+)')\n",
      "            sina_download_by_vid(vid, output_dir=output_dir, merge=merge,\n",
      "                info_only=info_only)\n",
      "            return\n",
      "        title = match1(video_page, 'title\\\\s*:\\\\s*\"([^\"]+)\"')\n",
      "        sina_download_by_vkey(vkey, title=title, output_dir=output_dir,\n",
      "            merge=merge, info_only=info_only)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed = ast.parse(code_example)\n",
    "\n",
    "for node in ast.walk(parsed):\n",
    "    # let's work only on functions & classes definitions\n",
    "    if not isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n",
    "        continue\n",
    "\n",
    "    if not len(node.body):\n",
    "        continue\n",
    "\n",
    "    if not isinstance(node.body[0], ast.Expr):\n",
    "        continue\n",
    "\n",
    "    if not hasattr(node.body[0], 'value') or not isinstance(node.body[0].value, ast.Str):\n",
    "        continue\n",
    "\n",
    "    # Uncomment lines below if you want print what and where we are removing\n",
    "    # print(node)\n",
    "    # print(node.body[0].value.s)\n",
    "\n",
    "    node.body = node.body[1:]\n",
    "    \n",
    "print('***** Processed source code output ******\\n=========================================')\n",
    "\n",
    "print(astor.to_source(parsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of examples with comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    308230\n",
       "True     149231\n",
       "Name: is_commented, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_commented(code):\n",
    "    try:\n",
    "        code_encoded = code.encode('utf-8')\n",
    "        code_bytes = BytesIO(code_encoded)\n",
    "        code_tokenized = tokenize(code_bytes.readline)\n",
    "        for type_id, string_value, _, _, _ in code_tokenized:\n",
    "            if type_id == 55:\n",
    "                return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "pydf['is_commented'] = pydf.code.apply(is_commented)\n",
    "pydf.is_commented.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoNaLa (Stackoverflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conala_columns = ['intent', 'snippet_actions', 'snippet_tokens']\n",
    "\n",
    "file_list = ['./dataset/data_conala/conala-corpus/conala-train.json.seq2seq', './dataset/data_conala/conala-corpus/conala-test.json.seq2seq']\n",
    "\n",
    "file_list_mined = ['./dataset/data_conala/conala-corpus/conala-train.json.seq2seq', './dataset/data_conala/train/conala-val.csv', './dataset/data_conala/conala-corpus/conala-mined.json.seq2seq']\n",
    "\n",
    "data_train = pickle.load(open('./dataset/data_conala/conala-corpus/conala-train.json.seq2seq', 'rb'))\n",
    "\n",
    "data_test = pickle.load(open('./dataset/data_conala/conala-corpus/conala-test.json.seq2seq', 'rb'))\n",
    "\n",
    "data_mined = pickle.load(open('./dataset/data_conala/conala-corpus/conala-mined.jsonl.seq2seq', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranx_action = [example['snippet_actions'] for example in data_train] + [example['snippet_actions'] for example in data_test]\n",
    "tranx_action = [example['snippet_actions'] for example in data_train]\n",
    "\n",
    "all_action = tranx_action + [example['snippet_actions'] for example in data_mined]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranx_code = [example['snippet_tokens'] for example in data_train] + [example['snippet_tokens'] for example in data_test]\n",
    "tranx_code = [example['snippet_tokens'] for example in data_train]\n",
    "\n",
    "all_code = tranx_action + [example['snippet_tokens'] for example in data_mined]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tranx_query = [example['rewritten_intent'] for example in data_train] + [example['rewritten_intent'] for example in data_test]\n",
    "tranx_query = [example['rewritten_intent'] for example in data_train]\n",
    "tranx_query = [nltk.word_tokenize(example) for example in tranx_query if isinstance(example, str)]\n",
    "\n",
    "all_queries = tranx_action + [example['intent'] for example in data_mined]\n",
    "all_queries = [nltk.word_tokenize(example) for example in all_queries if isinstance(example, str)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Max clean query_len example: 56\n",
      "Avg clean query_len example: 14\n",
      "Var clean query_len example: 44\n",
      "Max all examples query_len: 34\n",
      "Avg all examples query_len: 9\n"
     ]
    }
   ],
   "source": [
    "tranx_query_len = [len(query) for query in tranx_query]\n",
    "all_query_len = [len(query) for query in all_queries]\n",
    "\n",
    "print('Max clean query_len example: %d' % max(tranx_query_len), file=sys.stderr)\n",
    "print('Avg clean query_len example: %d' % np.average(tranx_query_len), file=sys.stderr)\n",
    "print('Var clean query_len example: %d' % np.var(tranx_query_len), file=sys.stderr)\n",
    "\n",
    "print('Max all examples query_len: %d' % max(all_query_len), file=sys.stderr)\n",
    "print('Avg all examples query_len: %d' % np.average(all_query_len), file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Max clean code_len example: 61\n",
      "Avg clean code_len example: 17\n",
      "Var clean code_len example: 77\n",
      "Max all examples code_len: 6527\n",
      "Avg all examples code_len: 23\n"
     ]
    }
   ],
   "source": [
    "tranx_code_len = [len(code) for code in tranx_code]\n",
    "all_code_len = [len(code) for code in all_code]\n",
    "\n",
    "print('Max clean code_len example: %d' % max(tranx_code_len), file=sys.stderr)\n",
    "print('Avg clean code_len example: %d' % np.average(tranx_code_len), file=sys.stderr)\n",
    "print('Var clean code_len example: %d' % np.var(tranx_code_len), file=sys.stderr)\n",
    "\n",
    "print('Max all examples code_len: %d' % max(all_code_len), file=sys.stderr)\n",
    "print('Avg all examples code_len: %d' % np.average(all_code_len), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of clean examples actions: 2371\n",
      "Max clean action_len example: 112\n",
      "Avg clean action_len example: 24\n",
      "Var clean action_len example: 152\n",
      "Actions larger than 100 clean example: 1\n",
      "Number of raw examples: 591167\n",
      "Max all examples action_len: 11238\n",
      "Avg all examples action_len: 36\n",
      "Action sequence larger than 100 within all examples: 33507\n",
      "Action sequence larger than 200 within all examples: 6683\n",
      "Action sequence larger than 300 within all examples: 1540\n"
     ]
    }
   ],
   "source": [
    "tranx_action_len = [len(action) for action in tranx_action]\n",
    "\n",
    "all_action_len = [len(action) for action in all_action]\n",
    "\n",
    "print('Number of clean examples actions: %d' % len(tranx_action_len), file=sys.stderr)\n",
    "print('Max clean action_len example: %d' % max(tranx_action_len), file=sys.stderr)\n",
    "print('Avg clean action_len example: %d' % np.average(tranx_action_len), file=sys.stderr)\n",
    "print('Var clean action_len example: %d' % np.var(tranx_action_len), file=sys.stderr)\n",
    "print('Actions larger than 100 clean example: %d' % len(list(filter(lambda x: x > 100, tranx_action_len))), file=sys.stderr)\n",
    "print('Number of raw examples: %d' % len(all_action_len), file=sys.stderr)\n",
    "print('Max all examples action_len: %d' % max(all_action_len), file=sys.stderr)\n",
    "print('Avg all examples action_len: %d' % np.average(all_action_len), file=sys.stderr)\n",
    "print('Action sequence larger than 100 within all examples: %d' % len(list(filter(lambda x: x > 100, all_action_len))), file=sys.stderr)\n",
    "print('Action sequence larger than 200 within all examples: %d' % len(list(filter(lambda x: x > 200, all_action_len))), file=sys.stderr)\n",
    "print('Action sequence larger than 300 within all examples: %d' % len(list(filter(lambda x: x > 300, all_action_len))), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple de mapping de variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Différence entre les exemples initiaux et prétraités à la main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable names are between quotes ` `, for example:\n",
      "append the sum of each tuple pair in the grouped list `list1` and list `list2` elements to list `list3`\n",
      "\n",
      "The initial intent was:\n",
      "How do i add two lists' elements into one list?\n",
      "\n",
      "The initial code is:\n",
      "list3 = [(a + b) for a, b in zip(list1, list2)]\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_json('./dataset/data_conala/conala-corpus/conala-train.json')\n",
    "\n",
    "initial_intent = df_train.iloc[37]['intent']\n",
    "rewritten_intent = df_train.iloc[37]['rewritten_intent']\n",
    "code = df_train.iloc[37]['snippet']\n",
    "\n",
    "print('Variable names are between quotes ` `, for example:')\n",
    "print(rewritten_intent)\n",
    "print()\n",
    "print('The initial intent was:')\n",
    "print(initial_intent)\n",
    "print()\n",
    "print('The initial code is:')\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping d'exemple dans le cas où il est \"cleené\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The intent becomes:\n",
      "append the sum of each tuple pair in the grouped list var_0 and list var_1 elements to list var_2\n"
     ]
    }
   ],
   "source": [
    "canonical_intent, slot_map = canonicalize_intent(rewritten_intent)\n",
    "lower_intent = canonical_intent.lower()\n",
    "print('The intent becomes:')\n",
    "print(lower_intent)\n",
    "intent_tokens = nltk.word_tokenize(lower_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Then the code change to:\n",
      "var_2 = [(a + b) for a, b in zip(var_0, var_1)]\n"
     ]
    }
   ],
   "source": [
    "encoded_reconstr_code_canonical = canonicalize_code(code, slot_map)\n",
    "\n",
    "print('Then the code switches to:')\n",
    "print(encoded_reconstr_code_canonical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The intent becomes:\n",
      "create list var_0 containing permutations of each element in list str_0 with variable var_1 as tuples\n",
      "{'var_0': {'value': 'done', 'quote': '`', 'type': 'var'}, 'str_0': {'value': '[a, b, c, d]', 'quote': '`', 'type': 'str'}, 'var_1': {'value': 'x', 'quote': '`', 'type': 'var'}}\n"
     ]
    }
   ],
   "source": [
    "canonical_intent, slot_map = canonicalize_intent(\"create list `done` containing permutations of each element in list `[a, b, c, d]` with variable `x` as tuples\")\n",
    "lower_intent = canonical_intent.lower()\n",
    "print('The intent becomes:')\n",
    "print(lower_intent)\n",
    "print(slot_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Then the code switches to:\n",
      "var_0 = [(el, var_1) for el in [str_0]]\n"
     ]
    }
   ],
   "source": [
    "encoded_reconstr_code_canonical = canonicalize_code(\"done = [(el, x) for el in [a, b, c, d]]\", slot_map)\n",
    "\n",
    "print('Then the code switches to:')\n",
    "print(encoded_reconstr_code_canonical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/n.beau/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "train_set_conala = pd.read_csv('./dataset/data_conala/train/conala-train.csv')\n",
    "dev_set_conala = pd.read_csv('./dataset/data_conala/train/conala-val.csv')\n",
    "test_set_django = pd.read_csv('./dataset/data_conala/test/conala-test.csv')\n",
    "\n",
    "total_set = pd.concat([train_set_conala, dev_set_conala, test_set_django])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total de variables remplacées 0\n"
     ]
    }
   ],
   "source": [
    "total_variables_changed = [len(eval(example[5])) for example in total_set.values]\n",
    "total_variables_changed = sum([len(eval(example)) for example in total_set['slot_map']])\n",
    "print(\"total de variables remplacées\", total_variables_changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1512\n"
     ]
    }
   ],
   "source": [
    "\n",
    "number_var_copiable = 0\n",
    "\n",
    "for x in total_set['intent']:\n",
    "    # print(x)\n",
    "    quote_position = [i for i,x in enumerate(eval(x)) if x=='`']\n",
    "    # print(quote_position)\n",
    "    list2 = [quote_position[i:i + 2] for i in range(0, len(quote_position), 2)]\n",
    "    # print(list2)\n",
    "    for y in list2:\n",
    "        try:\n",
    "            diff = y[1] - y[0]\n",
    "        except:\n",
    "            diff = 0\n",
    "        if diff == 2:\n",
    "            number_var_copiable +=1\n",
    "        else: \n",
    "            # print(x)\n",
    "            pass\n",
    "print(number_var_copiable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Django"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_file = './dataset/data_django/all.anno'\n",
    "code_file = './dataset/data_django/all.code'\n",
    "\n",
    "train_set_django = pd.read_csv('./dataset/data_django/train.csv')\n",
    "dev_set_django = pd.read_csv('./dataset/data_django/dev.csv')\n",
    "test_set_django = pd.read_csv('./dataset/data_django/test.csv')\n",
    "\n",
    "#pydf_django_preprocess = pd.concat([train_set_django, dev_set_django, test_set_django])\n",
    "pydf_django_preprocess = pd.concat([train_set_django, dev_set_django])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              intent  \\\n",
      "0  ['[CLS]', 'from', 'thread', '##ing', 'import',...   \n",
      "1  ['[CLS]', 'import', 'module', 'warnings', '.',...   \n",
      "2  ['[CLS]', 'from', 'dj', '##ango', '.', 'con', ...   \n",
      "3  ['[CLS]', 'from', 'dj', '##ango', '.', 'core',...   \n",
      "4  ['[CLS]', 'from', 'dj', '##ango', '.', 'core',...   \n",
      "\n",
      "                                      snippet_tokens  \\\n",
      "0           ['from', 'threading', 'import', 'local']   \n",
      "1                             ['import', 'warnings']   \n",
      "2  ['from', 'django', '.', 'conf', 'import', 'set...   \n",
      "3  ['from', 'django', '.', 'core', 'import', 'sig...   \n",
      "4  ['from', 'django', '.', 'core', '.', 'cache', ...   \n",
      "\n",
      "                                     snippet_actions slot_map  \n",
      "0  ['ImportFrom', 'threading', 'alias', 'local', ...       {}  \n",
      "1  ['Import', 'alias', 'warnings', 'Reduce_primit...       {}  \n",
      "2  ['ImportFrom', 'django.conf', 'alias', 'settin...       {}  \n",
      "3  ['ImportFrom', 'django.core', 'alias', 'signal...       {}  \n",
      "4  ['ImportFrom', 'django.core.cache.backends.bas...       {}  \n"
     ]
    }
   ],
   "source": [
    "print(pydf_django_preprocess.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Max clean query_len example django: 143\n",
      "Avg clean query_len example django: 19\n",
      "Var clean query_len example django: 173\n"
     ]
    }
   ],
   "source": [
    "tranx_query_len = [len(eval(example[0])) for example in pydf_django_preprocess.values]\n",
    "\n",
    "print('Max clean query_len example django: %d' % max(tranx_query_len), file=sys.stderr)\n",
    "print('Avg clean query_len example django: %d' % np.average(tranx_query_len), file=sys.stderr)\n",
    "print('Var clean query_len example django: %d' % np.var(tranx_query_len), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "max_len = [eval(example[0]) for example in pydf_django_preprocess.values if len(eval(example[0])) == 135]\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Max clean code_len example: 149\n",
      "Avg clean code_len example: 10\n",
      "Var clean code_len example: 81\n"
     ]
    }
   ],
   "source": [
    "tranx_code_len = [len(eval(example[1])) for example in pydf_django_preprocess.values]\n",
    "\n",
    "print('Max clean code_len example: %d' % max(tranx_code_len), file=sys.stderr)\n",
    "print('Avg clean code_len example: %d' % np.average(tranx_code_len), file=sys.stderr)\n",
    "print('Var clean code_len example: %d' % np.var(tranx_code_len), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Max clean action_len example: 148\n",
      "Avg clean action_len example: 15\n",
      "Var clean action_len example: 96\n"
     ]
    }
   ],
   "source": [
    "tranx_action_len = [len(eval(example[2])) for example in pydf_django_preprocess.values]\n",
    "\n",
    "print('Max clean action_len example: %d' % max(tranx_action_len), file=sys.stderr)\n",
    "print('Avg clean action_len example: %d' % np.average(tranx_action_len), file=sys.stderr)\n",
    "print('Var clean action_len example: %d' % np.var(tranx_action_len), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total de variables remplacées 0\n"
     ]
    }
   ],
   "source": [
    "total_set_django = pd.concat([train_set_django, dev_set_django, test_set_django])\n",
    "total_variables_changed = sum([len(eval(example)) for example in total_set_django['slot_map']])\n",
    "print(\"total de variables remplacées\", total_variables_changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "number_var_copiable = 0\n",
    "\n",
    "for x in total_set_django['intent']:\n",
    "    # print(x)\n",
    "    quote_position = [i for i,x in enumerate(eval(x)) if x=='`']\n",
    "    # print(quote_position)\n",
    "    list2 = [quote_position[i:i + 2] for i in range(0, len(quote_position), 2)]\n",
    "    # print(list2)\n",
    "    for y in list2:\n",
    "        try:\n",
    "            diff = y[1] - y[0]\n",
    "        except:\n",
    "            diff = 0\n",
    "        if diff == 2:\n",
    "            number_var_copiable +=1\n",
    "        else: \n",
    "            # print(x)\n",
    "            pass\n",
    "print(number_var_copiable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
